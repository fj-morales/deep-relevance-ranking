{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reanking BM25 + extra features with linear model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "# For Linear model\n",
    "# Inspiration from:\n",
    "# https://opensourceconnections.com/blog/2017/04/01/learning-to-rank-linear-models/\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# from math import sin\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "\n",
    "\n",
    "# REMOVE!!\n",
    "from eval_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def load_features(features_file):\n",
    "    with open(features_file, 'rt') as ff:\n",
    "        rels = []\n",
    "        qids = []\n",
    "        features =  []\n",
    "        docids = []\n",
    "        for feature_line in ff:\n",
    "            cols = feature_line.split(' ')\n",
    "            rels.append(cols[0])\n",
    "            qids.append(cols[1].split(':')[1])\n",
    "            features.append([x.split(':')[1] for x in cols[2:-1]])\n",
    "            docids.append(cols[-1:][0].split('=')[1].strip('\\n'))\n",
    "    return [np.array(rels, dtype=np.int), \n",
    "            qids, \n",
    "            np.array(features, dtype=np.float), \n",
    "            docids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_model(train_features_file):\n",
    "    [rels, qids, features, docids] = load_features(train_features_file)\n",
    "    \n",
    "    # Slice info for L2R Linear Model (Deep Relevance Ranking paper)\n",
    "    extra_features = features[:,-4:]\n",
    "    # Fitting linear model\n",
    "    # Ordinary Least Squares Linear Regression \n",
    "    linearModel = LinearRegression()\n",
    "    linearModel.fit(extra_features, rels)\n",
    "    \n",
    "#     print(linearModel.coef_)\n",
    "#     print(linearModel.intercept_)\n",
    "    return linearModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_file, linear_model):\n",
    "    [rels, qids, features, docids] = load_features(test_file)\n",
    "    extra_features = features[:,-4:]\n",
    "    \n",
    "    predictions = linear_model.predict(extra_features)\n",
    "    print(len(predictions))\n",
    "    \n",
    "    queries_dict = {}\n",
    "    for i, qid in enumerate(qids):\n",
    "        if qid not in queries_dict.keys():\n",
    "            queries_dict[qid] = [[qid, docids[i], predictions[i]]]\n",
    "        else:\n",
    "#             print(qid, docids[i], predictions[i])\n",
    "            queries_dict[qid].append([qid, docids[i], predictions[i]])\n",
    "    \n",
    "    # Sort predictions\n",
    "    {qid:value.sort(key=lambda x: x[2], reverse=True) for (qid, value) in queries_dict.items()}\n",
    "    # Add ranking number\n",
    "    {qid:[x.append(value.index(x) + 1) for x in value] for (qid, value) in queries_dict.items()}\n",
    "\n",
    "    return queries_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictions_run_file(predictions_dict, filename):\n",
    "    with open(filename, 'wt') as f_out:\n",
    "        for qid, value in predictions_dict.items():\n",
    "            [f_out.write(x[0] + ' Q0 ' +  x[1] + ' ' + str(x[3]) + ' ' + str(x[2])[0:7] + ' linearModel\\n') for x in value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "## System inputs, main variable options\n",
    "\n",
    "#     dataset = sys.argv[1] # 'bioasq'\n",
    "#     workdir = './' + dataset + '_dir/'\n",
    "#     data_split = sys.argv[2] # 'test'\n",
    "\n",
    "#     features_file = './bioasq_dir/bioasq.dev_features'\n",
    "    train_features_file = './bioasq_dir/bioasq.trai_features'\n",
    "    test_features_file = './bioasq_dir/bioasq.test_features'\n",
    "    \n",
    "    # train model\n",
    "    linear_model = train_linear_model(train_features_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39360\n"
     ]
    }
   ],
   "source": [
    "    # predict\n",
    "    ranked_dict = predict(test_features_file, linear_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "    run_linear_model_file = './bioasq_dir/run_bioasq_linearModel.test_filtered'\n",
    "    write_predictions_run_file(ranked_dict, run_linear_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './bioasq_dir/bioasq.test_qrels', './bioasq_dir/run_bioasq_linearModel.test_filtered']\n",
      "Run error:  None\n",
      "Linear model results: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'map': '0.4624', 'P_20': '0.2619', 'ndcg_cut_20': '0.5513'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    trec_eval_command = '../../eval/trec_eval'\n",
    "    qrels_file = './bioasq_dir/bioasq.test_qrels'\n",
    "    linear_model_results = eval(trec_eval_command, qrels_file, run_linear_model_file)\n",
    "    print('Linear model results: \\n')\n",
    "    linear_model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './bioasq_dir/bioasq.test_qrels', './bioasq_dir/run_bm25_bioasq.test_filtered']\n",
      "Run error:  None\n",
      "BM25 (default) results: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'map': '0.4598', 'P_20': '0.2557', 'ndcg_cut_20': '0.5518'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    trec_eval_command = '../../eval/trec_eval'\n",
    "    qrels_file = './bioasq_dir/bioasq.test_qrels'\n",
    "    run_bm25_file = './bioasq_dir/run_bm25_bioasq.test_filtered'\n",
    "    bm25_results = eval(trec_eval_command, qrels_file, run_bm25_file)\n",
    "    print('BM25 (default) results: \\n')\n",
    "    bm25_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
