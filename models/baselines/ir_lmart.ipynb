{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ir_lmart.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "# Stronger baseline: Listwise L2R - LambdaMART\n",
    "# Hyperparameter optimziation HPonsteroids requires Python 3!\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# REMOVE!!\n",
    "from ir_baseline import *\n",
    "\n",
    "# HPO\n",
    "\n",
    "\n",
    "import ConfigSpace as CS\n",
    "import ConfigSpace.hyperparameters as CSH\n",
    "\n",
    "from hpbandster.core.worker import Worker\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "# HPO server and stuff\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "import argparse\n",
    "\n",
    "import hpbandster.core.nameserver as hpns\n",
    "import hpbandster.core.result as hpres\n",
    "\n",
    "from hpbandster.optimizers import BOHB as BOHB\n",
    "from hpbandster.optimizers import RandomSearch as RS\n",
    "from hpbandster.examples.commons import MyWorker\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "# In[3]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def generate_run_file(pre_run_file, run_file):\n",
    "    \n",
    "    with open(pre_run_file, 'rt') as input_f:\n",
    "        pre_run = input_f.readlines()\n",
    "#         print(type(pre_run))\n",
    "    with open(run_file, 'wt') as out_f:\n",
    "        for line in pre_run:\n",
    "            out_f.write(line.replace('docid=','').replace('indri', 'lambdaMART'))\n",
    "        \n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "# Classes\n",
    "class L2Ranker:\n",
    "    def __init__(self, ranklib_location, params, normalization=[]):\n",
    "        self.ranklib_location = ranklib_location\n",
    "        # Works with Oracle JSE\n",
    "        # java version \"1.8.0_211\"\n",
    "        # Java(TM) SE Runtime Environment (build 1.8.0_211-b12)\n",
    "        # Java HotSpot(TM) 64-Bit Server VM (build 25.211-b12, mixed mode)\n",
    "        self.params = params\n",
    "        self.log_file = self.params[-1:][0] + '.log'\n",
    "        self.ranker_command = ['java', '-jar', ranklib_location + 'RankLib-2.12.jar']\n",
    "        self.normalization = normalization\n",
    "        self.save_model_file = ''\n",
    "        \n",
    "#     def build(self, ir_tool_params):\n",
    "    def train(self, train_data_file, save_model_file, config):\n",
    "        self.save_model_file = save_model_file\n",
    "        toolkit_parameters = [\n",
    "                                *self.ranker_command, # * to unpack list elements\n",
    "                                '-train',\n",
    "                                train_data_file,\n",
    "                                *self.normalization,\n",
    "                                *self.params,\n",
    "                                '-leaf', \n",
    "                                str(config['n_leaves']),\n",
    "                                '-shrinkage',\n",
    "                                str(config['learning_rate']),\n",
    "                                '-tree', # Oner regression tree per boosted iteration\n",
    "                                str(config['n_trees']),\n",
    "                                '-save',\n",
    "                                self.save_model_file   \n",
    "                            ] \n",
    "        \n",
    "#         print(toolkit_parameters)\n",
    "        with open(self.log_file, 'wt') as rf:\n",
    "            proc = subprocess.Popen(toolkit_parameters,stdin=subprocess.PIPE, stdout=rf, stderr=subprocess.STDOUT, shell=False)\n",
    "#         proc = subprocess.Popen(toolkit_parameters,stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=False)\n",
    "            \n",
    "        (out, err)= proc.communicate()\n",
    "#         print(out.decode('utf-8').splitlines())\n",
    "#         print(out)\n",
    "#         print(err)\n",
    "        print('Model saved: ', self.save_model_file)\n",
    "            \n",
    "  \n",
    "\n",
    "    def gen_run_file(self, test_data_file, run_file):\n",
    "        pre_run_file = run_file.replace('run_', 'pre_run_', 1)\n",
    "        toolkit_parameters = [\n",
    "                                *self.ranker_command, # * to unpack list elements\n",
    "                                '-load',\n",
    "                                self.save_model_file,\n",
    "                                *self.normalization,\n",
    "                                '-rank',\n",
    "                                test_data_file,\n",
    "                                '-indri',\n",
    "                                pre_run_file     \n",
    "                            ] \n",
    "        \n",
    "#         print(toolkit_parameters)\n",
    "        with open(self.log_file, 'at') as rf:\n",
    "            proc = subprocess.Popen(toolkit_parameters,stdin=subprocess.PIPE, stdout=rf, stderr=subprocess.STDOUT, shell=False)\n",
    "#         proc = subprocess.Popen(toolkit_parameters,stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=False)\n",
    "            \n",
    "        (out, err)= proc.communicate()\n",
    "#         print(out.decode('utf-8').splitlines())\n",
    "#         print(out)\n",
    "#         print(err)\n",
    "\n",
    "        \n",
    "        generate_run_file(pre_run_file, run_file)\n",
    "        \n",
    "#         print('Run model saved: ', run_file)\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "# try:\n",
    "#     import keras\n",
    "#     from keras.datasets import mnist\n",
    "#     from keras.models import Sequential\n",
    "#     from keras.layers import Dense, Dropout, Flatten\n",
    "#     from keras.layers import Conv2D, MaxPooling2D\n",
    "#     from keras import backend as K\n",
    "# except:\n",
    "#     raise ImportError(\"For this example you need to install keras.\")\n",
    "\n",
    "# try:\n",
    "#     import torchvision\n",
    "#     import torchvision.transforms as transforms\n",
    "# except:\n",
    "#     raise ImportError(\"For this example you need to install pytorch-vision.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class HpoWorker(Worker):\n",
    "    def __init__(self, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.run_val_file = ''\n",
    "            self.save_model_file = ''\n",
    "\n",
    "            \n",
    "    def compute(self, config, budget, *args, **kwargs):\n",
    "            \"\"\"\n",
    "            Simple example for a compute function using a feed forward network.\n",
    "            It is trained on the MNIST dataset.\n",
    "            The input parameter \"config\" (dictionary) contains the sampled configurations passed by the bohb optimizer\n",
    "            \"\"\"\n",
    "            \n",
    "            # Train model with config parameters\n",
    "            \n",
    "            \n",
    "                \n",
    "            #     pre_run_file = workdir + 'pre_run_' + dataset + l2r_model\n",
    "            \n",
    "            n_l = config['n_leaves']\n",
    "            l_r = config['learning_rate']\n",
    "            n_t = config['n_trees']\n",
    "            \n",
    "            config_suffix = '_leaves' + str(n_l) + '_lr' + str(l_r) + '_n' + str(n_t)\n",
    "            self.run_val_file = workdir + 'run_' + dataset + l2r_model + config_suffix\n",
    "            self.save_model_file = workdir + dataset + l2r_model + config_suffix\n",
    "            \n",
    "#             lmart_model = L2Ranker(ranklib_location, l2r_params)\n",
    "            lmart_model = L2Ranker(ranklib_location, l2r_params, norm_params)\n",
    "            lmart_model.train(train_data_file, self.save_model_file, config)\n",
    "            lmart_model.gen_run_file(val_data_file, self.run_val_file)\n",
    "            \n",
    "            # Evaluate Model\n",
    "            \n",
    "            val_results = eval(trec_eval_command, qrels_val_file, self.run_val_file)\n",
    "            val_results = val_results.splitlines()\n",
    "            val_map = float(val_results[0].split()[-1:][0])\n",
    "#             print('Aqui pedi imprimir map')\n",
    "\n",
    "            #import IPython; IPython.embed()\n",
    "            return ({\n",
    "                    'loss': 1 - val_map, # remember: HpBandSter always minimizes!\n",
    "#                     'info': {'model': lmart_model}\n",
    "                    'info': 'oh my god'\n",
    "            })\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_configspace():\n",
    "            \"\"\"\n",
    "            It builds the configuration space with the needed hyperparameters.\n",
    "            It is easily possible to implement different types of hyperparameters.\n",
    "            Beside float-hyperparameters on a log scale, it is also able to handle categorical input parameter.\n",
    "            :return: ConfigurationsSpace-Object\n",
    "            \"\"\"\n",
    "            cs = CS.ConfigurationSpace()\n",
    "            \n",
    "#             n_leaves = CSH.UniformIntegerHyperparameter('n_leaves', lower=1, upper=20, default_value=10, q=10, log=False) # for some reason q=10 is illegal?\n",
    "\n",
    "            n_leaves = CSH.UniformIntegerHyperparameter('n_leaves', lower=1, upper=20, default_value=10, log=False)\n",
    "            learning_rate = CSH.UniformFloatHyperparameter('learning_rate', lower=0.01, upper=0.9, default_value=0.1, log=False)\n",
    "            n_trees = CSH.UniformIntegerHyperparameter('n_trees', lower=100, upper=1500, default_value=1000, q=10 ,log=False)\n",
    "#             n_trees = CSH.UniformIntegerHyperparameter('n_trees', lower=1, upper=1000, default_value=1000, q=10, log=False)\n",
    "            \n",
    "            cs.add_hyperparameters([n_leaves, learning_rate, n_trees])\n",
    "\n",
    "            return cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fakeParser:\n",
    "    def __init__(self):\n",
    "        self.min_budget = 2 \n",
    "        self.max_budget = 4\n",
    "        self.n_iterations = 4 \n",
    "        self.n_workers =4\n",
    "        \n",
    "# In[6]:\n",
    "\n",
    "\n",
    "# In[3]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Options and variables\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Example 1 - sequential and local execution.')\n",
    "    parser.add_argument('--min_budget',   type=float, help='Minimum budget used during the optimization.',    default=2)\n",
    "    parser.add_argument('--max_budget',   type=float, help='Maximum budget used during the optimization.',    default=4)\n",
    "    parser.add_argument('--n_iterations', type=int,   help='Number of iterations performed by the optimizer', default=500)\n",
    "    parser.add_argument('--n_workers', type=int,   help='Number of workers to run in parallel.', default=5)\n",
    "\n",
    "#     args=parser.parse_args()\n",
    "    args = fakeParser()\n",
    "    \n",
    "#     dataset = sys.argv[1] # 'bioasq'\n",
    "#     workdir = './' + dataset + '_dir/'\n",
    "#     data_split = sys.argv[2] # 'test'\n",
    "\n",
    "    dataset = 'bioasq'\n",
    "    workdir = './' + dataset + '_dir/'\n",
    "    data_split =  'train'\n",
    "    k_fold = 's1' \n",
    "    ranklib_location = '../../../ranklib/'\n",
    "    \n",
    "#     train_data_file = './bioasq_dir/bioasq.trai_features_reduced'\n",
    "#     val_data_file = './bioasq_dir/bioasq.dev_features_reduced'\n",
    "#     test_data_file = './bioasq_dir/bioasq.test_features_reduced'\n",
    "    \n",
    "    train_data_file = './bioasq_dir/bioasq.trai_features'\n",
    "    val_data_file = './bioasq_dir/bioasq.dev_features'\n",
    "    test_data_file = './bioasq_dir/bioasq.test_features'\n",
    "    \n",
    "    l2r_model = '_lmart_'\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    enabled_features_file = workdir + dataset + l2r_model + 'enabled_features'\n",
    "    \n",
    "#     print(enabled_features_file)\n",
    "    # Train L2R model: LambdaMART\n",
    "    # Parameters \n",
    "    \n",
    "#     n_leaves = '10'\n",
    "#     learning_rate = '0.1'\n",
    "#     n_trees = '1000'\n",
    "#     hpo_params = [n_leaves, learning_rate, n_trees]\n",
    "    \n",
    "    \n",
    "    \n",
    "    metric2t = 'MAP' # 'MAP, NDCG@k, DCG@k, P@k, RR@k, ERR@k (default=ERR@10)'\n",
    "    \n",
    "    ranker_type = '6' # LambdaMART\n",
    "    \n",
    "    # normalization: Feature Engineering?\n",
    "    norm_params = ['-norm', 'zscore'] # 'sum', 'zscore', 'linear'\n",
    "    \n",
    "    l2r_params = [\n",
    "        '-validate',\n",
    "        val_data_file,\n",
    "        '-ranker',\n",
    "        ranker_type,\n",
    "        '-metric2t',\n",
    "        metric2t,\n",
    "        '-feature',\n",
    "        enabled_features_file\n",
    "    ]\n",
    "    \n",
    "    # Run train\n",
    "    \n",
    "#     lmart_model = L2Ranker(ranklib_location, l2r_params)\n",
    "#     lmart_model = L2Ranker(ranklib_location, l2r_params, norm_params)\n",
    "    \n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "#     lmart_model.train(train_data_file, hpo_params)\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "#     lmart_model.gen_run_file(test_data_file, run_file)\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "trec_eval_command = '../../eval/trec_eval'\n",
    "qrels_val_file = './bioasq_dir/bioasq.dev_qrels'\n",
    "#     eval(trec_eval_command, qrels_file, './run_l2linear')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "#     # HPO \n",
    "\n",
    "    \n",
    "    \n",
    "# #     worker = HpoWorker(run_id='0')\n",
    "#     cs = worker.get_configspace()\n",
    "\n",
    "#     config = cs.sample_configuration().get_dictionary()\n",
    "    \n",
    "        \n",
    "# #     pre_run_file = workdir + 'pre_run_' + dataset + l2r_model\n",
    "    \n",
    "#     run_file = workdir + 'run_' + dataset + l2r_model\n",
    "    \n",
    "#     print(config)\n",
    "#     res = worker.compute(config=config)\n",
    "#     print(res['loss'])\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "# Start a nameserver (see example_3)\n",
    "NS = hpns.NameServer(run_id='example1', host='127.0.0.1', port=None)\n",
    "NS.start()\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "workers=[]\n",
    "for i in range(args.n_workers):\n",
    "    worker = HpoWorker( nameserver='127.0.0.1',run_id='example1', id=i)\n",
    "    worker.run(background=True)\n",
    "    workers.append(worker)\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "# Run an optimizer (see example_2)\n",
    "bohb = BOHB(  configspace = worker.get_configspace(),\n",
    "                      run_id = 'example1', \n",
    "                      min_budget = args.min_budget, max_budget = args.max_budget\n",
    "               )\n",
    "res = bohb.run(n_iterations = args.n_iterations, min_n_workers = args.n_workers)\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "# Step 4: Shutdown\n",
    "# After the optimizer run, we must shutdown the master and the nameserver.\n",
    "bohb.shutdown(shutdown_workers=True)\n",
    "NS.shutdown()\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "# Step 5: Analysis\n",
    "# Each optimizer returns a hpbandster.core.result.Result object.\n",
    "# It holds informations about the optimization run like the incumbent (=best) configuration.\n",
    "# For further details about the Result object, see its documentation.\n",
    "# Here we simply print out the best config and some statistics about the performed runs.\n",
    "id2config = res.get_id2config_mapping()\n",
    "incumbent = res.get_incumbent_id()\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "all_runs = res.get_all_runs()\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "print('Best found configuration:', id2config[incumbent]['config'])\n",
    "print('A total of %i unique configurations where sampled.' % len(id2config.keys()))\n",
    "print('A total of %i runs where executed.' % len(res.get_all_runs()))\n",
    "print('Total budget corresponds to %.1f full function evaluations.'%(sum([r.budget for r in all_runs])/args.max_budget))\n",
    "print('Total budget corresponds to %.1f full function evaluations.'%(sum([r.budget for r in all_runs])/args.max_budget))\n",
    "print('The run took  %.1f seconds to complete.'%(all_runs[-1].time_stamps['finished'] - all_runs[0].time_stamps['started']))\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "#     qrels_test_file = './bioasq_dir/bioasq.test_qrels'\n",
    "#     run_val_file = './this.file'\n",
    "#     lmart_model = res['info']\n",
    "#     lmart_model.gen_run_file(test_data_file, run_val_file)\n",
    "#     eval(trec_eval_command, qrels_test_file, run_val_file)\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "id2config"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
