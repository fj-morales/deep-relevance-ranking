{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25 + RM3 with Anserini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import gzip\n",
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import re \n",
    "import shutil\n",
    "from itertools import islice\n",
    "import random\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Options\n",
    "\n",
    "# search best b and k now?\n",
    "# grid_search = 'yes' \n",
    "grid_search = 'no' \n",
    "\n",
    "# build index? \n",
    "build_index_flag = 'yes'\n",
    "# build_index_flag = 'no'\n",
    "\n",
    "# N of workers for multiprocessing used grid_search\n",
    "pool_size = 20\n",
    "\n",
    "hits = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "dataloc = '../../bioasq_data/'\n",
    "# dataloc = '../../robust04_data/split_2/'\n",
    "baseline_files ='./baseline_files/'\n",
    "corpus_files ='./corpus_files/'\n",
    "galago_loc='./galago-3.10-bin/bin/'\n",
    "anserini_loc = '../../../anserini/'\n",
    "\n",
    "## TREC storage\n",
    "trec_storage = '/ssd/francisco/trec_datasets/deep-relevance-ranking/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select data split to work with\n",
    "# split = \"test\"\n",
    "split = \"dev\"\n",
    "# split = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sc(text):\n",
    "##     text = re.sub('[.,?;*!%^&_+():-\\[\\]{}]', '', text.replace('\"', '').replace('/', '').replace('\\\\', '').replace(\"'\", '').strip())\n",
    "##     text = re.sub('[\\[\\]{}.,?;*!%^&_+():-]', '', text.replace('\"', '').replace('/', '').replace('\\\\', '').replace(\"'\", '').strip()) # DeepPaper method\n",
    "    text = re.sub(r'[^\\w\\s]',' ',text) # My method\n",
    "##     text = text.rstrip('.?')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pickle_docs(pickle_filename):\n",
    "    # Pickle to Trectext converter\n",
    "    with open(pickle_filename, 'rb') as f_in:\n",
    "        data = pickle.load(f_in)\n",
    "        if not os.path.exists(baseline_files):\n",
    "            os.makedirs(baseline_files)\n",
    "        if not os.path.exists(corpus_files):\n",
    "            os.makedirs(corpus_files)\n",
    "        docs = {}\n",
    "        for key, value in data.items():\n",
    "            if \"pmid\" in value.keys():\n",
    "                doc_code = value.pop('pmid')\n",
    "            else:\n",
    "                doc_code = key\n",
    "                \n",
    "# Uncomment                 \n",
    "#             doc = '<DOC>\\n' + \\\n",
    "#                   '<DOCNO>' + doc_code + '</DOCNO>\\n' + \\\n",
    "#                   '<TITLE>' + value.pop('title') + '</TITLE>\\n' + \\\n",
    "#                   '<TEXT>' + value.pop('abstractText') + '</TEXT>\\n' + \\\n",
    "#                   '</DOC>\\n'\n",
    "            \n",
    "            doc = '<DOC>\\n' + \\\n",
    "                  '<DOCNO>' + doc_code + '</DOCNO>\\n' + \\\n",
    "                  '<TITLE>' + remove_sc(value.pop('title')) + '</TITLE>\\n' + \\\n",
    "                  '<TEXT>' + remove_sc(value.pop('abstractText')) + '</TEXT>\\n' + \\\n",
    "                  '</DOC>\\n'\n",
    "            docs[doc_code] = doc\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_trecfile(docs, filename, compression = 'yes'):\n",
    "    # Pickle to Trectext converter\n",
    "    doc_list = []\n",
    "    if compression == 'yes':\n",
    "        with gzip.open(filename,'wt', encoding='utf-8') as f_out:\n",
    "            docus = {}\n",
    "            for key, value in docs.items():\n",
    "                f_out.write(value)\n",
    "    else:\n",
    "        with open(filename,'wt', encoding='utf-8') as f_out:\n",
    "            docus = {}\n",
    "            for key, value in docs.items():\n",
    "                f_out.write(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build corpus index with Anserini\n",
    "def build_index(index_input, index_loc, log_file):\n",
    "    if not os.path.exists(index_loc):\n",
    "            os.makedirs(index_loc) \n",
    "#     index_loc_param = '--indexPath=' + index_loc\n",
    "\n",
    "    anserini_index = anserini_loc + 'target/appassembler/bin/IndexCollection'\n",
    "    anserini_parameters = [\n",
    "#                            'nohup', \n",
    "                           'sh',\n",
    "                           anserini_index,\n",
    "                           '-collection',\n",
    "                           'TrecCollection',\n",
    "                           '-generator',\n",
    "                           'JsoupGenerator',\n",
    "                           '-threads',\n",
    "                            '16',\n",
    "                            '-input',\n",
    "                           index_input,\n",
    "                           '-index',\n",
    "                           index_loc,\n",
    "                           '-storePositions',\n",
    "                            '-keepStopwords',\n",
    "                            '-storeDocvectors',\n",
    "                            '-storeRawDocs']\n",
    "#                           ' >& ',\n",
    "#                           log_file,\n",
    "#                            '&']\n",
    "\n",
    "\n",
    "\n",
    "#     anserini_parameters = ['ls',\n",
    "#                           index_loc]\n",
    "\n",
    "\n",
    "#     print(anserini_parameters)\n",
    "\n",
    "    index_proc = subprocess.Popen(anserini_parameters,\n",
    "            stdout=subprocess.PIPE, shell=False)\n",
    "    (out, err) = index_proc.communicate()\n",
    "#     print(out.decode(\"utf-8\"))\n",
    "#     print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_queries_file(queries, filename):\n",
    "    queries_list = []\n",
    "    queries_dict = {}\n",
    "    query = {}\n",
    "    q_dict = {}\n",
    "    q_trec = {}\n",
    "    ids_dict = {}\n",
    "    id_num = 0\n",
    "    for q in queries:\n",
    "        str_id = str(id_num)\n",
    "        id_new = str_id.rjust(15, '0')\n",
    "#         print(q['body'])\n",
    "#         text = q['body']\n",
    "        text = remove_sc(q['body'])\n",
    "#         print(text)\n",
    "    \n",
    "#         text = re.sub(r'[^\\w\\s]',' ',text)\n",
    "##     text = text.lower()\n",
    "##         text = text.rstrip('.?')\n",
    "    \n",
    "        q_dict[q['id']] = q['body']\n",
    "        query['id_new'] = id_new\n",
    "        query['number'] = q['id']\n",
    "        query['text'] = '#stopword(' + text + ')'\n",
    "        queries_list.append(dict(query))\n",
    "        q_t = '<top>\\n\\n' + \\\n",
    "              '<num> Number: ' + id_new + '\\n' + \\\n",
    "              '<title> ' + q['body'] + '\\n\\n' + \\\n",
    "              '<desc> Description:' + '\\n\\n' + \\\n",
    "              '<narr> Narrative:' + '\\n\\n' + \\\n",
    "              '</top>\\n\\n'\n",
    "        q_trec[q['id']] = q_t\n",
    "        ids_dict[str(id_num)] = q['id']\n",
    "        id_num += 1\n",
    "    queries_dict['queries'] = queries_list\n",
    "\n",
    "    with open(filename, 'wt', encoding='utf-8') as q_file:\n",
    "        json.dump(queries_dict, q_file, indent = 4)\n",
    "    \n",
    "    return [q_dict, q_trec, ids_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_docs(q_topics_file, retrieved_docs_file, index_loc, b_val=0.2, k_val=0.8, n_docs=10, n_terms=10, w_ori_q=0.5, hits=100):\n",
    "    \n",
    "    anserini_search = anserini_loc + 'target/appassembler/bin/SearchCollection'\n",
    "#     print(b_val)\n",
    "    command = [ \n",
    "               'sh',\n",
    "               anserini_search,\n",
    "               '-topicreader',\n",
    "                'Trec',\n",
    "                '-index',\n",
    "                index_loc,\n",
    "                '-topics',\n",
    "                q_topics_file,\n",
    "                '-output',\n",
    "                retrieved_docs_file,\n",
    "                '-bm25',\n",
    "                '-b',\n",
    "                str(b_val),\n",
    "                '-k1',\n",
    "                str(k_val),\n",
    "                '-rm3',\n",
    "                '-rm3.fbDocs',\n",
    "                str(n_docs),\n",
    "                '-rm3.fbTerms',\n",
    "                str(n_terms),\n",
    "                '-rm3.originalQueryWeight',\n",
    "                str(w_ori_q),\n",
    "                '-hits',\n",
    "                str(hits)\n",
    "               ]\n",
    "#     print(command)\n",
    "#     command = command.encode('utf-8')\n",
    "    anserini_exec = subprocess.Popen(command, stdout=subprocess.PIPE, shell=False, encoding='utf-8')\n",
    "    (out, err) = anserini_exec.communicate()\n",
    "###     print(out)\n",
    "# ##    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return top 100 bm25 scored docs, given query and corpus indexed by anserini\n",
    "\n",
    "def generate_preds_file(retrieved_docs_file, q_dict, ids_dict, hits=100):\n",
    "    \n",
    "    with open(retrieved_docs_file, 'rt') as f_in:\n",
    "        aux_var = -1\n",
    "        bm25_docs = []\n",
    "        while aux_var != 0:\n",
    "            question = {}\n",
    "            lines_gen = islice(f_in, hits)\n",
    "            documents = []\n",
    "            for line in lines_gen:\n",
    "                id_aux = line.split(' ')[0]\n",
    "                current_key = ids_dict[id_aux]\n",
    "                documents.append(line.split(' ')[2])\n",
    "                \n",
    "###             print(documents)\n",
    "            aux_var = len(documents)\n",
    "            if aux_var == 0: \n",
    "                break\n",
    "# ##            print(aux_var)##\n",
    "# ##            print(documents)\n",
    "            question['id'] = current_key\n",
    "            question['body'] = q_dict[current_key]\n",
    "            \n",
    "            if \"bioasq\" in dataset_name: \n",
    "                documents_url = ['http://www.ncbi.nlm.nih.gov/pubmed/' + doc for doc in documents]\n",
    "                question['documents'] = documents_url\n",
    "            elif \"rob04\" in dataset_name:\n",
    "                question['documents'] = documents\n",
    "            bm25_docs.append(dict(question))\n",
    "            \n",
    "    return bm25_docs        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docus = generate_preds_file(retrieved_docs_file, q_dict, ids_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(docus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_files = [os.path.join(root, name)\n",
    "             for root, dirs, files in os.walk(dataloc)\n",
    "             for name in files\n",
    "             if all(y in name for y in ['docset', split, '.pkl'])]\n",
    "\n",
    "# pkl_files = [ x for x in os.listdir(dataloc) if all(y in x for y in ['docset', '.pkl'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pickle to trectext file format to be processed with galago\n",
    "# pkl_file = [s for s in pkl_files if split in s]\n",
    "# [output_file, doc_list ]= pickle_to_json(pkl_file[0])\n",
    "doc_list = []\n",
    "output_files = []\n",
    "all_docs = []\n",
    "for pkl_file in pkl_files:\n",
    "###     print(pkl_file)\n",
    "    docs = get_pickle_docs(pkl_file)\n",
    "    doc_list = doc_list + list(docs.keys())\n",
    "    all_docs.append(docs)\n",
    "    out_name = pkl_file.split('/')[-1:][0]\n",
    "    out_name = re.sub('\\.pkl', '', out_name)\n",
    "    output_file = corpus_files + out_name + '.gz'\n",
    "    trec_doc_file = trec_storage + out_name + '.gz'\n",
    "    output_files.append(output_file)\n",
    "    ### print(out_name)\n",
    "    to_trecfile(docs, output_file)\n",
    "    to_trecfile(docs, trec_doc_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random grid search sampling\n",
    "\n",
    "def get_random_params(hyper_params, num_iter):\n",
    "    random_h_params_list = []\n",
    "    while len(random_h_params_list) < num_iter:\n",
    "        random_h_params_set = []\n",
    "        for h_param_list in hyper_params:\n",
    "            sampled_h_param = random.sample(list(h_param_list), k=1)\n",
    "#             print(type(sampled_h_param[0]))\n",
    "#             print(sampled_h_param[0])\n",
    "            random_h_params_set.append(round(sampled_h_param[0], 3))\n",
    "        if not random_h_params_set in random_h_params_list:\n",
    "            random_h_params_list.append(random_h_params_set)\n",
    "#             print('Non repeated')\n",
    "        else:\n",
    "            print('repeated')\n",
    "    return random_h_params_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split = split\n",
    "print(data_split)\n",
    "\n",
    "if \"rob04\" in output_files[0]:\n",
    "    s = re.findall(\"(s[0-5]).pkl$\", pkl_file)\n",
    "    dataset_name = \"rob04\"\n",
    "    dataset_name_ext = dataset_name + '_'+ s[0]\n",
    "#     dataset_name_ext = dataset_name \n",
    "    gold_file = '../../robust04_data/rob04.' + split +'.json'\n",
    "#     with open(gold_file, 'w') as outfile:\n",
    "#         json.dump(query_data, outfile, indent = 4)\n",
    "    print(dataset_name_ext)\n",
    "elif \"bioasq\" in output_file:\n",
    "    print(\"bioasq\")\n",
    "    dataset_name = \"bioasq\"\n",
    "    dataset_name_ext = dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_loc = baseline_files + 'anserini_index' + '_' + dataset_name_ext + '_' + data_split\n",
    "# index_input = output_files\n",
    "index_input = corpus_files\n",
    "log_file = baseline_files + 'log_index_' + dataset_name_ext + '_' + data_split\n",
    "\n",
    "if build_index_flag == 'yes':\n",
    "    build_index(index_input, index_loc, log_file)\n",
    "    \n",
    "#     build_index(index_input, index_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_filename = [ x for x in os.listdir(dataloc) if all(y in x for y in [dataset_name +'.'+ data_split, '.json'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries_file = dataloc + q_filename[0]\n",
    "\n",
    "def load_queries(queries_file):\n",
    "    with open(queries_file, 'rb') as input_file:\n",
    "        query_data = json.load(input_file)\n",
    "        return query_data['questions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_files = [os.path.join(root, name)\n",
    "             for root, dirs, files in os.walk(dataloc)\n",
    "             for name in files\n",
    "             if all(y in name for y in [dataset_name +'.'+ data_split, '.json'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = []\n",
    "query_data = {}\n",
    "for file in query_files:\n",
    "    queries = queries + load_queries(file)\n",
    "# ##    print(queries)\n",
    "query_data['questions'] = queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_preds(file, preds):\n",
    "    with open(file, 'wt') as f_out:\n",
    "        json.dump(preds, f_out, indent=4)\n",
    "    print('Predictions file: ' + file + ', done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_files[0].strip('split_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_process():\n",
    "    print( 'Starting', multiprocessing.current_process().name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_question(query):\n",
    "    question = {}\n",
    "    question['body'] = query['body']\n",
    "    question['id'] = query['id']\n",
    "###     print(query['body'].rstrip('.'))\n",
    "#     documents = get_bm25_docs(query['body'].rstrip('.'), index_loc)\n",
    "    documents = get_bm25_docs(query['body'], index_loc)\n",
    "    if \"bioasq\" in dataset_name: \n",
    "        documents_url = ['http://www.ncbi.nlm.nih.gov/pubmed/' + doc for doc in documents]\n",
    "        question['documents'] = documents_url\n",
    "    elif \"rob04\" in dataset_name:\n",
    "        question['documents'] = documents\n",
    "    return dict(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_bm25_docs(query_data['questions'][0]['body'], index_loc)\n",
    "index_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_queries_file = baseline_files + 'bm25_queries_' + dataset_name_ext + '_' + data_split + '.json'\n",
    "[q_dict, q_trec, ids_dict]= generate_queries_file(queries,bm25_queries_file)\n",
    "\n",
    "q_topic_filename = dataset_name_ext + '_' + 'query_topics'  + '_' + data_split + '.txt'\n",
    "q_topics_file = baseline_files + q_topic_filename\n",
    "trec_q_topics_file = trec_storage + q_topic_filename\n",
    "\n",
    "to_trecfile(q_trec, q_topics_file, compression = 'no')\n",
    "to_trecfile(q_trec, trec_q_topics_file, compression = 'no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = 0.2\n",
    "# k = 0.8\n",
    "# retrieved_docs_file = baseline_files + 'bm25_preds_' + dataset_name_ext + '_' + data_split + '_' + 'b' + str(b) + 'k' + str(k) + '.txt'\n",
    "# retrieve_docs(q_topics_file, retrieved_docs_file, q_dict, index_loc, b_val=0.2, k_val=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_bioasq2treceval_qrels(bioasq_data, filename):\n",
    "    with open(filename, 'wt') as f:\n",
    "        for q in bioasq_data['questions']:\n",
    "            for d in q['documents']:\n",
    "                f.write('{0} 0 {1} 1'.format(q['id'], d))\n",
    "                f.write('\\n')\n",
    "\n",
    "def format_bioasq2treceval_qret(bioasq_data, system_name, filename):\n",
    "    with open(filename, 'wt') as f:\n",
    "        for q in bioasq_data['questions']:\n",
    "            rank = 1\n",
    "            for d in q['documents']:\n",
    "                \n",
    "                sim = (len(q['documents']) + 1 - rank) / float(len(q['documents']))\n",
    "                f.write('{0} {1} {2} {3} {4} {5}'.format(q['id'], 0, d, rank, sim, system_name))\n",
    "                f.write('\\n')\n",
    "                rank += 1\n",
    "\n",
    "def trec_evaluate(qrels_file, qret_file):\n",
    "    trec_eval_res = subprocess.Popen(\n",
    "        ['./trec_eval', '-m', 'all_trec', qrels_file, qret_file],\n",
    "        stdout=subprocess.PIPE, shell=False)\n",
    "\n",
    "#     print(trec_eval_res)\n",
    "    (out, err) = trec_eval_res.communicate()\n",
    "    trec_eval_res = out.decode(\"utf-8\")\n",
    "    print(trec_eval_res)\n",
    "    print(out)\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(golden_file, predictions_file):\n",
    "\n",
    "    system_name = predictions_file\n",
    "    \n",
    "    with open(golden_file, 'r') as f:\n",
    "        golden_data = json.load(f)\n",
    "\n",
    "    with open(predictions_file, 'r') as f:\n",
    "        predictions_data = json.load(f)\n",
    "\n",
    "    temp_dir = uuid.uuid4().hex\n",
    "    qrels_temp_file = '{0}/{1}'.format(temp_dir, 'qrels.txt')\n",
    "    qret_temp_file = '{0}/{1}'.format(temp_dir, 'qret.txt')\n",
    "\n",
    "    try:\n",
    "        if not os.path.exists(temp_dir):\n",
    "            os.makedirs(temp_dir)\n",
    "        else:\n",
    "            sys.exit(\"Possible uuid collision\")\n",
    "\n",
    "        format_bioasq2treceval_qrels(golden_data, qrels_temp_file)\n",
    "        format_bioasq2treceval_qret(predictions_data, system_name, qret_temp_file)\n",
    "\n",
    "        trec_evaluate(qrels_temp_file, qret_temp_file)\n",
    "    finally:\n",
    "        os.remove(qrels_temp_file)\n",
    "        os.remove(qret_temp_file)\n",
    "        os.rmdir(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_computing(params):\n",
    "    b = params[0]\n",
    "    k = params[1]\n",
    "    n_doc = params[2]\n",
    "    n_term = params[3]\n",
    "    w_ori_q = params[4]\n",
    "#     b = 0.2\n",
    "#     k = 0.8\n",
    "    params_suffix = 'b' + str(b) + 'k' + str(k) + 'n_doc' + str(n_doc) + 'n_term' + str(n_term) + 'w_ori_q' + str(w_ori_q)\n",
    "\n",
    "    bm25_preds_file = baseline_files + 'bm25_rm3_preds_' + dataset_name_ext + '_' + data_split + '_' + params_suffix + '.json'\n",
    "    \n",
    "    ###     print(bm25_preds_file)\n",
    "    if os.path.isfile(bm25_preds_file):\n",
    "        print(bm25_preds_file + \"Already exists!!\")\n",
    "        return\n",
    "    retrieved_docs_file = baseline_files + 'run_bm25_rm3_preds_' + dataset_name_ext + '_' + data_split + '_' + params_suffix + '.txt'\n",
    "    #print(b)\n",
    "    #print(k)\n",
    "    retrieve_docs(q_topics_file, retrieved_docs_file, index_loc, b, k, n_doc, n_term, w_ori_q)\n",
    "    bm25_preds = {}\n",
    "    bm25_preds['questions'] = generate_preds_file(retrieved_docs_file, q_dict, ids_dict)\n",
    "\n",
    "    save_preds(bm25_preds_file, bm25_preds)  \n",
    "    \n",
    "    golden_file = dataloc + dataset_name_ext + '.' + data_split + '.json'\n",
    "    \n",
    "    if os.path.exists(golden_file):\n",
    "        print('yes, we can evaluate!')    \n",
    "        print(golden_file)    \n",
    "        evaluate(golden_file,bm25_preds_file)\n",
    "    else:\n",
    "        print('no, we cannot evaluate  :( !')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Starting ForkPoolWorker-127\n",
      "Starting ForkPoolWorker-128\n",
      "Predictions file: ./baseline_files/bm25_rm3_preds_bioasq_dev_b0.7k0.9n_doc232n_term36w_ori_q0.5.json, done!\n",
      "yes, we can evaluate!\n",
      "../../bioasq_data/bioasq.dev.json\n",
      "6a1f146d48fb436382715aabeb21921d\n",
      "runid                 \tall\t./baseline_files/bm25_rm3_preds_bioasq_dev_b0.7k0.9n_doc232n_term36w_ori_q0.5.json\n",
      "num_q                 \tall\t100\n",
      "num_ret               \tall\t10000\n",
      "num_rel               \tall\t1415\n",
      "num_rel_ret           \tall\t1093\n",
      "map                   \tall\t0.4178\n",
      "gm_map                \tall\t0.2103\n",
      "Rprec                 \tall\t0.3921\n",
      "bpref                 \tall\t0.8339\n",
      "recip_rank            \tall\t0.6571\n",
      "iprec_at_recall_0.00  \tall\t0.6887\n",
      "iprec_at_recall_0.10  \tall\t0.5884\n",
      "iprec_at_recall_0.20  \tall\t0.5430\n",
      "iprec_at_recall_0.30  \tall\t0.5082\n",
      "iprec_at_recall_0.40  \tall\t0.4712\n",
      "iprec_at_recall_0.50  \tall\t0.4461\n",
      "iprec_at_recall_0.60  \tall\t0.3862\n",
      "iprec_at_recall_0.70  \tall\t0.3479\n",
      "iprec_at_recall_0.80  \tall\t0.3116\n",
      "iprec_at_recall_0.90  \tall\t0.2688\n",
      "iprec_at_recall_1.00  \tall\t0.2147\n",
      "P_5                   \tall\t0.3760\n",
      "P_10                  \tall\t0.3250\n",
      "P_15                  \tall\t0.2867\n",
      "P_20                  \tall\t0.2495\n",
      "P_30                  \tall\t0.2037\n",
      "P_100                 \tall\t0.1093\n",
      "P_200                 \tall\t0.0546\n",
      "P_500                 \tall\t0.0219\n",
      "P_1000                \tall\t0.0109\n",
      "recall_5              \tall\t0.2925\n",
      "recall_10             \tall\t0.3920\n",
      "recall_15             \tall\t0.4476\n",
      "recall_20             \tall\t0.4918\n",
      "recall_30             \tall\t0.5641\n",
      "recall_100            \tall\t0.8339\n",
      "recall_200            \tall\t0.8339\n",
      "recall_500            \tall\t0.8339\n",
      "recall_1000           \tall\t0.8339\n",
      "infAP                 \tall\t0.4178\n",
      "gm_bpref              \tall\t0.7025\n",
      "Rprec_mult_0.20       \tall\t0.4750\n",
      "Rprec_mult_0.40       \tall\t0.4637\n",
      "Rprec_mult_0.60       \tall\t0.4401\n",
      "Rprec_mult_0.80       \tall\t0.4093\n",
      "Rprec_mult_1.00       \tall\t0.3921\n",
      "Rprec_mult_1.20       \tall\t0.3262\n",
      "Rprec_mult_1.40       \tall\t0.3070\n",
      "Rprec_mult_1.60       \tall\t0.2877\n",
      "Rprec_mult_1.80       \tall\t0.2731\n",
      "Rprec_mult_2.00       \tall\t0.2622\n",
      "utility               \tall\t-78.1400\n",
      "11pt_avg              \tall\t0.4341\n",
      "binG                  \tall\t0.4052\n",
      "G                     \tall\t0.4052\n",
      "ndcg                  \tall\t0.6158\n",
      "ndcg_rel              \tall\t0.5715\n",
      "Rndcg                 \tall\t0.5187\n",
      "ndcg_cut_5            \tall\t0.5034\n",
      "ndcg_cut_10           \tall\t0.5027\n",
      "ndcg_cut_15           \tall\t0.5007\n",
      "ndcg_cut_20           \tall\t0.5006\n",
      "ndcg_cut_30           \tall\t0.5134\n",
      "ndcg_cut_100          \tall\t0.6158\n",
      "ndcg_cut_200          \tall\t0.6158\n",
      "ndcg_cut_500          \tall\t0.6158\n",
      "ndcg_cut_1000         \tall\t0.6158\n",
      "map_cut_5             \tall\t0.2429\n",
      "map_cut_10            \tall\t0.2960\n",
      "map_cut_15            \tall\t0.3274\n",
      "map_cut_20            \tall\t0.3464\n",
      "map_cut_30            \tall\t0.3672\n",
      "map_cut_100           \tall\t0.4178\n",
      "map_cut_200           \tall\t0.4178\n",
      "map_cut_500           \tall\t0.4178\n",
      "map_cut_1000          \tall\t0.4178\n",
      "relative_P_5          \tall\t0.5010\n",
      "relative_P_10         \tall\t0.5226\n",
      "relative_P_15         \tall\t0.5270\n",
      "relative_P_20         \tall\t0.5377\n",
      "relative_P_30         \tall\t0.5802\n",
      "relative_P_100        \tall\t0.8339\n",
      "relative_P_200        \tall\t0.8339\n",
      "relative_P_500        \tall\t0.8339\n",
      "relative_P_1000       \tall\t0.8339\n",
      "success_1             \tall\t0.5600\n",
      "success_5             \tall\t0.7500\n",
      "success_10            \tall\t0.8300\n",
      "set_P                 \tall\t0.1093\n",
      "set_relative_P        \tall\t0.8339\n",
      "set_recall            \tall\t0.8339\n",
      "set_map               \tall\t0.0912\n",
      "set_F                 \tall\t0.1754\n",
      "num_nonrel_judged_ret \tall\t0\n",
      "\n",
      "b'runid                 \\tall\\t./baseline_files/bm25_rm3_preds_bioasq_dev_b0.7k0.9n_doc232n_term36w_ori_q0.5.json\\nnum_q                 \\tall\\t100\\nnum_ret               \\tall\\t10000\\nnum_rel               \\tall\\t1415\\nnum_rel_ret           \\tall\\t1093\\nmap                   \\tall\\t0.4178\\ngm_map                \\tall\\t0.2103\\nRprec                 \\tall\\t0.3921\\nbpref                 \\tall\\t0.8339\\nrecip_rank            \\tall\\t0.6571\\niprec_at_recall_0.00  \\tall\\t0.6887\\niprec_at_recall_0.10  \\tall\\t0.5884\\niprec_at_recall_0.20  \\tall\\t0.5430\\niprec_at_recall_0.30  \\tall\\t0.5082\\niprec_at_recall_0.40  \\tall\\t0.4712\\niprec_at_recall_0.50  \\tall\\t0.4461\\niprec_at_recall_0.60  \\tall\\t0.3862\\niprec_at_recall_0.70  \\tall\\t0.3479\\niprec_at_recall_0.80  \\tall\\t0.3116\\niprec_at_recall_0.90  \\tall\\t0.2688\\niprec_at_recall_1.00  \\tall\\t0.2147\\nP_5                   \\tall\\t0.3760\\nP_10                  \\tall\\t0.3250\\nP_15                  \\tall\\t0.2867\\nP_20                  \\tall\\t0.2495\\nP_30                  \\tall\\t0.2037\\nP_100                 \\tall\\t0.1093\\nP_200                 \\tall\\t0.0546\\nP_500                 \\tall\\t0.0219\\nP_1000                \\tall\\t0.0109\\nrecall_5              \\tall\\t0.2925\\nrecall_10             \\tall\\t0.3920\\nrecall_15             \\tall\\t0.4476\\nrecall_20             \\tall\\t0.4918\\nrecall_30             \\tall\\t0.5641\\nrecall_100            \\tall\\t0.8339\\nrecall_200            \\tall\\t0.8339\\nrecall_500            \\tall\\t0.8339\\nrecall_1000           \\tall\\t0.8339\\ninfAP                 \\tall\\t0.4178\\ngm_bpref              \\tall\\t0.7025\\nRprec_mult_0.20       \\tall\\t0.4750\\nRprec_mult_0.40       \\tall\\t0.4637\\nRprec_mult_0.60       \\tall\\t0.4401\\nRprec_mult_0.80       \\tall\\t0.4093\\nRprec_mult_1.00       \\tall\\t0.3921\\nRprec_mult_1.20       \\tall\\t0.3262\\nRprec_mult_1.40       \\tall\\t0.3070\\nRprec_mult_1.60       \\tall\\t0.2877\\nRprec_mult_1.80       \\tall\\t0.2731\\nRprec_mult_2.00       \\tall\\t0.2622\\nutility               \\tall\\t-78.1400\\n11pt_avg              \\tall\\t0.4341\\nbinG                  \\tall\\t0.4052\\nG                     \\tall\\t0.4052\\nndcg                  \\tall\\t0.6158\\nndcg_rel              \\tall\\t0.5715\\nRndcg                 \\tall\\t0.5187\\nndcg_cut_5            \\tall\\t0.5034\\nndcg_cut_10           \\tall\\t0.5027\\nndcg_cut_15           \\tall\\t0.5007\\nndcg_cut_20           \\tall\\t0.5006\\nndcg_cut_30           \\tall\\t0.5134\\nndcg_cut_100          \\tall\\t0.6158\\nndcg_cut_200          \\tall\\t0.6158\\nndcg_cut_500          \\tall\\t0.6158\\nndcg_cut_1000         \\tall\\t0.6158\\nmap_cut_5             \\tall\\t0.2429\\nmap_cut_10            \\tall\\t0.2960\\nmap_cut_15            \\tall\\t0.3274\\nmap_cut_20            \\tall\\t0.3464\\nmap_cut_30            \\tall\\t0.3672\\nmap_cut_100           \\tall\\t0.4178\\nmap_cut_200           \\tall\\t0.4178\\nmap_cut_500           \\tall\\t0.4178\\nmap_cut_1000          \\tall\\t0.4178\\nrelative_P_5          \\tall\\t0.5010\\nrelative_P_10         \\tall\\t0.5226\\nrelative_P_15         \\tall\\t0.5270\\nrelative_P_20         \\tall\\t0.5377\\nrelative_P_30         \\tall\\t0.5802\\nrelative_P_100        \\tall\\t0.8339\\nrelative_P_200        \\tall\\t0.8339\\nrelative_P_500        \\tall\\t0.8339\\nrelative_P_1000       \\tall\\t0.8339\\nsuccess_1             \\tall\\t0.5600\\nsuccess_5             \\tall\\t0.7500\\nsuccess_10            \\tall\\t0.8300\\nset_P                 \\tall\\t0.1093\\nset_relative_P        \\tall\\t0.8339\\nset_recall            \\tall\\t0.8339\\nset_map               \\tall\\t0.0912\\nset_F                 \\tall\\t0.1754\\nnum_nonrel_judged_ret \\tall\\t0\\n'\n",
      "None\n",
      "hello\n",
      "Predictions file: ./baseline_files/bm25_rm3_preds_bioasq_dev_b0.45k3.1n_doc304n_term287w_ori_q0.7.json, done!\n",
      "yes, we can evaluate!\n",
      "../../bioasq_data/bioasq.dev.json\n",
      "54e6f9e2bca44196bcadf51147da561b\n",
      "runid                 \tall\t./baseline_files/bm25_rm3_preds_bioasq_dev_b0.45k3.1n_doc304n_term287w_ori_q0.7.json\n",
      "num_q                 \tall\t100\n",
      "num_ret               \tall\t10000\n",
      "num_rel               \tall\t1415\n",
      "num_rel_ret           \tall\t1020\n",
      "map                   \tall\t0.3840\n",
      "gm_map                \tall\t0.1684\n",
      "Rprec                 \tall\t0.3734\n",
      "bpref                 \tall\t0.7819\n",
      "recip_rank            \tall\t0.6261\n",
      "iprec_at_recall_0.00  \tall\t0.6704\n",
      "iprec_at_recall_0.10  \tall\t0.5941\n",
      "iprec_at_recall_0.20  \tall\t0.5529\n",
      "iprec_at_recall_0.30  \tall\t0.4994\n",
      "iprec_at_recall_0.40  \tall\t0.4300\n",
      "iprec_at_recall_0.50  \tall\t0.3895\n",
      "iprec_at_recall_0.60  \tall\t0.3413\n",
      "iprec_at_recall_0.70  \tall\t0.3040\n",
      "iprec_at_recall_0.80  \tall\t0.2540\n",
      "iprec_at_recall_0.90  \tall\t0.2047\n",
      "iprec_at_recall_1.00  \tall\t0.1874\n",
      "P_5                   \tall\t0.4080\n",
      "P_10                  \tall\t0.3160\n",
      "P_15                  \tall\t0.2587\n",
      "P_20                  \tall\t0.2245\n",
      "P_30                  \tall\t0.1863\n",
      "P_100                 \tall\t0.1020\n",
      "P_200                 \tall\t0.0510\n",
      "P_500                 \tall\t0.0204\n",
      "P_1000                \tall\t0.0102\n",
      "recall_5              \tall\t0.3244\n",
      "recall_10             \tall\t0.3983\n",
      "recall_15             \tall\t0.4340\n",
      "recall_20             \tall\t0.4677\n",
      "recall_30             \tall\t0.5202\n",
      "recall_100            \tall\t0.7819\n",
      "recall_200            \tall\t0.7819\n",
      "recall_500            \tall\t0.7819\n",
      "recall_1000           \tall\t0.7819\n",
      "infAP                 \tall\t0.3840\n",
      "gm_bpref              \tall\t0.4975\n",
      "Rprec_mult_0.20       \tall\t0.4621\n",
      "Rprec_mult_0.40       \tall\t0.4464\n",
      "Rprec_mult_0.60       \tall\t0.4164\n",
      "Rprec_mult_0.80       \tall\t0.3937\n",
      "Rprec_mult_1.00       \tall\t0.3734\n",
      "Rprec_mult_1.20       \tall\t0.3132\n",
      "Rprec_mult_1.40       \tall\t0.2977\n",
      "Rprec_mult_1.60       \tall\t0.2743\n",
      "Rprec_mult_1.80       \tall\t0.2610\n",
      "Rprec_mult_2.00       \tall\t0.2523\n",
      "utility               \tall\t-79.6000\n",
      "11pt_avg              \tall\t0.4025\n",
      "binG                  \tall\t0.3787\n",
      "G                     \tall\t0.3787\n",
      "ndcg                  \tall\t0.5836\n",
      "ndcg_rel              \tall\t0.5508\n",
      "Rndcg                 \tall\t0.4919\n",
      "ndcg_cut_5            \tall\t0.5177\n",
      "ndcg_cut_10           \tall\t0.4929\n",
      "ndcg_cut_15           \tall\t0.4781\n",
      "ndcg_cut_20           \tall\t0.4760\n",
      "ndcg_cut_30           \tall\t0.4859\n",
      "ndcg_cut_100          \tall\t0.5836\n",
      "ndcg_cut_200          \tall\t0.5836\n",
      "ndcg_cut_500          \tall\t0.5836\n",
      "ndcg_cut_1000         \tall\t0.5836\n",
      "map_cut_5             \tall\t0.2482\n",
      "map_cut_10            \tall\t0.2904\n",
      "map_cut_15            \tall\t0.3095\n",
      "map_cut_20            \tall\t0.3221\n",
      "map_cut_30            \tall\t0.3395\n",
      "map_cut_100           \tall\t0.3840\n",
      "map_cut_200           \tall\t0.3840\n",
      "map_cut_500           \tall\t0.3840\n",
      "map_cut_1000          \tall\t0.3840\n",
      "relative_P_5          \tall\t0.5515\n",
      "relative_P_10         \tall\t0.5207\n",
      "relative_P_15         \tall\t0.5049\n",
      "relative_P_20         \tall\t0.5084\n",
      "relative_P_30         \tall\t0.5354\n",
      "relative_P_100        \tall\t0.7819\n",
      "relative_P_200        \tall\t0.7819\n",
      "relative_P_500        \tall\t0.7819\n",
      "relative_P_1000       \tall\t0.7819\n",
      "success_1             \tall\t0.5000\n",
      "success_5             \tall\t0.8000\n",
      "success_10            \tall\t0.8800\n",
      "set_P                 \tall\t0.1020\n",
      "set_relative_P        \tall\t0.7819\n",
      "set_recall            \tall\t0.7819\n",
      "set_map               \tall\t0.0800\n",
      "set_F                 \tall\t0.1636\n",
      "num_nonrel_judged_ret \tall\t0\n",
      "\n",
      "b'runid                 \\tall\\t./baseline_files/bm25_rm3_preds_bioasq_dev_b0.45k3.1n_doc304n_term287w_ori_q0.7.json\\nnum_q                 \\tall\\t100\\nnum_ret               \\tall\\t10000\\nnum_rel               \\tall\\t1415\\nnum_rel_ret           \\tall\\t1020\\nmap                   \\tall\\t0.3840\\ngm_map                \\tall\\t0.1684\\nRprec                 \\tall\\t0.3734\\nbpref                 \\tall\\t0.7819\\nrecip_rank            \\tall\\t0.6261\\niprec_at_recall_0.00  \\tall\\t0.6704\\niprec_at_recall_0.10  \\tall\\t0.5941\\niprec_at_recall_0.20  \\tall\\t0.5529\\niprec_at_recall_0.30  \\tall\\t0.4994\\niprec_at_recall_0.40  \\tall\\t0.4300\\niprec_at_recall_0.50  \\tall\\t0.3895\\niprec_at_recall_0.60  \\tall\\t0.3413\\niprec_at_recall_0.70  \\tall\\t0.3040\\niprec_at_recall_0.80  \\tall\\t0.2540\\niprec_at_recall_0.90  \\tall\\t0.2047\\niprec_at_recall_1.00  \\tall\\t0.1874\\nP_5                   \\tall\\t0.4080\\nP_10                  \\tall\\t0.3160\\nP_15                  \\tall\\t0.2587\\nP_20                  \\tall\\t0.2245\\nP_30                  \\tall\\t0.1863\\nP_100                 \\tall\\t0.1020\\nP_200                 \\tall\\t0.0510\\nP_500                 \\tall\\t0.0204\\nP_1000                \\tall\\t0.0102\\nrecall_5              \\tall\\t0.3244\\nrecall_10             \\tall\\t0.3983\\nrecall_15             \\tall\\t0.4340\\nrecall_20             \\tall\\t0.4677\\nrecall_30             \\tall\\t0.5202\\nrecall_100            \\tall\\t0.7819\\nrecall_200            \\tall\\t0.7819\\nrecall_500            \\tall\\t0.7819\\nrecall_1000           \\tall\\t0.7819\\ninfAP                 \\tall\\t0.3840\\ngm_bpref              \\tall\\t0.4975\\nRprec_mult_0.20       \\tall\\t0.4621\\nRprec_mult_0.40       \\tall\\t0.4464\\nRprec_mult_0.60       \\tall\\t0.4164\\nRprec_mult_0.80       \\tall\\t0.3937\\nRprec_mult_1.00       \\tall\\t0.3734\\nRprec_mult_1.20       \\tall\\t0.3132\\nRprec_mult_1.40       \\tall\\t0.2977\\nRprec_mult_1.60       \\tall\\t0.2743\\nRprec_mult_1.80       \\tall\\t0.2610\\nRprec_mult_2.00       \\tall\\t0.2523\\nutility               \\tall\\t-79.6000\\n11pt_avg              \\tall\\t0.4025\\nbinG                  \\tall\\t0.3787\\nG                     \\tall\\t0.3787\\nndcg                  \\tall\\t0.5836\\nndcg_rel              \\tall\\t0.5508\\nRndcg                 \\tall\\t0.4919\\nndcg_cut_5            \\tall\\t0.5177\\nndcg_cut_10           \\tall\\t0.4929\\nndcg_cut_15           \\tall\\t0.4781\\nndcg_cut_20           \\tall\\t0.4760\\nndcg_cut_30           \\tall\\t0.4859\\nndcg_cut_100          \\tall\\t0.5836\\nndcg_cut_200          \\tall\\t0.5836\\nndcg_cut_500          \\tall\\t0.5836\\nndcg_cut_1000         \\tall\\t0.5836\\nmap_cut_5             \\tall\\t0.2482\\nmap_cut_10            \\tall\\t0.2904\\nmap_cut_15            \\tall\\t0.3095\\nmap_cut_20            \\tall\\t0.3221\\nmap_cut_30            \\tall\\t0.3395\\nmap_cut_100           \\tall\\t0.3840\\nmap_cut_200           \\tall\\t0.3840\\nmap_cut_500           \\tall\\t0.3840\\nmap_cut_1000          \\tall\\t0.3840\\nrelative_P_5          \\tall\\t0.5515\\nrelative_P_10         \\tall\\t0.5207\\nrelative_P_15         \\tall\\t0.5049\\nrelative_P_20         \\tall\\t0.5084\\nrelative_P_30         \\tall\\t0.5354\\nrelative_P_100        \\tall\\t0.7819\\nrelative_P_200        \\tall\\t0.7819\\nrelative_P_500        \\tall\\t0.7819\\nrelative_P_1000       \\tall\\t0.7819\\nsuccess_1             \\tall\\t0.5000\\nsuccess_5             \\tall\\t0.8000\\nsuccess_10            \\tall\\t0.8800\\nset_P                 \\tall\\t0.1020\\nset_relative_P        \\tall\\t0.7819\\nset_recall            \\tall\\t0.7819\\nset_map               \\tall\\t0.0800\\nset_F                 \\tall\\t0.1636\\nnum_nonrel_judged_ret \\tall\\t0\\n'\n",
      "None\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    grid_search = 'yes'\n",
    "    \n",
    "    if grid_search == 'yes':\n",
    "        ## Heavy grid search\n",
    "        brange = np.arange(0.1,1,0.05)\n",
    "        krange = np.arange(0.1,4,0.1)\n",
    "        N_range = np.arange(5,500,1) # num of docs\n",
    "        M_range = np.arange(5,500,1) # num of terms\n",
    "        lamb_range = np.arange(0,1,0.1) # weights of original query\n",
    "\n",
    "        ## Light grid search\n",
    "#         brange = [0.2]\n",
    "#         krange = [0.8]\n",
    "#         N_range = np.arange(1,50,2)\n",
    "#         M_range = np.arange(1,50,2)\n",
    "#         lamb_range = np.arange(0,1,0.2)\n",
    "        \n",
    "        h_param_ranges = [brange, krange, N_range, M_range, lamb_range]\n",
    "        n_iters = 2\n",
    "        params = get_random_params(h_param_ranges, n_iters)\n",
    "\n",
    "    else:\n",
    "        brange = [0.2]\n",
    "        krange = [0.8]\n",
    "        N_range = [10]\n",
    "        M_range = [10]\n",
    "        lamb_range = [0.5]\n",
    "       \n",
    "        params = [[round(b,3), round(k,3), round(n_doc,3), round(n_term,3), round(w_ori_q,3)] \n",
    "                  for b in brange for k in krange for N in N_range for M in M_range for Lambda in lamb_range]\n",
    "    \n",
    "    pool_size = 2\n",
    "    print(len(params))\n",
    "    pool = multiprocessing.Pool(processes=pool_size,\n",
    "                                initializer=start_process,\n",
    "                                )\n",
    "    pool_outputs = pool.map(bm25_computing, params)\n",
    "    pool.close() # no more tasks\n",
    "    pool.join()  # wrap up current tasks\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "#     shutil.rmtree(corpus_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
