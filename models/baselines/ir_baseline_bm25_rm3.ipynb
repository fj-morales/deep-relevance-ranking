{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25 + RM3 with Anserini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import gzip\n",
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import re \n",
    "import shutil\n",
    "from itertools import islice\n",
    "import random\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sc(text):\n",
    "##     text = re.sub('[.,?;*!%^&_+():-\\[\\]{}]', '', text.replace('\"', '').replace('/', '').replace('\\\\', '').replace(\"'\", '').strip())\n",
    "##     text = re.sub('[\\[\\]{}.,?;*!%^&_+():-]', '', text.replace('\"', '').replace('/', '').replace('\\\\', '').replace(\"'\", '').strip()) # DeepPaper method\n",
    "    text = re.sub(r'[^\\w\\s]',' ',text) # My method\n",
    "##     text = text.rstrip('.?')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pickle_docs(pickle_filename):\n",
    "    # Pickle to Trectext converter\n",
    "    with open(pickle_filename, 'rb') as f_in:\n",
    "        data = pickle.load(f_in)\n",
    "        if not os.path.exists(baseline_files):\n",
    "            os.makedirs(baseline_files)\n",
    "        \n",
    "        if os.path.exists(corpus_files):\n",
    "            shutil.rmtree(corpus_files)\n",
    "            os.makedirs(corpus_files)\n",
    "        else:\n",
    "            os.makedirs(corpus_files)\n",
    "\n",
    "            \n",
    "        docs = {}\n",
    "        for key, value in data.items():\n",
    "            if \"pmid\" in value.keys():\n",
    "                doc_code = value.pop('pmid')\n",
    "            else:\n",
    "                doc_code = key\n",
    "                \n",
    "# Uncomment                 \n",
    "#             doc = '<DOC>\\n' + \\\n",
    "#                   '<DOCNO>' + doc_code + '</DOCNO>\\n' + \\\n",
    "#                   '<TITLE>' + value.pop('title') + '</TITLE>\\n' + \\\n",
    "#                   '<TEXT>' + value.pop('abstractText') + '</TEXT>\\n' + \\\n",
    "#                   '</DOC>\\n'\n",
    "            \n",
    "            doc = '<DOC>\\n' + \\\n",
    "                  '<DOCNO>' + doc_code + '</DOCNO>\\n' + \\\n",
    "                  '<TITLE>' + remove_sc(value.pop('title')) + '</TITLE>\\n' + \\\n",
    "                  '<TEXT>' + remove_sc(value.pop('abstractText')) + '</TEXT>\\n' + \\\n",
    "                  '</DOC>\\n'\n",
    "            docs[doc_code] = doc\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_trecfile(docs, filename, compression = 'yes'):\n",
    "    # Pickle to Trectext converter\n",
    "    doc_list = []\n",
    "    if compression == 'yes':\n",
    "        with gzip.open(filename,'wt', encoding='utf-8') as f_out:\n",
    "            docus = {}\n",
    "            for key, value in docs.items():\n",
    "                f_out.write(value)\n",
    "    else:\n",
    "        with open(filename,'wt', encoding='utf-8') as f_out:\n",
    "            docus = {}\n",
    "            for key, value in docs.items():\n",
    "                f_out.write(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build corpus index with Anserini\n",
    "def build_index(index_input, index_loc, log_file):\n",
    "    if not os.path.exists(index_loc):\n",
    "            os.makedirs(index_loc) \n",
    "#     index_loc_param = '--indexPath=' + index_loc\n",
    "\n",
    "    anserini_index = anserini_loc + 'target/appassembler/bin/IndexCollection'\n",
    "    anserini_parameters = [\n",
    "#                            'nohup', \n",
    "                           'sh',\n",
    "                           anserini_index,\n",
    "                           '-collection',\n",
    "                           'TrecCollection',\n",
    "                           '-generator',\n",
    "                           'JsoupGenerator',\n",
    "                           '-threads',\n",
    "                            '16',\n",
    "                            '-input',\n",
    "                           index_input,\n",
    "                           '-index',\n",
    "                           index_loc,\n",
    "                           '-storePositions',\n",
    "                            '-keepStopwords',\n",
    "                            '-storeDocvectors',\n",
    "                            '-storeRawDocs']\n",
    "#                           ' >& ',\n",
    "#                           log_file,\n",
    "#                            '&']\n",
    "\n",
    "\n",
    "\n",
    "#     anserini_parameters = ['ls',\n",
    "#                           index_loc]\n",
    "\n",
    "\n",
    "#     print(anserini_parameters)\n",
    "\n",
    "    index_proc = subprocess.Popen(anserini_parameters,\n",
    "            stdout=subprocess.PIPE, shell=False)\n",
    "    (out, err) = index_proc.communicate()\n",
    "#     print(out.decode(\"utf-8\"))\n",
    "#     print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_queries_file(queries, filename):\n",
    "    queries_list = []\n",
    "    queries_dict = {}\n",
    "    query = {}\n",
    "    q_dict = {}\n",
    "    q_trec = {}\n",
    "    ids_dict = {}\n",
    "    id_num = 0\n",
    "    for q in queries:\n",
    "        str_id = str(id_num)\n",
    "        id_new = str_id.rjust(15, '0')\n",
    "#         print(q['body'])\n",
    "#         text = q['body']\n",
    "        text = remove_sc(q['body'])\n",
    "#         print(text)\n",
    "    \n",
    "#         text = re.sub(r'[^\\w\\s]',' ',text)\n",
    "##     text = text.lower()\n",
    "##         text = text.rstrip('.?')\n",
    "    \n",
    "        q_dict[q['id']] = q['body']\n",
    "        query['id_new'] = id_new\n",
    "        query['number'] = q['id']\n",
    "        query['text'] = '#stopword(' + text + ')'\n",
    "        queries_list.append(dict(query))\n",
    "        q_t = '<top>\\n\\n' + \\\n",
    "              '<num> Number: ' + id_new + '\\n' + \\\n",
    "              '<title> ' + q['body'] + '\\n\\n' + \\\n",
    "              '<desc> Description:' + '\\n\\n' + \\\n",
    "              '<narr> Narrative:' + '\\n\\n' + \\\n",
    "              '</top>\\n\\n'\n",
    "        q_trec[q['id']] = q_t\n",
    "        ids_dict[str(id_num)] = q['id']\n",
    "        id_num += 1\n",
    "    queries_dict['queries'] = queries_list\n",
    "\n",
    "    with open(filename, 'wt', encoding='utf-8') as q_file:\n",
    "        json.dump(queries_dict, q_file, indent = 4)\n",
    "    \n",
    "    return [q_dict, q_trec, ids_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_docs(q_topics_file, retrieved_docs_file, index_loc, b_val=0.2, k_val=0.8, n_docs=10, n_terms=10, w_ori_q=0.5, hits=100):\n",
    "    \n",
    "    anserini_search = anserini_loc + 'target/appassembler/bin/SearchCollection'\n",
    "#     print(b_val)\n",
    "    command = [ \n",
    "               'sh',\n",
    "               anserini_search,\n",
    "               '-topicreader',\n",
    "                'Trec',\n",
    "                '-index',\n",
    "                index_loc,\n",
    "                '-topics',\n",
    "                q_topics_file,\n",
    "                '-output',\n",
    "                retrieved_docs_file,\n",
    "                '-bm25',\n",
    "                '-b',\n",
    "                str(b_val),\n",
    "                '-k1',\n",
    "                str(k_val),\n",
    "                '-rm3',\n",
    "                '-rm3.fbDocs',\n",
    "                str(n_docs),\n",
    "                '-rm3.fbTerms',\n",
    "                str(n_terms),\n",
    "                '-rm3.originalQueryWeight',\n",
    "                str(w_ori_q),\n",
    "                '-hits',\n",
    "                str(hits)\n",
    "               ]\n",
    "#     print(command)\n",
    "#     command = command.encode('utf-8')\n",
    "    anserini_exec = subprocess.Popen(command, stdout=subprocess.PIPE, shell=False, encoding='utf-8')\n",
    "    (out, err) = anserini_exec.communicate()\n",
    "###     print(out)\n",
    "# ##    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return top 100 bm25 scored docs, given query and corpus indexed by anserini\n",
    "\n",
    "def generate_preds_file(retrieved_docs_file, q_dict, ids_dict, hits=100):\n",
    "    \n",
    "    with open(retrieved_docs_file, 'rt') as f_in:\n",
    "        aux_var = -1\n",
    "        bm25_docs = []\n",
    "        while aux_var != 0:\n",
    "            question = {}\n",
    "            lines_gen = islice(f_in, hits)\n",
    "            documents = []\n",
    "            for line in lines_gen:\n",
    "                id_aux = line.split(' ')[0]\n",
    "                current_key = ids_dict[id_aux]\n",
    "                documents.append(line.split(' ')[2])\n",
    "                \n",
    "###             print(documents)\n",
    "            aux_var = len(documents)\n",
    "            if aux_var == 0: \n",
    "                break\n",
    "# ##            print(aux_var)##\n",
    "# ##            print(documents)\n",
    "            question['id'] = current_key\n",
    "            question['body'] = q_dict[current_key]\n",
    "            \n",
    "            if \"bioasq\" in dataset_name: \n",
    "                documents_url = ['http://www.ncbi.nlm.nih.gov/pubmed/' + doc for doc in documents]\n",
    "                question['documents'] = documents_url\n",
    "            elif \"rob04\" in dataset_name:\n",
    "                question['documents'] = documents\n",
    "            bm25_docs.append(dict(question))\n",
    "            \n",
    "    return bm25_docs        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docus = generate_preds_file(retrieved_docs_file, q_dict, ids_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(docus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_files = [os.path.join(root, name)\n",
    "             for root, dirs, files in os.walk(dataloc)\n",
    "             for name in files\n",
    "             if all(y in name for y in ['docset', split, '.pkl'])]\n",
    "\n",
    "# pkl_files = [ x for x in os.listdir(dataloc) if all(y in x for y in ['docset', '.pkl'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../bioasq_data/bioasq_bm25_docset_top100.test.pkl']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkl_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pickle to trectext file format to be processed with galago\n",
    "# pkl_file = [s for s in pkl_files if split in s]\n",
    "# [output_file, doc_list ]= pickle_to_json(pkl_file[0])\n",
    "doc_list = []\n",
    "output_files = []\n",
    "all_docs = []\n",
    "for pkl_file in pkl_files:\n",
    "###     print(pkl_file)\n",
    "    docs = get_pickle_docs(pkl_file)\n",
    "    doc_list = doc_list + list(docs.keys())\n",
    "    all_docs.append(docs)\n",
    "    out_name = pkl_file.split('/')[-1:][0]\n",
    "    out_name = re.sub('\\.pkl', '', out_name)\n",
    "    output_file = corpus_files + out_name + '.gz'\n",
    "    trec_doc_file = trec_storage + out_name\n",
    "    output_files.append(output_file)\n",
    "    ### print(out_name)\n",
    "    to_trecfile(docs, output_file)\n",
    "    to_trecfile(docs, trec_doc_file, 'no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random grid search sampling\n",
    "\n",
    "def get_random_params(hyper_params, num_iter):\n",
    "    random_h_params_list = []\n",
    "    while len(random_h_params_list) < num_iter:\n",
    "        random_h_params_set = []\n",
    "        for h_param_list in hyper_params:\n",
    "            sampled_h_param = random.sample(list(h_param_list), k=1)\n",
    "#             print(type(sampled_h_param[0]))\n",
    "#             print(sampled_h_param[0])\n",
    "            random_h_params_set.append(round(sampled_h_param[0], 3))\n",
    "        if not random_h_params_set in random_h_params_list:\n",
    "            random_h_params_list.append(random_h_params_set)\n",
    "#             print('Non repeated')\n",
    "        else:\n",
    "            print('repeated')\n",
    "    return random_h_params_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../bioasq_data/bioasq_bm25_docset_top100.test.pkl'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkl_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "bioasq\n"
     ]
    }
   ],
   "source": [
    "data_split = split\n",
    "print(data_split)\n",
    "\n",
    "if \"rob04\" in output_files[0]:\n",
    "    s = re.findall(\"(s[0-5]).pkl$\", pkl_file)\n",
    "    dataset_name = \"rob04\"\n",
    "    dataset_name_ext = dataset_name + '_'+ s[0]\n",
    "#     dataset_name_ext = dataset_name \n",
    "    gold_file = '../../robust04_data/rob04.' + split +'.json'\n",
    "#     with open(gold_file, 'w') as outfile:\n",
    "#         json.dump(query_data, outfile, indent = 4)\n",
    "    print(dataset_name_ext)\n",
    "elif \"bioasq\" in output_file:\n",
    "    print(\"bioasq\")\n",
    "    dataset_name = \"bioasq\"\n",
    "    dataset_name_ext = dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_loc = baseline_files + 'anserini_index' + '_' + dataset_name_ext + '_' + data_split\n",
    "# index_input = output_files\n",
    "index_input = corpus_files\n",
    "log_file = baseline_files + 'log_index_' + dataset_name_ext + '_' + data_split\n",
    "\n",
    "if build_index_flag == 'yes':\n",
    "    build_index(index_input, index_loc, log_file)\n",
    "    \n",
    "#     build_index(index_input, index_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./corpus_files/bioasq_bm25_docset_top100.test.gz'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_filename = [ x for x in os.listdir(dataloc) if all(y in x for y in [dataset_name +'.'+ data_split, '.json'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bioasq.test.json']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries_file = dataloc + q_filename[0]\n",
    "\n",
    "def load_queries(queries_file):\n",
    "    with open(queries_file, 'rb') as input_file:\n",
    "        query_data = json.load(input_file)\n",
    "        return query_data['questions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_files = [os.path.join(root, name)\n",
    "             for root, dirs, files in os.walk(dataloc)\n",
    "             for name in files\n",
    "             if all(y in name for y in [dataset_name +'.'+ data_split, '.json'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = []\n",
    "query_data = {}\n",
    "for file in query_files:\n",
    "    queries = queries + load_queries(file)\n",
    "# ##    print(queries)\n",
    "query_data['questions'] = queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_preds(file, preds):\n",
    "    with open(file, 'wt') as f_out:\n",
    "        json.dump(preds, f_out, indent=4)\n",
    "#     print('Predictions file: ' + file + ', done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../bioasq_data/bioasq.test.json'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_files[0].strip('split_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_process():\n",
    "    print( 'Starting', multiprocessing.current_process().name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_question(query):\n",
    "    question = {}\n",
    "    question['body'] = query['body']\n",
    "    question['id'] = query['id']\n",
    "###     print(query['body'].rstrip('.'))\n",
    "#     documents = get_bm25_docs(query['body'].rstrip('.'), index_loc)\n",
    "    documents = get_bm25_docs(query['body'], index_loc)\n",
    "    if \"bioasq\" in dataset_name: \n",
    "        documents_url = ['http://www.ncbi.nlm.nih.gov/pubmed/' + doc for doc in documents]\n",
    "        question['documents'] = documents_url\n",
    "    elif \"rob04\" in dataset_name:\n",
    "        question['documents'] = documents\n",
    "    return dict(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./baseline_files/anserini_index_bioasq_test'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_bm25_docs(query_data['questions'][0]['body'], index_loc)\n",
    "index_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_queries_file = baseline_files + 'bm25_queries_' + dataset_name_ext + '_' + data_split + '.json'\n",
    "[q_dict, q_trec, ids_dict]= generate_queries_file(queries,bm25_queries_file)\n",
    "\n",
    "q_topic_filename = dataset_name_ext + '_' + 'query_topics'  + '_' + data_split + '.txt'\n",
    "q_topics_file = baseline_files + q_topic_filename\n",
    "trec_q_topics_file = trec_storage + q_topic_filename\n",
    "\n",
    "to_trecfile(q_trec, q_topics_file, compression = 'no')\n",
    "to_trecfile(q_trec, trec_q_topics_file, compression = 'no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = 0.2\n",
    "# k = 0.8\n",
    "# retrieved_docs_file = baseline_files + 'bm25_preds_' + dataset_name_ext + '_' + data_split + '_' + 'b' + str(b) + 'k' + str(k) + '.txt'\n",
    "# retrieve_docs(q_topics_file, retrieved_docs_file, q_dict, index_loc, b_val=0.2, k_val=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_bioasq2treceval_qrels(bioasq_data, filename):\n",
    "    with open(filename, 'wt') as f:\n",
    "        for q in bioasq_data['questions']:\n",
    "            for d in q['documents']:\n",
    "                f.write('{0} 0 {1} 1'.format(q['id'], d))\n",
    "                f.write('\\n')\n",
    "\n",
    "def format_bioasq2treceval_qret(bioasq_data, system_name, filename):\n",
    "    with open(filename, 'wt') as f:\n",
    "        for q in bioasq_data['questions']:\n",
    "            rank = 1\n",
    "            for d in q['documents']:\n",
    "                \n",
    "                sim = (len(q['documents']) + 1 - rank) / float(len(q['documents']))\n",
    "                f.write('{0} {1} {2} {3} {4} {5}'.format(q['id'], 0, d, rank, sim, system_name))\n",
    "                f.write('\\n')\n",
    "                rank += 1\n",
    "\n",
    "def trec_evaluate(qrels_file, qret_file):\n",
    "    trec_eval_res = subprocess.Popen(\n",
    "#         ['./trec_eval', '-m', 'all_trec', qrels_file, qret_file],\n",
    "        ['./trec_eval', \n",
    "         '-m', 'map',\n",
    "         '-m', 'P.20',\n",
    "         '-m', 'ndcg_cut.20',\n",
    "         qrels_file, qret_file],\n",
    "        stdout=subprocess.PIPE, shell=False)\n",
    "\n",
    "    (out, err) = trec_eval_res.communicate()\n",
    "    trec_eval_res = out.decode(\"utf-8\")\n",
    "#     print(trec_eval_res)\n",
    "#     print(out)\n",
    "#     print(err)\n",
    "####     return trec_eval_res.split('\\tall\\t')\n",
    "    return trec_eval_res.strip('map').replace('P_20','').replace('ndcg_cut_20','').replace(' ','').replace('\\tall\\t','').split('\\n')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(golden_file, predictions_file):\n",
    "\n",
    "    system_name = predictions_file\n",
    "    \n",
    "    with open(golden_file, 'r') as f:\n",
    "        golden_data = json.load(f)\n",
    "\n",
    "    with open(predictions_file, 'r') as f:\n",
    "        predictions_data = json.load(f)\n",
    "\n",
    "    temp_dir = uuid.uuid4().hex\n",
    "    qrels_temp_file = '{0}/{1}'.format(temp_dir, 'qrels.txt')\n",
    "    qret_temp_file = '{0}/{1}'.format(temp_dir, 'qret.txt')\n",
    "\n",
    "    try:\n",
    "        if not os.path.exists(temp_dir):\n",
    "            os.makedirs(temp_dir)\n",
    "        else:\n",
    "            sys.exit(\"Possible uuid collision\")\n",
    "\n",
    "        format_bioasq2treceval_qrels(golden_data, qrels_temp_file)\n",
    "        format_bioasq2treceval_qret(predictions_data, system_name, qret_temp_file)\n",
    "\n",
    "        results = trec_evaluate(qrels_temp_file, qret_temp_file)\n",
    "    finally:\n",
    "#         print('something')\n",
    "        os.remove(qrels_temp_file)\n",
    "        os.remove(qret_temp_file)\n",
    "        os.rmdir(temp_dir)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_computing(params):\n",
    "    b = params[0]\n",
    "    k = params[1]\n",
    "    N = params[2]\n",
    "    M = params[3]\n",
    "    Lambda = params[4]\n",
    "#     b = 0.2\n",
    "#     k = 0.8\n",
    "    params_suffix = 'b' + str(b) + 'k' + str(k) + 'N' + str(N) + 'M' + str(M) + 'Lambda' + str(Lambda)\n",
    "\n",
    "    bm25_preds_file = baseline_files + 'bm25_rm3_preds_' + dataset_name_ext + '_' + data_split + '_' + params_suffix + '.json'\n",
    "    \n",
    "    ###     print(bm25_preds_file)\n",
    "    if os.path.isfile(bm25_preds_file):\n",
    "        print(bm25_preds_file + \"Already exists!!\")\n",
    "#         return\n",
    "    retrieved_docs_file = baseline_files + 'run_bm25_rm3_preds_' + dataset_name_ext + '_' + data_split + '_' + params_suffix + '.txt'\n",
    "    #print(b)\n",
    "    #print(k)\n",
    "    retrieve_docs(q_topics_file, retrieved_docs_file, index_loc, b, k, N, M, Lambda)\n",
    "    bm25_preds = {}\n",
    "    bm25_preds['questions'] = generate_preds_file(retrieved_docs_file, q_dict, ids_dict)\n",
    "\n",
    "    save_preds(bm25_preds_file, bm25_preds)  \n",
    "    \n",
    "    if 'rob04' in dataset_name_ext:\n",
    "        golden_file = dataloc + 'rob04.' + data_split + '.' + s[0]   + '.json'\n",
    "    else:\n",
    "        golden_file = dataloc + dataset_name_ext + '.' + data_split + '.json'\n",
    "    \n",
    "    if os.path.exists(golden_file):\n",
    "#         print('yes, we can evaluate!')    \n",
    "#         print(golden_file)    \n",
    "        [map_metric, p_20, ndcg_20] = evaluate(golden_file,bm25_preds_file)\n",
    "    else:\n",
    "        print('no, we cannot evaluate  :( !')  \n",
    "        \n",
    "#     results = {\n",
    "#         'b': b,\n",
    "#         'k': k,\n",
    "#         'N': N,\n",
    "#         'M': M,\n",
    "#         'Lambda': Lambda,\n",
    "#         'map': map_metric,\n",
    "#         'p_20': p_20,\n",
    "#         'ndcg_20': ndcg_20\n",
    "        \n",
    "#     }\n",
    "\n",
    "    results = [\n",
    "        b,\n",
    "        k,\n",
    "        N,\n",
    "        M,\n",
    "        Lambda,\n",
    "        float(map_metric),\n",
    "        float(p_20),\n",
    "        float(ndcg_20)\n",
    "    ]\n",
    "    os.remove(retrieved_docs_file)\n",
    "    os.remove(bm25_preds_file)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "def find_best_dev_model(best_model_params_file, random_iterations = 5000):\n",
    "#     random_search = 'yes'\n",
    "    \n",
    "    if random_search == 'yes':\n",
    "        ## Heavy random search\n",
    "        brange = np.arange(0.1,1,0.05)\n",
    "        krange = np.arange(0.1,4,0.1)\n",
    "        N_range = np.arange(5,500,1) # num of docs\n",
    "        M_range = np.arange(5,500,1) # num of terms\n",
    "        lamb_range = np.arange(0,1,0.1) # weights of original query\n",
    "\n",
    "        ## Light random search\n",
    "#         brange = [0.2]\n",
    "#         krange = [0.8]\n",
    "#         N_range = np.arange(1,50,2)\n",
    "#         M_range = np.arange(1,50,2)\n",
    "#         lamb_range = np.arange(0,1,0.2)\n",
    "        \n",
    "        h_param_ranges = [brange, krange, N_range, M_range, lamb_range]\n",
    "        params = get_random_params(h_param_ranges, random_iterations)\n",
    "\n",
    "    else:\n",
    "        brange = [0.2]\n",
    "        krange = [0.8]\n",
    "        N_range = [11]\n",
    "        M_range = [10]\n",
    "        lamb_range = [0.5]\n",
    "       \n",
    "        params = [[round(b,3), round(k,3), round(N,3), round(M,3), round(Lambda,3)] \n",
    "                  for b in brange for k in krange for N in N_range for M in M_range for Lambda in lamb_range]\n",
    "    \n",
    "    pool_size = 20\n",
    "#     print(len(params))\n",
    "    pool = multiprocessing.Pool(processes=pool_size,\n",
    "                                initializer=start_process,\n",
    "                                )\n",
    "\n",
    "#     pool_outputs = pool.map(bm25_computing, params)\n",
    "    \n",
    "\n",
    "    pool_outputs = pool.map_async(bm25_computing, params)\n",
    "#     pool_outputs.get()\n",
    "    ###\n",
    "\n",
    "    \n",
    "    ##\n",
    "    \n",
    "    \n",
    "    pool.close() # no more tasks\n",
    "    while (True):\n",
    "        if (pool_outputs.ready()): break\n",
    "        remaining = pool_outputs._number_left\n",
    "        time.sleep(2)\n",
    "        print(\"Waiting for\", remaining, \"tasks to complete...\")\n",
    "      \n",
    "    pool.join()  # wrap up current tasks\n",
    "    pool_outputs.get()\n",
    "    best_model_params = max(pool_outputs.get(), key=lambda x: x[5])\n",
    "    \n",
    "    best_model_dict = {\n",
    "        'b': best_model_params[0],\n",
    "        'k': best_model_params[1],\n",
    "        'N': best_model_params[2],\n",
    "        'M': best_model_params[3],\n",
    "        'Lambda': best_model_params[4],\n",
    "        'random_iterations': random_iterations,\n",
    "        'map': best_model_params[5],\n",
    "        'p_20': best_model_params[6],\n",
    "        'ndcg_20': best_model_params[7]\n",
    "        \n",
    "    }\n",
    "    best_model_dict = {k:str(v) for k, v in best_model_dict.items()} # everything to string\n",
    "    \n",
    "    with open(best_model_params_file, 'wt') as best_model_f:\n",
    "        json.dump(best_model_dict, best_model_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_metrics(best_model_params_file):\n",
    "    try:\n",
    "        with open(best_model_params_file, 'rt') as best_model_in:\n",
    "            best_dev_params = json.load(best_model_in)\n",
    "    #         print(best_dev_model_params)\n",
    "    #         best_dev_params = {k:float(v) for k, v in best_dev_params.items()}\n",
    "        params = [best_dev_params['b'],\n",
    "                  best_dev_params['k'],\n",
    "                  best_dev_params['N'],\n",
    "                  best_dev_params['M'],\n",
    "                  best_dev_params['Lambda']\n",
    "                 ]\n",
    "        test_results = bm25_computing(params)\n",
    "        return test_results\n",
    "    except:\n",
    "        print('No dev model file. Run Dev model first!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model_params_file = baseline_files + dataset_name_ext + '_bm25_rm3_best_model_test.json'\n",
    "# find_best_dev_model(best_model_params_file, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.45', '0.4', '458', '488', '0.4', 0.4742, 0.2727, 0.5367]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        dataloc = sys.argv[1]\n",
    "        split = sys.argv[2]\n",
    "        random_iter = sys.argv[3]\n",
    "    except:\n",
    "        sys.exit(\"Provide data location, split, and number of random iterations\")\n",
    "    \n",
    "    ## Options\n",
    "\n",
    "    # search best b and k now?\n",
    "    random_search = 'yes' \n",
    "    # random_search = 'no' \n",
    "\n",
    "    # build index? \n",
    "    build_index_flag = 'yes'\n",
    "    # build_index_flag = 'no'\n",
    "\n",
    "    # N of workers for multiprocessing used random_search\n",
    "    pool_size = 20\n",
    "\n",
    "    hits = 100\n",
    "    \n",
    "    \n",
    "    # Define paths\n",
    "#     dataloc = '../../bioasq_data/'\n",
    "    # dataloc = '../../robust04_data/split_1/'\n",
    "    baseline_files ='./baseline_files/'\n",
    "    corpus_files ='./corpus_files/'\n",
    "    galago_loc='./galago-3.10-bin/bin/'\n",
    "    anserini_loc = '../../../anserini/'\n",
    "\n",
    "    ## TREC storage\n",
    "    trec_storage = '/ssd/francisco/trec_datasets/deep-relevance-ranking/'\n",
    "    \n",
    "    \n",
    "    # Select data split to work with\n",
    "#     split = \"test\"\n",
    "    # split = \"dev\"\n",
    "    # split = \"train\"\n",
    "    \n",
    "    \n",
    "    best_model_params_file = baseline_files + dataset_name_ext + '_bm25_rm3_best_model_dev.json'\n",
    "    ## best_model_params_file = baseline_files + dataset_name_ext + '_bm25_rm3_best_model_'+ split + '.json'\n",
    "    ## find_best_dev_model(best_model_params_file, 2)\n",
    "    if 'dev' in split:\n",
    "        find_best_dev_model(best_model_params_file, int(random_iter))\n",
    "    if 'test' in split:\n",
    "        test_results = get_test_metrics(best_model_params_file)\n",
    "        print(test_results)\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
