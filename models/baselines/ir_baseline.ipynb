{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting pkl files to json files to be read in galago tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import gzip\n",
    "import os\n",
    "import subprocess\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "dataloc = '/mnt/warehouse/data/bioasq_data/'\n",
    "# dataloc = '/mnt/warehouse/data/robust04_data/split_2/'\n",
    "baseline_files ='./baseline_files/'\n",
    "galago_loc='/opt/galago-3.10-bin/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_to_json(pickle_filename):\n",
    "    # Pickle to Trectext converter\n",
    "    doc_list = []\n",
    "    with open(dataloc + pickle_filename, 'rb') as f_in:\n",
    "        data = pickle.load(f_in)\n",
    "        if not os.path.exists(baseline_files):\n",
    "            os.makedirs(baseline_files)\n",
    "        out_file = baseline_files + pickle_filename[:-4] + '.gz'\n",
    "        with gzip.open(out_file,'wt', encoding='utf-8') as f_out:\n",
    "            docu = {}\n",
    "            for key, value in data.items():\n",
    "                if \"pmid\" in value.keys():\n",
    "                    doc_code = value.pop('pmid')\n",
    "                else:\n",
    "                    doc_code = key\n",
    "                f_out.write('<DOC>\\n' + \n",
    "                            '<DOCNO>' + doc_code + '</DOCNO>\\n' +\n",
    "                            '<TITLE>' + value.pop('title') + '</TITLE>\\n' +\n",
    "                            '<TEXT>' + value.pop('abstractText') + '</TEXT>\\n' + \n",
    "                            '</DOC>\\n')\n",
    "                doc_list.append(doc_code)\n",
    "        return [out_file, doc_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build corpus index \n",
    "def build_index(index_input, index_loc):\n",
    "    index_input_param = '--inputPath+' + index_input    \n",
    "    index_loc_param = '--indexPath=' + index_loc\n",
    "    print(index_input_param)\n",
    "    print(index_loc_param)\n",
    "    if not os.path.exists(index_loc):\n",
    "            os.makedirs(index_loc) \n",
    "    index_proc = subprocess.Popen(\n",
    "            [galago_loc + 'galago', 'build', '--stemmer+krovetz',\n",
    "                index_input_param, index_loc_param],\n",
    "            stdout=subprocess.PIPE, shell=False)\n",
    "    (out, err) = index_proc.communicate()\n",
    "    print(out.decode(\"utf-8\"))\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Return top 100 bm25 scored docs, given query and corpus indexed by galago\n",
    "# def get_bm25_docs(query, index_loc):\n",
    "#     index_loc_param = '--index=' + index_loc   \n",
    "#     if \"'\" in query:\n",
    "#         query_param = '--query=\"#stopword(' + query.rstrip('.') + ')\"' \n",
    "#     else:\n",
    "#         query_param = '--query=\\'#stopword(' + query.rstrip('.') + ')\\'' \n",
    "        \n",
    "# #     print(query_param)\n",
    "\n",
    "#     command = galago_loc + 'galago batch-search --verbose=false --requested=100 ' + \\\n",
    "#          index_loc_param + ' --scorer=bm25 --stemmer+krovetz ' + \\\n",
    "#          query_param + ' | cut -d\" \" -f3'\n",
    "#     galago_bm25_exec = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True)\n",
    "#     (out, err) = galago_bm25_exec.communicate()\n",
    "#     bm25_documents = out.decode(\"utf-8\")\n",
    "#     return bm25_documents.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return top 100 bm25 scored docs, given query and corpus indexed by galago\n",
    "def get_bm25_docs(query, index_loc, b_val, k_val):\n",
    "    index_loc_param = '--index=' + index_loc  \n",
    "    b=' --b=' + str(b_val)\n",
    "    k=' --k=' + str(k_val)\n",
    "    if \"'\" in query:\n",
    "        query_param = '--query=\"#stopword(' + query.rstrip('.') + ')\"' \n",
    "    else:\n",
    "        query_param = '--query=\\'#stopword(' + query.rstrip('.') + ')\\'' \n",
    "\n",
    "    command = galago_loc + 'galago batch-search --verbose=false --requested=100 ' + \\\n",
    "         index_loc_param + ' --scorer=bm25' + \\\n",
    "         b + \\\n",
    "         k + \\\n",
    "         ' --stemmer+krovetz ' + \\\n",
    "         query_param + ' | cut -d\" \" -f3'\n",
    "#     print(command)\n",
    "    galago_bm25_exec = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True)\n",
    "    (out, err) = galago_bm25_exec.communicate()\n",
    "    bm25_documents = out.decode(\"utf-8\")\n",
    "    return bm25_documents.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ## Testing (remove)\n",
    "\n",
    "# import numpy as np\n",
    "# index_loc = '/home/fmorales/msc_project/not-a-punching-bag/reproduction/deep-relevance-ranking/models/baselines/baseline_files/index_bioasq_test'\n",
    "# # # q = 'Has \\\"RNA interference\\\" been awarded Nobel prize.'\n",
    "# # # q = 'Describe Wellens\\' Syndrome.'\n",
    "# # # q = 'Can the Micro-C XL method achieve mononucleosome resolution?'\n",
    "# # # q = 'What is the role of the UBC9 enzyme in the protein sumoylation pathway?'\n",
    "# # # q = \"Has \\\"RNA interference\\\" been awarded Nobel prize?\"\n",
    "# q = \"What is the role of gamma-secreatase complex in Alzheimer's Disease?\"\n",
    "# # q = \"List diseases associated with the  Dopamine Receptor D4 (DRD4).\"\n",
    "# # \"List the classical symptoms of the Moschcowitz syndrome (Thrombotic thrombocytopenic purpura).\"\n",
    "# # q = \"List the classical symptoms of the Moschcowitz syndrome (Thrombotic thrombocytopenic purpura).\"\n",
    "\n",
    "# b_range = np.arange(0.4, 0.9, 0.1)\n",
    "# k_range = np.arange(1,2.5,0.2)\n",
    "# for b in b_range:\n",
    "#     for k in k_range:\n",
    "#         print(round(b,1),round(k,1))\n",
    "#         valor = get_bm25_docs(q, index_loc, round(b,1), round(k,1))\n",
    "#         print(valor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_files = [ x for x in os.listdir(dataloc) if all(y in x for y in ['docset', '.pkl'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bioasq_bm25_docset_top100.dev.pkl',\n",
       " 'bioasq_bm25_docset_top100.test.pkl',\n",
       " 'bioasq_bm25_docset_top100.train.pkl']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkl_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pickle to trectext file formar to be processed with galago\n",
    "for pkl_file in pkl_files[0:1]:\n",
    "    [output_file, doc_list ]= pickle_to_json(pkl_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev\n",
      "bioasq\n"
     ]
    }
   ],
   "source": [
    "if \"dev\" in output_file:\n",
    "    print(\"dev\")\n",
    "    data_split = \"dev\"\n",
    "elif \"test\" in output_file:\n",
    "    print(\"test\")\n",
    "    data_split = \"test\"\n",
    "elif \"train\" in output_file:\n",
    "    print(\"train\")\n",
    "    data_split = train\n",
    "\n",
    "    \n",
    "if \"rob04\" in output_file:\n",
    "    print(\"rob04\")\n",
    "    dataset_name = \"rob04\"\n",
    "elif \"bioasq\" in output_file:\n",
    "    print(\"bioasq\")\n",
    "    dataset_name = \"bioasq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--inputPath+./baseline_files/bioasq_bm25_docset_top100.dev.gz\n",
      "--indexPath=./baseline_files/index_bioasq_dev\n",
      "Running without server!\n",
      "Use --server=true to enable web-based status page.\n",
      "/mnt/warehouse/CODE/msc_project/not-a-punching-bag/reproduction/deep-relevance-ranking/models/baselines/./baseline_files/bioasq_bm25_docset_top100.dev.gz detected as trectext\n",
      "Done Indexing.\n",
      "  - 0.01 Hours\n",
      "  - 0.79 Minutes\n",
      "  - 47.65 Seconds\n",
      "Documents Indexed: 9945.\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "index_loc = baseline_files + 'index' + '_' + dataset_name + '_' + data_split\n",
    "index_input = output_file\n",
    "build_index(index_input, index_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_filename = [ x for x in os.listdir(dataloc) if all(y in x for y in [dataset_name +'.'+ data_split, '.json'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bioasq.dev.json']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_file = dataloc + q_filename[0]\n",
    "with open(queries_file, 'rb') as input_file:\n",
    "    query_data = json.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_preds(file, preds):\n",
    "    with open(file, 'wt') as f_out:\n",
    "        json.dump(preds, f_out, indent=4)\n",
    "    print('Predictions file: ' + file + ', done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./baseline_files/index_bioasq_dev\n"
     ]
    }
   ],
   "source": [
    "print(index_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions file: ./baseline_files/bm25_preds.b0.4k1.1bioasqdev.json, done!\n",
      "Predictions file: ./baseline_files/bm25_preds.b0.4k1.5bioasqdev.json, done!\n",
      "Predictions file: ./baseline_files/bm25_preds.b0.4k1.9bioasqdev.json, done!\n",
      "Predictions file: ./baseline_files/bm25_preds.b0.6k1.1bioasqdev.json, done!\n",
      "Predictions file: ./baseline_files/bm25_preds.b0.6k1.5bioasqdev.json, done!\n",
      "Predictions file: ./baseline_files/bm25_preds.b0.6k1.9bioasqdev.json, done!\n",
      "Predictions file: ./baseline_files/bm25_preds.b0.8k1.1bioasqdev.json, done!\n",
      "Predictions file: ./baseline_files/bm25_preds.b0.8k1.5bioasqdev.json, done!\n",
      "Predictions file: ./baseline_files/bm25_preds.b0.8k1.9bioasqdev.json, done!\n"
     ]
    }
   ],
   "source": [
    "brange = np.arange(0.4,0.9,0.2)\n",
    "krange = np.arange(1.1,2,0.4)\n",
    "\n",
    "for b in brange:\n",
    "    b = round(b,1)\n",
    "    for k in krange:\n",
    "        k = round(k,1)\n",
    "        bm25_preds = {}\n",
    "        questions = []\n",
    "        question = {}\n",
    "        for query in query_data['questions']:\n",
    "            question['body'] = query['body']\n",
    "            question['id'] = query['id']\n",
    "        #     print(query['body'].rstrip('.'))\n",
    "        #     documents = get_bm25_docs(query['body'].rstrip('.'), index_loc)\n",
    "            documents = get_bm25_docs(query['body'], index_loc, b, k)\n",
    "            if \"bioasq\" in dataset_name: \n",
    "                documents_url = ['http://www.ncbi.nlm.nih.gov/pubmed/' + doc for doc in documents]\n",
    "                question['documents'] = documents_url\n",
    "            elif \"rob04\" in dataset_name:\n",
    "                question['documents'] = documents\n",
    "            questions.append(dict(question))\n",
    "        bm25_preds_file = baseline_files + 'bm25_preds.' + 'b' + str(b) + 'k' + str(k) + dataset_name + data_split + '.json'\n",
    "        bm25_preds['questions'] = questions\n",
    "        save_preds(bm25_preds_file, bm25_preds)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brange = np.arange(0.4,0.9,0.2)\n",
    "krange = np.arange(1.1,2,0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[1.1 1.5 1.9]'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(krange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments (remove safely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "random_preds = {}\n",
    "questions = []\n",
    "question = {}\n",
    "for query in query_data['questions']:\n",
    "    question['body'] = query['body']\n",
    "    question['id'] = query['id']\n",
    "#     print(query['body'].rstrip('.'))\n",
    "#     documents = get_bm25_docs(query['body'].rstrip('.'), index_loc)\n",
    "    documents = sample(doc_list, 100)\n",
    "    if \"bioasq\" in dataset_name: \n",
    "        documents_url = ['http://www.ncbi.nlm.nih.gov/pubmed/' + doc for doc in documents]\n",
    "        question['documents'] = documents_url\n",
    "    elif \"rob04\" in dataset_name:\n",
    "        question['documents'] = documents\n",
    "    questions.append(dict(question))\n",
    "    \n",
    "random_preds['questions'] = questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_preds_file = baseline_files + 'random_preds.' + dataset_name + data_split + '.json'\n",
    "with open(random_preds_file, 'wt') as f_out:\n",
    "    json.dump(random_preds, f_out, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46844"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
