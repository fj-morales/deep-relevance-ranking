{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ir_baseline.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# import pickle\n",
    "import json\n",
    "# import gzip\n",
    "import os\n",
    "import subprocess\n",
    "# import numpy as np\n",
    "# import multiprocessing\n",
    "# import re \n",
    "# import csv\n",
    "# import torch\n",
    "import sys\n",
    "# import shutil\n",
    "# import random\n",
    "\n",
    "# import argparse\n",
    "\n",
    "# import uuid\n",
    "# import datetime\n",
    "# import time\n",
    "\n",
    "# import bz2\n",
    "# import pandas as pd\n",
    "# # import dbmanager  as dbmanager\n",
    "# from os.path import join\n",
    "\n",
    "## My libraries\n",
    "\n",
    "import utils\n",
    "import bioasq_corpus_parser\n",
    "import bioasq_query_parser\n",
    "# from utils import *\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "class Index:\n",
    "    def __init__(self, ir_toolkit_location, parameter_file_location):\n",
    "        self.ir_toolkit_location = ir_toolkit_location\n",
    "        self.parameter_file_location = parameter_file_location\n",
    "#     def build(self, ir_tool_params):\n",
    "    def build(self):\n",
    "        \n",
    "#         utils.create_dir(self.index_location)\n",
    "    #     index_loc_param = '--indexPath=' + index_loc\n",
    "        stopwords_file = './stopwords'\n",
    "        build_index_command = self.ir_toolkit_location + 'bin/IndriBuildIndex'\n",
    "        toolkit_parameters = [\n",
    "                                build_index_command,\n",
    "                                self.parameter_file_location,\n",
    "                                stopwords_file\n",
    "                                ]\n",
    "\n",
    "        print(toolkit_parameters)\n",
    "\n",
    "        proc = subprocess.Popen(toolkit_parameters,stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=False)\n",
    "        (out, err) = proc.communicate()\n",
    "        print(out.decode(\"utf-8\"))\n",
    "        print('Index error: ', err)\n",
    "        if err == None:\n",
    "            return 'Ok'\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "class Query:\n",
    "    def __init__(self, ir_toolkit_location, query_file, query_parameter_file, run_filename):\n",
    "        self.ir_toolkit_location = ir_toolkit_location\n",
    "        self.query_file = query_file\n",
    "        self.query_parameter_file = query_parameter_file\n",
    "        self.run_filename = run_filename\n",
    "        \n",
    "#     def build(self, ir_tool_params):\n",
    "    def run(self):\n",
    "        \n",
    "#         utils.create_dir(self.index_location)\n",
    "    \n",
    "        stopwords_file = './stopwords'\n",
    "        query_command = self.ir_toolkit_location + 'bin/IndriRunQuery'\n",
    "        toolkit_parameters = [\n",
    "                                query_command,\n",
    "                                self.query_file,\n",
    "                                self.query_parameter_file,\n",
    "                                stopwords_file]\n",
    "        print(toolkit_parameters)\n",
    "        with open(self.run_filename, 'wt') as rf:\n",
    "#             proc = subprocess.Popen(toolkit_parameters,stdin=subprocess.PIPE, stdout=rf, stderr=subprocess.STDOUT, shell=False)\n",
    "            proc = subprocess.Popen(toolkit_parameters,stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=False)\n",
    "            proc2 = subprocess.Popen(['grep', '^.*[ ]Q0[ ]'],stdin=proc.stdout, stdout=rf, stderr=subprocess.STDOUT, shell=False)\n",
    "            (out, err)= proc2.communicate()\n",
    "\n",
    "            print('Run error: ', err)\n",
    "            if err == None:\n",
    "                return 'Ok'\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def eval(trec_eval_command, qrel, qret):\n",
    "    \n",
    "    metrics = '-m map -m P.20 -m ndcg_cut.20'\n",
    "    toolkit_parameters = [\n",
    "                            trec_eval_command,\n",
    "                            '-m',\n",
    "                            'map',\n",
    "                            '-m',\n",
    "                            'P.20',\n",
    "                            '-m',\n",
    "                            'ndcg_cut.20',\n",
    "                            qrel,\n",
    "                            qret]\n",
    "\n",
    "    print(toolkit_parameters)\n",
    "\n",
    "    proc = subprocess.Popen(toolkit_parameters, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=False)\n",
    "    (out, err) = proc.communicate()\n",
    "    print(out.decode(\"utf-8\"))\n",
    "    print('Run error: ', err)\n",
    "    if err == None:\n",
    "        return 'Ok'\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Specific for BioASQ\n",
    "# for BioASQ only\n",
    "\n",
    "def get_doc_year(corpus_dir):\n",
    "    doc_year_files = [os.path.join('./',root, name)\n",
    "             for root, dirs, files in os.walk(corpus_dir)\n",
    "             for name in files\n",
    "             if all(y in name for y in ['year'])]\n",
    "#     print(doc_year_files)\n",
    "    \n",
    "    doc_years_dict = {}\n",
    "    for doc_year in doc_year_files:\n",
    "        with open(doc_year, 'rt') as dy_f:\n",
    "            dy_dict = json.load(dy_f)\n",
    "            doc_years_dict.update(dy_dict)\n",
    "    return doc_years_dict\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Specific for BioASQ\n",
    "# for BioASQ only\n",
    "\n",
    "def filter_year(run_filename, run_filename_filtered, doc_years_dict):\n",
    "    if 'train' in run_filename:\n",
    "        filter_year = 2015\n",
    "    else:\n",
    "        filter_year = 2016\n",
    "    with open(run_filename, 'rt') as rf:\n",
    "        run_dict = {}\n",
    "        doc_rank = 0\n",
    "        for line in rf:\n",
    "\n",
    "            elem = line.split(' ')\n",
    "            q_id = elem[0]\n",
    "            doc = elem[2]\n",
    "            \n",
    "            doc_score = elem[4]\n",
    "            doc_year = doc_years_dict[doc]            \n",
    "            if int(doc_year) <= filter_year:\n",
    "                if q_id in run_dict:\n",
    "                    doc_rank += 1\n",
    "                    if doc_rank == 100:\n",
    "                        pass\n",
    "#                         print('orig: ', elem[3])                        \n",
    "                    if doc_rank > 100:\n",
    "                        continue\n",
    "                    docu = [q_id, doc, doc_rank, doc_score, doc_year]\n",
    "                    run_dict[q_id].append(docu)\n",
    "                else:\n",
    "                    if doc_rank < 100:\n",
    "                        try: \n",
    "#                             print('previous: ',  docu)\n",
    "                            pass\n",
    "                        except: \n",
    "                            pass\n",
    "                    doc_rank = 1\n",
    "                    docu = [q_id, doc, doc_rank, doc_score, doc_year]\n",
    "                    run_dict[q_id] = [docu]\n",
    "\n",
    "        with open(run_filename_filtered, 'wt') as filter_f:\n",
    "            for key, value in run_dict.items():\n",
    "                for val in value:\n",
    "                    filter_f.write(val[0] + ' Q0 ' + val[1] + ' ' + str(val[2]) + ' ' + str(val[3]) + ' indri\\n')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "# #     ir_toolkit_location = sys.argv[1] # '../indri/'\n",
    "# #     parameter_file_location = sys.argv[2] # './bioasq_index_param_file'\n",
    "\n",
    "#     # create dataset files dir\n",
    "    \n",
    "    \n",
    "    \n",
    "    dataset = sys.argv[1] # 'bioasq'\n",
    "    workdir = './' + dataset + '_dir/'\n",
    "    split = sys.argv[2] # 'test'\n",
    "    \n",
    "    try:\n",
    "        build_index_flag = sys.argv[3] # True\n",
    "    except:\n",
    "        build_index_flag = False\n",
    "    \n",
    "#     # generate corpus files to index\n",
    "    \n",
    "    pool_size = 40 # scales very well when server is not used\n",
    "#     # Get all filenames\n",
    "    \n",
    "#     # Options\n",
    "#     data_dir = '/ssd/francisco/pubmed19-test/'\n",
    "    data_dir = '/ssd/francisco/pubmed19/'\n",
    "    \n",
    "    \n",
    "    to_index_dir =  workdir + dataset + '_corpus/'\n",
    "    index_dir = workdir + dataset + '_indri_index'\n",
    "\n",
    "    ir_toolkit_location = '../../../indri/'\n",
    "    trec_eval_command = '../../eval/trec_eval'\n",
    "    parameter_file_location = workdir + 'bioasq_index_param_file'\n",
    "\n",
    "    \n",
    "#     # Generate query files\n",
    "    \n",
    "    if build_index_flag == True: \n",
    "        \n",
    "        \n",
    "        utils.create_dir(to_index_dir)\n",
    "        utils.create_dir(index_dir)\n",
    "\n",
    "        # Parse Pubmed (BioASQ) dataset\n",
    "        bioasq_corpus_parser.corpus_parser(data_dir, to_index_dir, pool_size) # time consuming\n",
    "\n",
    "        index_data = Index(ir_toolkit_location, parameter_file_location)\n",
    "        index_data.build() # time consuming\n",
    "\n",
    "    \n",
    "#     # Generate qrels and qret\n",
    "    \n",
    "    queries_file = '../../bioasq_data/bioasq.' + split + '.json'\n",
    "\n",
    "    prefix = queries_file.split('/')[-1].strip('.json')\n",
    "    filename_prefix = workdir + prefix\n",
    "    \n",
    "#     print(filename_prefix)\n",
    "    \n",
    "    trec_query_file = filename_prefix + '_trec_query'\n",
    "    qrels_file = filename_prefix + '_qrels'\n",
    "    \n",
    "    bioasq_query_parser.query_parser(queries_file, trec_query_file, qrels_file) # fast\n",
    "    \n",
    "#     # Run query\n",
    "    \n",
    "\n",
    "    run_filename = workdir + 'run_' + prefix\n",
    "    query_parameter_file = workdir + dataset + '_query_params'\n",
    "    \n",
    "\n",
    "    bm25_query = Query(ir_toolkit_location, trec_query_file, query_parameter_file, run_filename)\n",
    "    bm25_query.run() # fast\n",
    "    \n",
    "    # BIOASQ: Filter docus by year\n",
    "    doc_years_dict = get_doc_year(to_index_dir)\n",
    "    run_filename_filtered = run_filename + '_filtered'\n",
    "    filter_year(run_filename, run_filename_filtered, doc_years_dict)\n",
    "#     # Eval\n",
    "    eval(trec_eval_command, qrels_file, run_filename_filtered)    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
