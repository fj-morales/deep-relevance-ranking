{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import argparse\n",
    "from ir_utils import load_queries\n",
    "\n",
    "\n",
    "# Models:\n",
    "\n",
    "from ir_lmart import *\n",
    "\n",
    "# HPO\n",
    "\n",
    "from hpo import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_budget_data_file(budget, query_list, train_data_file):\n",
    "    # Budget is percentage of training data: \n",
    "    # min_budget = 10%\n",
    "    # max_budget = 100%\n",
    "    if (int(budget) <= 100 or int(budget) >= 10):\n",
    "        len_queries = len(query_list)\n",
    "        budgeted_queries = round(len_queries * (budget / 100))\n",
    "        print('total budget:', len_queries)\n",
    "        print('allocated budget:', budgeted_queries)\n",
    "        train_budget_queries_file = train_data_file + '_budget' + str(budget)\n",
    "        if not os.path.exists(train_budget_queries_file):\n",
    "            with open(train_data_file, 'rt') as f_in:\n",
    "                with open(train_budget_queries_file, 'wt') as budget_file_out:\n",
    "                    for query_feature in f_in:                        \n",
    "                        qid = query_feature.split()[1].split(':')[1]\n",
    "#                         print(qid)\n",
    "                        if qid in query_list[0:budgeted_queries]:\n",
    "                            \n",
    "                            budget_file_out.write(query_feature)\n",
    "        else:\n",
    "            print(\"File already exists\")\n",
    "            return train_budget_queries_file                \n",
    "    else:\n",
    "        print('Budget is outside the limits (10% < b < 100%): ', budget)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_hpo(params_list, hpo_params):\n",
    "    \n",
    "    train_data_file = params_list[0]\n",
    "    val_data_file = params_list[1]\n",
    "    fold_dir = params_list[2]\n",
    "    lmart_model = params_list[3]\n",
    "    qrels_val_file = params_list[4]\n",
    "    \n",
    "    hpo_params_suffix = 'nl' + str(hpo_params['n_leaves']) + 'lr' + str(hpo_params['learning_rate']) + 'nt' + str(hpo_params['n_trees'])\n",
    "    \n",
    "    save_model_file = fold_dir + dataset_fold + '_lmart_' + hpo_params_suffix + '_model' \n",
    "    \n",
    "    lmart_model.train(train_data_file, save_model_file, hpo_params)\n",
    "    run_file = fold_dir + 'run_' + dataset_fold + '_lmart_' + hpo_params_suffix\n",
    "    \n",
    "#   lmart_model.gen_run_file(test_data_file, run_file)\n",
    "\n",
    "    lmart_model.gen_run_file(val_data_file, run_file)\n",
    "    \n",
    "#     print(qrels_val_file)\n",
    "#     print(run_file)\n",
    "    eval_results = eval(trec_eval_command, qrels_val_file, run_file)\n",
    "    eval_results.update(lmart_model.hpo_config)\n",
    "    eval_results['lmart_model'] = lmart_model\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_process():\n",
    "    print( 'Starting', multiprocessing.current_process().name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_multi_hpo(params_list, hpo_params_list, pool_size):\n",
    "   \n",
    "    eval_hpo_partial = partial(eval_hpo, params_list)\n",
    "\n",
    "    pool = multiprocessing.Pool(processes=pool_size,\n",
    "                                initializer=start_process,\n",
    "                                )\n",
    "\n",
    "    pool_outputs = pool.map_async(eval_hpo_partial, hpo_params_list)\n",
    "    pool.close() # no more tasks\n",
    "    pool.join()  # wrap up current tasks\n",
    "    print('Total parameters: ' + str(len(pool_outputs.get())))\n",
    "    return pool_outputs.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fakeParser:\n",
    "    def __init__(self):\n",
    "        self.dataset = 'bioasq' \n",
    "#         self.dataset = 'robust' \n",
    "        self.data_split = 'all'\n",
    "#         self.data_split = 'train'\n",
    "#         self.data_split = 'dev'\n",
    "#         self.build_index = True\n",
    "        self.build_index = None\n",
    "        self.fold = 'all'\n",
    "        self.gen_features = True\n",
    "#         self.gen_features = None\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='Example 1 - sequential and local execution.')\n",
    "    parser.add_argument('--dataset',   type=str, help='')\n",
    "    parser.add_argument('--data_split',   type=str, help='')\n",
    "    parser.add_argument('--fold', type=str,   help='')\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='Example 1 - sequential and local execution.')\n",
    "    parser.add_argument('--min_budget',   type=float, help='Minimum budget used during the optimization.',    default=2)\n",
    "    parser.add_argument('--max_budget',   type=float, help='Maximum budget used during the optimization.',    default=4)\n",
    "    parser.add_argument('--n_iterations', type=int,   help='Number of iterations performed by the optimizer', default=500)\n",
    "    parser.add_argument('--n_workers', type=int,   help='Number of workers to run in parallel.', default=5)\n",
    "    \n",
    "    \n",
    "#     args=parser.parse_args()\n",
    "    args = fakeParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    hpo_method = 'rs'\n",
    "    hpo_method = 'bohb'\n",
    "\n",
    "    random_iterations = 20 # these are outside parameters\n",
    "\n",
    "    nleaves_range = np.arange(1,51,1)\n",
    "    lrate_range = np.arange(0.1,1,0.1)\n",
    "    ntrees_range = np.arange(1,51,1)\n",
    "\n",
    "    h_param_ranges = [nleaves_range, lrate_range, ntrees_range]\n",
    "\n",
    "    if hpo_method == 'rs':\n",
    "        h_params = get_random_params(h_param_ranges, random_iterations)\n",
    "    elif hpo_method == 'gs':\n",
    "        h_params = get_grid_search_params(h_param_ranges)\n",
    "\n",
    "    hpo_params_list = [{'n_leaves': x[0], 'learning_rate': x[1], 'n_trees': x[2]} for x in h_params]\n",
    "\n",
    "    print(len(hpo_params_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "total budget: 1751\n",
      "allocated budget: 175\n",
      "File already exists\n"
     ]
    }
   ],
   "source": [
    "    budget = 10\n",
    "    dataset = args.dataset\n",
    "    workdir = './' + dataset + '_dir/'\n",
    "    \n",
    "    ranklib_location = '../../../ranklib/'\n",
    "    \n",
    "    if (not args.fold or args.dataset == 'bioasq'):\n",
    "        folds = ['']\n",
    "    elif args.fold == 'all':\n",
    "        folds = ['1','2','3','4','5']\n",
    "#         args.fold = ['1']\n",
    "    else:\n",
    "        folds = [args.fold]\n",
    "    \n",
    "    # Get features for every fold and data_split\n",
    "    \n",
    "    for fold in folds:\n",
    "        \n",
    "        print(fold)\n",
    "        \n",
    "        if args.dataset == 'bioasq':\n",
    "            fold_dir = workdir\n",
    "        else:\n",
    "            fold_dir = workdir + 's' + fold + '/'\n",
    "\n",
    "            \n",
    "        if args.dataset == 'bioasq':\n",
    "            train_queries_file = '../../bioasq_data/bioasq.' + 'train' + '.json'\n",
    "            \n",
    "            query_list = load_queries(train_queries_file)\n",
    "            qid_list = [q['id'] for q in query_list] \n",
    "            train_features_file =  fold_dir + dataset + '_' + 'train' + '_features'\n",
    "            budget_train_data_file = get_train_budget_data_file(budget, qid_list, train_features_file)\n",
    "\n",
    "        elif args.dataset == 'robust':\n",
    "            train_queries_file = '../../robust04_data/split_' + fold + '/rob04.' +  'train' + '.s' + fold + '.json'\n",
    "#                 q_file = queries_file\n",
    "\n",
    "            query_list = load_queries(train_queries_file)\n",
    "            qid_list = [q['id'] for q in query_list] \n",
    "            print(len(qid_list))\n",
    "            \n",
    "            train_features_fold_file = fold_dir + dataset + '_' + 'train' + '_s' + fold  + '_features'\n",
    "            budget_train_data_file = get_train_budget_data_file(budget, qid_list, train_features_fold_file)\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
