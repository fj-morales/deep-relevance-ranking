{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires Python 3!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n",
      "{'dropout_rate': 0.33424365007468426, 'lr': 0.05882653809492812, 'num_conv_layers': 3, 'num_fc_units': 32, 'num_filters_1': 18, 'optimizer': 'Adam', 'num_filters_2': 61, 'num_filters_3': 34}\n",
      "{'loss': 0.888671875, 'info': {'test accuracy': 0.1135, 'train accuracy': 0.1136474609375, 'validation accuracy': 0.111328125, 'number of parameters': 30273}}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import torch\n",
    "    import torch.utils.data\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "except:\n",
    "    raise ImportError(\"For this example you need to install pytorch.\")\n",
    "\n",
    "try:\n",
    "    import torchvision\n",
    "    import torchvision.transforms as transforms\n",
    "except:\n",
    "    raise ImportError(\"For this example you need to install pytorch-vision.\")\n",
    "\n",
    "\n",
    "\n",
    "import ConfigSpace as CS\n",
    "import ConfigSpace.hyperparameters as CSH\n",
    "\n",
    "from hpbandster.core.worker import Worker\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "\n",
    "\n",
    "class PyTorchWorker(Worker):\n",
    "    def __init__(self, N_train = 8192, N_valid = 1024, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "\n",
    "            batch_size = 64\n",
    "\n",
    "            # Load the MNIST Data here\n",
    "            train_dataset = torchvision.datasets.MNIST(root='../../data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "            test_dataset = torchvision.datasets.MNIST(root='../../data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "            train_sampler = torch.utils.data.sampler.SubsetRandomSampler(range(N_train))\n",
    "            validation_sampler = torch.utils.data.sampler.SubsetRandomSampler(range(N_train, N_train+N_valid))\n",
    "\n",
    "\n",
    "            self.train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "            self.validation_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=1024, sampler=validation_sampler)\n",
    "\n",
    "            self.test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "\n",
    "    def compute(self, config, budget, working_directory, *args, **kwargs):\n",
    "            \"\"\"\n",
    "            Simple example for a compute function using a feed forward network.\n",
    "            It is trained on the MNIST dataset.\n",
    "            The input parameter \"config\" (dictionary) contains the sampled configurations passed by the bohb optimizer\n",
    "            \"\"\"\n",
    "\n",
    "            # device = torch.device('cpu')\n",
    "            model = MNISTConvNet(num_conv_layers=config['num_conv_layers'],\n",
    "                                                    num_filters_1=config['num_filters_1'],\n",
    "                                                    num_filters_2=config['num_filters_2'] if 'num_filters_2' in config else None,\n",
    "                                                    num_filters_3=config['num_filters_3'] if 'num_filters_3' in config else None,\n",
    "                                                    dropout_rate=config['dropout_rate'],\n",
    "                                                    num_fc_units=config['num_fc_units'],\n",
    "                                                    kernel_size=3\n",
    "            )\n",
    "\n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "            if config['optimizer'] == 'Adam':\n",
    "                    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "            else:\n",
    "                    optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'], momentum=config['sgd_momentum'])\n",
    "\n",
    "            for epoch in range(int(budget)):\n",
    "                    loss = 0\n",
    "                    model.train()\n",
    "                    for i, (x, y) in enumerate(self.train_loader):\n",
    "                            optimizer.zero_grad()\n",
    "                            output = model(x)\n",
    "                            loss = F.nll_loss(output, y)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "            train_accuracy = self.evaluate_accuracy(model, self.train_loader)\n",
    "            validation_accuracy = self.evaluate_accuracy(model, self.validation_loader)\n",
    "            test_accuracy = self.evaluate_accuracy(model, self.test_loader)\n",
    "\n",
    "            return ({\n",
    "                    'loss': 1-validation_accuracy, # remember: HpBandSter always minimizes!\n",
    "                    'info': {       'test accuracy': test_accuracy,\n",
    "                                            'train accuracy': train_accuracy,\n",
    "                                            'validation accuracy': validation_accuracy,\n",
    "                                            'number of parameters': model.number_of_parameters(),\n",
    "                                    }\n",
    "\n",
    "            })\n",
    "\n",
    "    def evaluate_accuracy(self, model, data_loader):\n",
    "            model.eval()\n",
    "            correct=0\n",
    "            with torch.no_grad():\n",
    "                    for x, y in data_loader:\n",
    "                            output = model(x)\n",
    "                            #test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "                            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "                            correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "            #import pdb; pdb.set_trace()\n",
    "            accuracy = correct/len(data_loader.sampler)\n",
    "            return(accuracy)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_configspace():\n",
    "            \"\"\"\n",
    "            It builds the configuration space with the needed hyperparameters.\n",
    "            It is easily possible to implement different types of hyperparameters.\n",
    "            Beside float-hyperparameters on a log scale, it is also able to handle categorical input parameter.\n",
    "            :return: ConfigurationsSpace-Object\n",
    "            \"\"\"\n",
    "            cs = CS.ConfigurationSpace()\n",
    "\n",
    "            lr = CSH.UniformFloatHyperparameter('lr', lower=1e-6, upper=1e-1, default_value='1e-2', log=True)\n",
    "\n",
    "            # For demonstration purposes, we add different optimizers as categorical hyperparameters.\n",
    "            # To show how to use conditional hyperparameters with ConfigSpace, we'll add the optimizers 'Adam' and 'SGD'.\n",
    "            # SGD has a different parameter 'momentum'.\n",
    "            optimizer = CSH.CategoricalHyperparameter('optimizer', ['Adam', 'SGD'])\n",
    "\n",
    "            sgd_momentum = CSH.UniformFloatHyperparameter('sgd_momentum', lower=0.0, upper=0.99, default_value=0.9, log=False)\n",
    "\n",
    "            cs.add_hyperparameters([lr, optimizer, sgd_momentum])\n",
    "\n",
    "            # The hyperparameter sgd_momentum will be used,if the configuration\n",
    "            # contains 'SGD' as optimizer.\n",
    "            cond = CS.EqualsCondition(sgd_momentum, optimizer, 'SGD')\n",
    "            cs.add_condition(cond)\n",
    "\n",
    "            num_conv_layers =  CSH.UniformIntegerHyperparameter('num_conv_layers', lower=1, upper=3, default_value=2)\n",
    "\n",
    "            num_filters_1 = CSH.UniformIntegerHyperparameter('num_filters_1', lower=4, upper=64, default_value=16, log=True)\n",
    "            num_filters_2 = CSH.UniformIntegerHyperparameter('num_filters_2', lower=4, upper=64, default_value=16, log=True)\n",
    "            num_filters_3 = CSH.UniformIntegerHyperparameter('num_filters_3', lower=4, upper=64, default_value=16, log=True)\n",
    "\n",
    "\n",
    "            cs.add_hyperparameters([num_conv_layers, num_filters_1, num_filters_2, num_filters_3])\n",
    "\n",
    "            # You can also use inequality conditions:\n",
    "            cond = CS.GreaterThanCondition(num_filters_2, num_conv_layers, 1)\n",
    "            cs.add_condition(cond)\n",
    "\n",
    "            cond = CS.GreaterThanCondition(num_filters_3, num_conv_layers, 2)\n",
    "            cs.add_condition(cond)\n",
    "\n",
    "\n",
    "            dropout_rate = CSH.UniformFloatHyperparameter('dropout_rate', lower=0.0, upper=0.9, default_value=0.5, log=False)\n",
    "            num_fc_units = CSH.UniformIntegerHyperparameter('num_fc_units', lower=8, upper=256, default_value=32, log=True)\n",
    "\n",
    "            cs.add_hyperparameters([dropout_rate, num_fc_units])\n",
    "\n",
    "            return cs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MNISTConvNet(torch.nn.Module):\n",
    "    def __init__(self, num_conv_layers, num_filters_1, num_filters_2, num_filters_3, dropout_rate, num_fc_units, kernel_size):\n",
    "            super().__init__()\n",
    "\n",
    "            self.conv1 = nn.Conv2d(1, num_filters_1, kernel_size=kernel_size)\n",
    "            self.conv2 = None\n",
    "            self.conv3 = None\n",
    "\n",
    "            output_size = (28-kernel_size + 1)//2\n",
    "            num_output_filters = num_filters_1\n",
    "\n",
    "            if num_conv_layers > 1:\n",
    "                    self.conv2 = nn.Conv2d(num_filters_1, num_filters_2, kernel_size=kernel_size)\n",
    "                    num_output_filters = num_filters_2\n",
    "                    output_size = (output_size - kernel_size + 1)//2\n",
    "\n",
    "            if num_conv_layers > 2:\n",
    "                    self.conv3 = nn.Conv2d(num_filters_2, num_filters_3, kernel_size=kernel_size)\n",
    "                    num_output_filters = num_filters_3\n",
    "                    output_size = (output_size - kernel_size + 1)//2\n",
    "\n",
    "            self.dropout = nn.Dropout(p = dropout_rate)\n",
    "\n",
    "            self.conv_output_size = num_output_filters*output_size*output_size\n",
    "\n",
    "            self.fc1 = nn.Linear(self.conv_output_size, num_fc_units)\n",
    "            self.fc2 = nn.Linear(num_fc_units, 10)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "            # switched order of pooling and relu compared to the original example\n",
    "            # to make it identical to the keras worker\n",
    "            # seems to also give better accuracies\n",
    "            x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "\n",
    "            if not self.conv2 is None:\n",
    "                    x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "\n",
    "            if not self.conv3 is None:\n",
    "                    x = F.max_pool2d(F.relu(self.conv3(x)), 2)\n",
    "\n",
    "            x = self.dropout(x)\n",
    "\n",
    "            x = x.view(-1, self.conv_output_size)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc2(x)\n",
    "            return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "    def number_of_parameters(self):\n",
    "            return(sum(p.numel() for p in self.parameters() if p.requires_grad))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    worker = PyTorchWorker(run_id='0')\n",
    "    cs = worker.get_configspace()\n",
    "\n",
    "    config = cs.sample_configuration().get_dictionary()\n",
    "    print(config)\n",
    "    res = worker.compute(config=config, budget=2, working_directory='.')\n",
    "    print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
