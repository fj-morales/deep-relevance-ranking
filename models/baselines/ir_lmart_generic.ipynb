{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ir_lmart.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "# Stronger baseline: Listwise L2R - LambdaMART\n",
    "# Hyperparameter optimziation HPonsteroids requires Python 3!\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from functools import partial\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# REMOVE!!\n",
    "from eval_utils import *\n",
    "\n",
    "# HPO\n",
    "from hpo_utils import *\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import ConfigSpace as CS\n",
    "import ConfigSpace.hyperparameters as CSH\n",
    "\n",
    "from hpbandster.core.worker import Worker\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "# HPO server and stuff\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "import argparse\n",
    "\n",
    "import hpbandster.core.nameserver as hpns\n",
    "import hpbandster.core.result as hpres\n",
    "\n",
    "from hpbandster.optimizers import BOHB as BOHB\n",
    "from hpbandster.optimizers import RandomSearch as RS\n",
    "from hpbandster.examples.commons import MyWorker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[3]:\n",
    "\n",
    "\n",
    "# Functions\n",
    "def generate_run_file(pre_run_file, run_file):\n",
    "    \n",
    "    with open(pre_run_file, 'rt') as input_f:\n",
    "        pre_run = input_f.readlines()\n",
    "        print(type(pre_run))\n",
    "    with open(run_file, 'wt') as out_f:\n",
    "        for line in pre_run:\n",
    "            out_f.write(line.replace('docid=','').replace('indri', 'lambdaMART'))\n",
    "        \n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "# Classes\n",
    "class L2Ranker:\n",
    "    def __init__(self, ranklib_location, params, normalization=[]):\n",
    "        self.ranklib_location = ranklib_location\n",
    "        # Works with Oracle JSE\n",
    "        # java version \"1.8.0_211\"\n",
    "        # Java(TM) SE Runtime Environment (build 1.8.0_211-b12)\n",
    "        # Java HotSpot(TM) 64-Bit Server VM (build 25.211-b12, mixed mode)\n",
    "        self.params = params\n",
    "        self.ranker_command = ['java', '-jar', ranklib_location + 'RankLib-2.12.jar']\n",
    "        self.normalization = normalization\n",
    "        self.save_model_file = ''\n",
    "        \n",
    "#     def build(self, ir_tool_params):\n",
    "    def train(self, train_data_file, save_model_file, hpo_config):\n",
    "        self.save_model_file = save_model_file\n",
    "        self.log_file = save_model_file + '.log'\n",
    "        self.hpo_config= hpo_config\n",
    "        toolkit_parameters = [\n",
    "                                *self.ranker_command, # * to unpack list elements\n",
    "                                '-train',\n",
    "                                train_data_file,\n",
    "                                *self.normalization,\n",
    "                                *self.params,\n",
    "                                '-leaf', \n",
    "                                str(self.hpo_config['n_leaves']),\n",
    "                                '-shrinkage',\n",
    "                                str(self.hpo_config['learning_rate']),\n",
    "                                '-tree', # Oner regression tree per boosted iteration\n",
    "                                str(self.hpo_config['n_trees']),\n",
    "                                '-save',\n",
    "                                self.save_model_file   \n",
    "                            ] \n",
    "        \n",
    "#         print(toolkit_parameters)\n",
    "        with open(self.log_file, 'wt') as rf:\n",
    "            proc = subprocess.Popen(toolkit_parameters,stdin=subprocess.PIPE, stdout=rf, stderr=subprocess.PIPE, shell=False)\n",
    "#         proc = subprocess.Popen(toolkit_parameters,stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=False)\n",
    "            \n",
    "        (out, err)= proc.communicate()\n",
    "#         print(out.decode('utf-8').splitlines())\n",
    "#         print(out)\n",
    "\n",
    "        if err == b'':\n",
    "            print('Model saved: ', self.save_model_file)\n",
    "        else:\n",
    "#             print('error:', err, type(err))\n",
    "            print('Something went wrong on training, see log: ', self.log_file)\n",
    "            \n",
    "  \n",
    "\n",
    "    def gen_run_file(self, test_data_file, run_file):\n",
    "        pre_run_file = run_file.replace('run_', 'pre_run_', 1)\n",
    "        toolkit_parameters = [\n",
    "                                *self.ranker_command, # * to unpack list elements\n",
    "                                '-load',\n",
    "                                self.save_model_file,\n",
    "                                *self.normalization,\n",
    "                                '-rank',\n",
    "                                test_data_file,\n",
    "                                '-indri',\n",
    "                                pre_run_file     \n",
    "                            ] \n",
    "        \n",
    "#         print(toolkit_parameters)\n",
    "        with open(self.log_file, 'at') as rf:\n",
    "            proc = subprocess.Popen(toolkit_parameters,stdin=subprocess.PIPE, stdout=rf, stderr=subprocess.STDOUT, shell=False)\n",
    "#         proc = subprocess.Popen(toolkit_parameters,stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=False)\n",
    "            \n",
    "        (out, err)= proc.communicate()\n",
    "#         print(out.decode('utf-8').splitlines())\n",
    "#         print(out)\n",
    "        print(err)\n",
    "    \n",
    "        print(run_file)\n",
    "        print(pre_run_file)\n",
    "        \n",
    "        generate_run_file(pre_run_file, run_file)\n",
    "        \n",
    "#         print('Run model saved: ', run_file)\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "# try:\n",
    "#     import keras\n",
    "#     from keras.datasets import mnist\n",
    "#     from keras.models import Sequential\n",
    "#     from keras.layers import Dense, Dropout, Flatten\n",
    "#     from keras.layers import Conv2D, MaxPooling2D\n",
    "#     from keras import backend as K\n",
    "# except:\n",
    "#     raise ImportError(\"For this example you need to install keras.\")\n",
    "\n",
    "# try:\n",
    "#     import torchvision\n",
    "#     import torchvision.transforms as transforms\n",
    "# except:\n",
    "#     raise ImportError(\"For this example you need to install pytorch-vision.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fakeParser:\n",
    "    def __init__(self):\n",
    "        self.min_budget = 2 \n",
    "        self.max_budget = 4\n",
    "        self.n_iterations = 4 \n",
    "        self.n_workers =4\n",
    "        self.dataset = 'bioasq' \n",
    "        self.data_split = 'test'\n",
    "#         self.data_split = 'train'\n",
    "#         self.data_split = 'dev'\n",
    "#         self.build_index = True\n",
    "        self.build_index = None\n",
    "        self.fold = '1'\n",
    "        self.gen_features = True\n",
    "#         self.gen_features = None\n",
    "        \n",
    "# In[6]:\n",
    "\n",
    "\n",
    "# In[3]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Options and variables\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Example 1 - sequential and local execution.')\n",
    "    parser.add_argument('--min_budget',   type=float, help='Minimum budget used during the optimization.',    default=2)\n",
    "    parser.add_argument('--max_budget',   type=float, help='Maximum budget used during the optimization.',    default=4)\n",
    "    parser.add_argument('--n_iterations', type=int,   help='Number of iterations performed by the optimizer', default=500)\n",
    "    parser.add_argument('--n_workers', type=int,   help='Number of workers to run in parallel.', default=5)\n",
    "    \n",
    "    parser.add_argument('--dataset',   type=str, help='')\n",
    "    parser.add_argument('--data_split',   type=str, help='')\n",
    "    parser.add_argument('--pool_size',   type=int, help='')\n",
    "#     parser.add_argument('--build_index', action='store_true')\n",
    "    parser.add_argument('--fold', type=str,   help='')\n",
    "#     parser.add_argument('--gen_features', action='store_true')\n",
    "\n",
    "#     args=parser.parse_args()\n",
    "    args = fakeParser()\n",
    "    \n",
    "#     dataset = sys.argv[1] # 'bioasq'\n",
    "#     workdir = './' + dataset + '_dir/'\n",
    "#     data_split = sys.argv[2] # 'test'\n",
    "\n",
    "    dataset = args.dataset\n",
    "    workdir = './' + dataset + '_dir/'\n",
    "    data_split =  args.data_split\n",
    "    fold = args.fold\n",
    "        \n",
    "    ranklib_location = '../../../ranklib/'\n",
    "    \n",
    "    \n",
    "    if (not args.fold or args.dataset == 'bioasq'):\n",
    "        args.fold = ['']\n",
    "    elif args.fold == 'all':\n",
    "        args.fold = ['1','2','3','4','5']\n",
    "#         args.fold = ['1']\n",
    "    else:\n",
    "        args.fold = [args.fold]\n",
    "        \n",
    "    for f in args.fold:\n",
    "        \n",
    "        fold = f # '1'\n",
    "        \n",
    "        if args.dataset == 'bioasq':\n",
    "            fold_dir = workdir\n",
    "        else:\n",
    "            fold_dir = workdir + 's' + fold + '/'\n",
    "        utils.create_dir(fold_dir)\n",
    "    \n",
    "\n",
    "        train_data_file = fold_dir + 'rob04.train.' + fold + '_features'\n",
    "        val_data_file = fold_dir + 'rob04.dev.'   + fold + '_features'\n",
    "        test_data_file = fold_dir + 'rob04.test.' + fold + '_features'\n",
    "\n",
    "        l2r_model = 'lmart'\n",
    "\n",
    "        enabled_features_file = workdir + dataset + '_' + l2r_model + '_enabled_features' # dont'f change to fold_dir!\n",
    "\n",
    "    #     print(enabled_features_file)\n",
    "        # Train L2R model: LambdaMART\n",
    "        # Parameters \n",
    "\n",
    "        n_leaves = '10'\n",
    "        learning_rate = '0.1'\n",
    "        n_trees = '1000'\n",
    "        hpo_params = {'n_leaves': n_leaves, 'learning_rate': learning_rate, 'n_trees': n_trees}\n",
    "\n",
    "\n",
    "\n",
    "        metric2t = 'MAP' # 'MAP, NDCG@k, DCG@k, P@k, RR@k, ERR@k (default=ERR@10)'\n",
    "\n",
    "        ranker_type = '6' # LambdaMART\n",
    "\n",
    "        # normalization: Feature Engineering?\n",
    "    #     norm_params = ['-norm', 'zscore'] # 'sum', 'zscore', 'linear'\n",
    "\n",
    "        norm_params = ['-norm', 'zscore'] # 'sum', 'zscore', 'linear'\n",
    "\n",
    "        l2r_params = [\n",
    "            '-validate',\n",
    "            val_data_file,\n",
    "            '-ranker',\n",
    "            ranker_type,\n",
    "            '-metric2t',\n",
    "            metric2t,\n",
    "            '-feature',\n",
    "            enabled_features_file\n",
    "        ]\n",
    "\n",
    "        # Run train\n",
    "\n",
    "        lmart_model = L2Ranker(ranklib_location, l2r_params)\n",
    "    #     lmart_model = L2Ranker(ranklib_location, l2r_params, norm_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trec_eval_command = '../../eval/trec_eval'\n",
    "qrels_val_file = fold_dir + 'rob04.dev.' + fold + '_qrels'\n",
    "\n",
    "params_list = [\n",
    "    train_data_file,\n",
    "    val_data_file,\n",
    "    fold_dir,\n",
    "    lmart_model,\n",
    "    qrels_val_file\n",
    "]\n",
    "\n",
    "def eval_hpo(params_list, hpo_params):\n",
    "    \n",
    "    train_data_file = params_list[0]\n",
    "    val_data_file = params_list[1]\n",
    "    fold_dir = params_list[2]\n",
    "    lmart_model = params_list[3]\n",
    "    qrels_val_file = params_list[4]\n",
    "    \n",
    "    hpo_params_suffix = 'nl' + str(hpo_params['n_leaves']) + 'lr' + str(hpo_params['learning_rate']) + 'nt' + str(hpo_params['n_trees'])\n",
    "    save_model_file = fold_dir + 'rob04.' + fold + '_lmart_' + hpo_params_suffix + '_model' \n",
    "    \n",
    "    lmart_model.train(train_data_file, save_model_file, hpo_params)\n",
    "    run_file = fold_dir + 'run_' + 'rob04.' + fold + '_lmart_' + hpo_params_suffix\n",
    "    \n",
    "#   lmart_model.gen_run_file(test_data_file, run_file)\n",
    "\n",
    "    lmart_model.gen_run_file(val_data_file, run_file)\n",
    "    \n",
    "#     print(qrels_val_file)\n",
    "#     print(run_file)\n",
    "    eval_results = eval(trec_eval_command, qrels_val_file, run_file)\n",
    "    eval_results.update(lmart_model.hpo_config)\n",
    "    eval_results['lmart_model'] = lmart_model\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_dev_model(best_model_params_file, random_iterations = 5000):\n",
    "#     random_search = 'yes'\n",
    "    \n",
    "    if random_search == 'yes':\n",
    "        ## Heavy random search\n",
    "        brange = np.arange(0.1,1,0.05)\n",
    "        krange = np.arange(0.1,4,0.1)\n",
    "        N_range = np.arange(5,500,1) # num of docs\n",
    "        M_range = np.arange(5,500,1) # num of terms\n",
    "        lamb_range = np.arange(0,1,0.1) # weights of original query\n",
    "\n",
    "        ## Light random search\n",
    "#         brange = [0.2]\n",
    "#         krange = [0.8]\n",
    "#         N_range = np.arange(1,50,2)\n",
    "#         M_range = np.arange(1,50,2)\n",
    "#         lamb_range = np.arange(0,1,0.2)\n",
    "        \n",
    "        h_param_ranges = [brange, krange, N_range, M_range, lamb_range]\n",
    "        params = get_random_params(h_param_ranges, random_iterations)\n",
    "\n",
    "    else:\n",
    "        brange = [0.2]\n",
    "        krange = [0.8]\n",
    "        N_range = [11]\n",
    "        M_range = [10]\n",
    "        lamb_range = [0.5]\n",
    "       \n",
    "        params = [[round(b,3), round(k,3), round(N,3), round(M,3), round(Lambda,3)] \n",
    "                  for b in brange for k in krange for N in N_range for M in M_range for Lambda in lamb_range]\n",
    "   \n",
    "    print('# Params: ', len(params)) \n",
    "    pool_size = 20\n",
    "#     print(len(params))\n",
    "    pool = multiprocessing.Pool(processes=pool_size,\n",
    "                                initializer=start_process,\n",
    "                                )\n",
    "\n",
    "#     pool_outputs = pool.map(bm25_computing, params)\n",
    "    \n",
    "\n",
    "    pool_outputs = pool.map_async(bm25_computing, params)\n",
    "#     pool_outputs.get()\n",
    "    ###\n",
    "\n",
    "    \n",
    "    ##\n",
    "    \n",
    "    \n",
    "    pool.close() # no more tasks\n",
    "    while (True):\n",
    "        if (pool_outputs.ready()): break\n",
    "        remaining = pool_outputs._number_left\n",
    "#         remaining2 = remaining1\n",
    "#         remaining1 = pool_outputs._number_left\n",
    "        if remaining%10 == 0:\n",
    "            print(\"Waiting for\", remaining, \"tasks to complete...\")\n",
    "            time.sleep(2)\n",
    "        \n",
    "      \n",
    "    pool.join()  # wrap up current tasks\n",
    "    pool_outputs.get()\n",
    "    params_file = './best_ir_model/' + dataset_name_ext + '_' + 'bm25_rm3_' + split + '_hparams.pickle'\n",
    "    pickle.dump(pool_outputs.get(), open(params_file, \"wb\" ) )\n",
    "    print('Total parameters: ' + str(len(pool_outputs.get())))\n",
    "    best_model_params = max(pool_outputs.get(), key=lambda x: x[5])\n",
    "    \n",
    "    best_model_dict = {\n",
    "        'b': best_model_params[0],\n",
    "        'k': best_model_params[1],\n",
    "        'N': best_model_params[2],\n",
    "        'M': best_model_params[3],\n",
    "        'Lambda': best_model_params[4],\n",
    "        'random_iterations': random_iterations,\n",
    "        'map': best_model_params[5],\n",
    "        'p_20': best_model_params[6],\n",
    "        'ndcg_20': best_model_params[7]\n",
    "        \n",
    "    }\n",
    "    best_model_dict = {k:str(v) for k, v in best_model_dict.items()} # everything to string\n",
    "    \n",
    "    with open(best_model_params_file, 'wt') as best_model_f:\n",
    "        json.dump(best_model_dict, best_model_f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_process():\n",
    "    print( 'Starting', multiprocessing.current_process().name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_multi_hpo(params_list, hpo_params_list, pool_size):\n",
    "   \n",
    "    eval_hpo_partial = partial(eval_hpo, params_list)\n",
    "\n",
    "    pool = multiprocessing.Pool(processes=pool_size,\n",
    "                                initializer=start_process,\n",
    "                                )\n",
    "\n",
    "    pool_outputs = pool.map_async(eval_hpo_partial, hpo_params_list)\n",
    "    pool.close() # no more tasks\n",
    "    pool.join()  # wrap up current tasks\n",
    "    print('Total parameters: ' + str(len(pool_outputs.get())))\n",
    "    return pool_outputs.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "hpo_method = 'rs'\n",
    "\n",
    "random_iterations = 20 # these are outside parameters\n",
    "\n",
    "\n",
    "nleaves_range = np.arange(1,51,1)\n",
    "lrate_range = np.arange(0.1,1,0.1)\n",
    "ntrees_range = np.arange(1,51,1)\n",
    "\n",
    "h_param_ranges = [nleaves_range, lrate_range, ntrees_range]\n",
    "\n",
    "if hpo_method == 'rs':\n",
    "    h_params = get_random_params(h_param_ranges, random_iterations)\n",
    "elif hpo_method == 'gs':\n",
    "    h_params = get_grid_search_params(h_param_ranges)\n",
    "\n",
    "hpo_params_list = [{'n_leaves': x[0], 'learning_rate': x[1], 'n_trees': x[2]} for x in h_params]\n",
    "\n",
    "print(len(hpo_params_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ForkPoolWorker-5\n",
      "Starting ForkPoolWorker-3\n",
      "Starting ForkPoolWorker-2\n",
      "Starting ForkPoolWorker-4\n",
      "Starting ForkPoolWorker-1\n",
      "Model saved:  ./robust_dir/s1/rob04.s1_lmart_nl23lr0.8nt16_model\n",
      "Model saved:  ./robust_dir/s1/rob04.s1_lmart_nl38lr0.9nt9_model\n",
      "Model saved:  ./robust_dir/s1/rob04.s1_lmart_nl36lr0.1nt19_model\n",
      "None\n",
      "./robust_dir/s1/run_rob04.s1_lmart_nl23lr0.8nt16\n",
      "./robust_dir/s1/pre_run_rob04.s1_lmart_nl23lr0.8nt16\n",
      "<class 'list'>\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './robust_dir/s1/rob04.dev.s1_qrels', './robust_dir/s1/run_rob04.s1_lmart_nl23lr0.8nt16']\n",
      "Run error:  None\n",
      "None\n",
      "./robust_dir/s1/run_rob04.s1_lmart_nl36lr0.1nt19\n",
      "./robust_dir/s1/pre_run_rob04.s1_lmart_nl36lr0.1nt19\n",
      "<class 'list'>\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './robust_dir/s1/rob04.dev.s1_qrels', './robust_dir/s1/run_rob04.s1_lmart_nl36lr0.1nt19']\n",
      "Run error:  None\n",
      "None\n",
      "./robust_dir/s1/run_rob04.s1_lmart_nl38lr0.9nt9\n",
      "./robust_dir/s1/pre_run_rob04.s1_lmart_nl38lr0.9nt9\n",
      "<class 'list'>\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './robust_dir/s1/rob04.dev.s1_qrels', './robust_dir/s1/run_rob04.s1_lmart_nl38lr0.9nt9']\n",
      "Run error:  None\n",
      "Model saved:  ./robust_dir/s1/rob04.s1_lmart_nl39lr0.8nt15_model\n",
      "None\n",
      "./robust_dir/s1/run_rob04.s1_lmart_nl39lr0.8nt15\n",
      "./robust_dir/s1/pre_run_rob04.s1_lmart_nl39lr0.8nt15\n",
      "<class 'list'>\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './robust_dir/s1/rob04.dev.s1_qrels', './robust_dir/s1/run_rob04.s1_lmart_nl39lr0.8nt15']\n",
      "Run error:  None\n",
      "Model saved:  ./robust_dir/s1/rob04.s1_lmart_nl34lr0.2nt31_model\n",
      "Model saved:  ./robust_dir/s1/rob04.s1_lmart_nl31lr0.7nt3_model\n",
      "None\n",
      "./robust_dir/s1/run_rob04.s1_lmart_nl31lr0.7nt3\n",
      "./robust_dir/s1/pre_run_rob04.s1_lmart_nl31lr0.7nt3\n",
      "<class 'list'>\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './robust_dir/s1/rob04.dev.s1_qrels', './robust_dir/s1/run_rob04.s1_lmart_nl31lr0.7nt3']\n",
      "None\n",
      "./robust_dir/s1/run_rob04.s1_lmart_nl34lr0.2nt31\n",
      "./robust_dir/s1/pre_run_rob04.s1_lmart_nl34lr0.2nt31\n",
      "Run error:  None\n",
      "<class 'list'>\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './robust_dir/s1/rob04.dev.s1_qrels', './robust_dir/s1/run_rob04.s1_lmart_nl34lr0.2nt31']\n",
      "Run error:  None\n",
      "Model saved:  ./robust_dir/s1/rob04.s1_lmart_nl21lr0.9nt18_model\n",
      "None\n",
      "./robust_dir/s1/run_rob04.s1_lmart_nl21lr0.9nt18\n",
      "./robust_dir/s1/pre_run_rob04.s1_lmart_nl21lr0.9nt18\n",
      "<class 'list'>\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './robust_dir/s1/rob04.dev.s1_qrels', './robust_dir/s1/run_rob04.s1_lmart_nl21lr0.9nt18']\n",
      "Run error:  None\n",
      "Model saved:  ./robust_dir/s1/rob04.s1_lmart_nl4lr0.4nt25_model\n",
      "Model saved:  ./robust_dir/s1/rob04.s1_lmart_nl4lr0.6nt40_model\n",
      "None\n",
      "./robust_dir/s1/run_rob04.s1_lmart_nl4lr0.4nt25\n",
      "./robust_dir/s1/pre_run_rob04.s1_lmart_nl4lr0.4nt25\n",
      "<class 'list'>\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './robust_dir/s1/rob04.dev.s1_qrels', './robust_dir/s1/run_rob04.s1_lmart_nl4lr0.4nt25']\n",
      "Run error:  None\n",
      "Model saved:  ./robust_dir/s1/rob04.s1_lmart_nl2lr0.1nt20_model\n",
      "None\n",
      "./robust_dir/s1/run_rob04.s1_lmart_nl4lr0.6nt40\n",
      "./robust_dir/s1/pre_run_rob04.s1_lmart_nl4lr0.6nt40\n",
      "<class 'list'>\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './robust_dir/s1/rob04.dev.s1_qrels', './robust_dir/s1/run_rob04.s1_lmart_nl4lr0.6nt40']\n",
      "Run error:  None\n",
      "None\n",
      "./robust_dir/s1/run_rob04.s1_lmart_nl2lr0.1nt20\n",
      "./robust_dir/s1/pre_run_rob04.s1_lmart_nl2lr0.1nt20\n",
      "<class 'list'>\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './robust_dir/s1/rob04.dev.s1_qrels', './robust_dir/s1/run_rob04.s1_lmart_nl2lr0.1nt20']\n",
      "Run error:  None\n",
      "Model saved:  ./robust_dir/s1/rob04.s1_lmart_nl2lr0.6nt40_model\n",
      "None\n",
      "./robust_dir/s1/run_rob04.s1_lmart_nl2lr0.6nt40\n",
      "./robust_dir/s1/pre_run_rob04.s1_lmart_nl2lr0.6nt40\n",
      "<class 'list'>\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './robust_dir/s1/rob04.dev.s1_qrels', './robust_dir/s1/run_rob04.s1_lmart_nl2lr0.6nt40']\n",
      "Run error:  None\n",
      "Model saved:  ./robust_dir/s1/rob04.s1_lmart_nl30lr0.6nt1_model\n",
      "None\n",
      "./robust_dir/s1/run_rob04.s1_lmart_nl30lr0.6nt1\n",
      "./robust_dir/s1/pre_run_rob04.s1_lmart_nl30lr0.6nt1\n",
      "<class 'list'>\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './robust_dir/s1/rob04.dev.s1_qrels', './robust_dir/s1/run_rob04.s1_lmart_nl30lr0.6nt1']\n",
      "Run error:  None\n",
      "Model saved:  ./robust_dir/s1/rob04.s1_lmart_nl21lr0.9nt24_model\n",
      "None\n",
      "./robust_dir/s1/run_rob04.s1_lmart_nl21lr0.9nt24\n",
      "./robust_dir/s1/pre_run_rob04.s1_lmart_nl21lr0.9nt24\n",
      "<class 'list'>\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './robust_dir/s1/rob04.dev.s1_qrels', './robust_dir/s1/run_rob04.s1_lmart_nl21lr0.9nt24']\n",
      "Run error:  None\n",
      "Model saved:  ./robust_dir/s1/rob04.s1_lmart_nl34lr0.8nt17_model\n",
      "None\n",
      "./robust_dir/s1/run_rob04.s1_lmart_nl34lr0.8nt17\n",
      "./robust_dir/s1/pre_run_rob04.s1_lmart_nl34lr0.8nt17\n",
      "<class 'list'>\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './robust_dir/s1/rob04.dev.s1_qrels', './robust_dir/s1/run_rob04.s1_lmart_nl34lr0.8nt17']\n",
      "Run error:  None\n",
      "Model saved:  ./robust_dir/s1/rob04.s1_lmart_nl4lr0.8nt9_model\n",
      "None\n",
      "./robust_dir/s1/run_rob04.s1_lmart_nl4lr0.8nt9\n",
      "./robust_dir/s1/pre_run_rob04.s1_lmart_nl4lr0.8nt9\n",
      "<class 'list'>\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './robust_dir/s1/rob04.dev.s1_qrels', './robust_dir/s1/run_rob04.s1_lmart_nl4lr0.8nt9']\n",
      "Run error:  None\n",
      "Model saved:  ./robust_dir/s1/rob04.s1_lmart_nl35lr0.3nt38_model\n",
      "Model saved:  ./robust_dir/s1/rob04.s1_lmart_nl38lr0.4nt32_model\n",
      "None\n",
      "./robust_dir/s1/run_rob04.s1_lmart_nl35lr0.3nt38\n",
      "./robust_dir/s1/pre_run_rob04.s1_lmart_nl35lr0.3nt38\n",
      "<class 'list'>\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './robust_dir/s1/rob04.dev.s1_qrels', './robust_dir/s1/run_rob04.s1_lmart_nl35lr0.3nt38']\n",
      "Run error:  None\n",
      "None\n",
      "./robust_dir/s1/run_rob04.s1_lmart_nl38lr0.4nt32\n",
      "./robust_dir/s1/pre_run_rob04.s1_lmart_nl38lr0.4nt32\n",
      "<class 'list'>\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './robust_dir/s1/rob04.dev.s1_qrels', './robust_dir/s1/run_rob04.s1_lmart_nl38lr0.4nt32']\n",
      "Run error:  None\n",
      "Model saved:  ./robust_dir/s1/rob04.s1_lmart_nl32lr0.7nt7_model\n",
      "None\n",
      "./robust_dir/s1/run_rob04.s1_lmart_nl32lr0.7nt7\n",
      "./robust_dir/s1/pre_run_rob04.s1_lmart_nl32lr0.7nt7\n",
      "<class 'list'>\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './robust_dir/s1/rob04.dev.s1_qrels', './robust_dir/s1/run_rob04.s1_lmart_nl32lr0.7nt7']\n",
      "Run error:  None\n",
      "Model saved:  ./robust_dir/s1/rob04.s1_lmart_nl1lr0.4nt38_model\n",
      "None\n",
      "./robust_dir/s1/run_rob04.s1_lmart_nl1lr0.4nt38\n",
      "./robust_dir/s1/pre_run_rob04.s1_lmart_nl1lr0.4nt38\n",
      "<class 'list'>\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './robust_dir/s1/rob04.dev.s1_qrels', './robust_dir/s1/run_rob04.s1_lmart_nl1lr0.4nt38']\n",
      "Run error:  None\n",
      "Model saved:  ./robust_dir/s1/rob04.s1_lmart_nl24lr0.6nt39_model\n",
      "None\n",
      "./robust_dir/s1/run_rob04.s1_lmart_nl24lr0.6nt39\n",
      "./robust_dir/s1/pre_run_rob04.s1_lmart_nl24lr0.6nt39\n",
      "<class 'list'>\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './robust_dir/s1/rob04.dev.s1_qrels', './robust_dir/s1/run_rob04.s1_lmart_nl24lr0.6nt39']\n",
      "Run error:  None\n",
      "Total parameters: 20\n"
     ]
    }
   ],
   "source": [
    "# hpo_params_list = [hpo_params]\n",
    "pool_size = 5\n",
    "\n",
    "hpo_results = eval_multi_hpo(params_list, hpo_params_list, pool_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_hpo = max(hpo_results, key=lambda x: x['map'])\n",
    "best_model_hpo['random_iterations'] = random_iterations\n",
    "best_model_hpo['data_split'] = 'validation'\n",
    "best_lmart_model = best_model_hpo.pop('lmart_model')\n",
    "best_model_hpo = {k: str(v) for k, v in best_model_hpo.items()}\n",
    "\n",
    "hpo_results_file = fold_dir +  dataset + '_' + fold + '_' + l2r_model + '_hpo_results.pickle'\n",
    "best_model_hpo_file = fold_dir +  'best_' + dataset + '_' + fold + '_' + l2r_model + '_hparams.json'\n",
    "\n",
    "pickle.dump(hpo_results, open(hpo_results_file, \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'map': '0.1720',\n",
       "  'P_20': '0.2704',\n",
       "  'ndcg_cut_20': '0.3268',\n",
       "  'n_leaves': 39,\n",
       "  'learning_rate': 0.8,\n",
       "  'n_trees': 15,\n",
       "  'lmart_model': <__main__.L2Ranker at 0x7fb219e421d0>},\n",
       " {'map': '0.2003',\n",
       "  'P_20': '0.3092',\n",
       "  'ndcg_cut_20': '0.3636',\n",
       "  'n_leaves': 34,\n",
       "  'learning_rate': 0.2,\n",
       "  'n_trees': 31,\n",
       "  'lmart_model': <__main__.L2Ranker at 0x7fb219e426a0>},\n",
       " {'map': '0.1785',\n",
       "  'P_20': '0.2724',\n",
       "  'ndcg_cut_20': '0.3228',\n",
       "  'n_leaves': 38,\n",
       "  'learning_rate': 0.9,\n",
       "  'n_trees': 9,\n",
       "  'lmart_model': <__main__.L2Ranker at 0x7fb219e98f28>},\n",
       " {'map': '0.1502',\n",
       "  'P_20': '0.2398',\n",
       "  'ndcg_cut_20': '0.2929',\n",
       "  'n_leaves': 23,\n",
       "  'learning_rate': 0.8,\n",
       "  'n_trees': 16,\n",
       "  'lmart_model': <__main__.L2Ranker at 0x7fb219e98a58>},\n",
       " {'map': '0.2089',\n",
       "  'P_20': '0.3133',\n",
       "  'ndcg_cut_20': '0.3654',\n",
       "  'n_leaves': 36,\n",
       "  'learning_rate': 0.1,\n",
       "  'n_trees': 19,\n",
       "  'random_iterations': 20,\n",
       "  'data_split': 'validation'},\n",
       " {'map': '0.1703',\n",
       "  'P_20': '0.2684',\n",
       "  'ndcg_cut_20': '0.3239',\n",
       "  'n_leaves': 31,\n",
       "  'learning_rate': 0.7,\n",
       "  'n_trees': 3,\n",
       "  'lmart_model': <__main__.L2Ranker at 0x7fb219e42438>},\n",
       " {'map': '0.1624',\n",
       "  'P_20': '0.2429',\n",
       "  'ndcg_cut_20': '0.3093',\n",
       "  'n_leaves': 21,\n",
       "  'learning_rate': 0.9,\n",
       "  'n_trees': 18,\n",
       "  'lmart_model': <__main__.L2Ranker at 0x7fb219e42908>},\n",
       " {'map': '0.1040',\n",
       "  'P_20': '0.1092',\n",
       "  'ndcg_cut_20': '0.1717',\n",
       "  'n_leaves': 4,\n",
       "  'learning_rate': 0.4,\n",
       "  'n_trees': 25,\n",
       "  'lmart_model': <__main__.L2Ranker at 0x7fb219e42b70>},\n",
       " {'map': '0.1112',\n",
       "  'P_20': '0.1490',\n",
       "  'ndcg_cut_20': '0.2148',\n",
       "  'n_leaves': 4,\n",
       "  'learning_rate': 0.6,\n",
       "  'n_trees': 40,\n",
       "  'lmart_model': <__main__.L2Ranker at 0x7fb219e42dd8>},\n",
       " {'map': '0.1849',\n",
       "  'P_20': '0.2755',\n",
       "  'ndcg_cut_20': '0.3460',\n",
       "  'n_leaves': 2,\n",
       "  'learning_rate': 0.6,\n",
       "  'n_trees': 40,\n",
       "  'lmart_model': <__main__.L2Ranker at 0x7fb219e442e8>},\n",
       " {'map': '0.1363',\n",
       "  'P_20': '0.1878',\n",
       "  'ndcg_cut_20': '0.2642',\n",
       "  'n_leaves': 2,\n",
       "  'learning_rate': 0.1,\n",
       "  'n_trees': 20,\n",
       "  'lmart_model': <__main__.L2Ranker at 0x7fb219e44080>},\n",
       " {'map': '0.1624',\n",
       "  'P_20': '0.2429',\n",
       "  'ndcg_cut_20': '0.3093',\n",
       "  'n_leaves': 21,\n",
       "  'learning_rate': 0.9,\n",
       "  'n_trees': 24,\n",
       "  'lmart_model': <__main__.L2Ranker at 0x7fb219e447b8>},\n",
       " {'map': '0.1693',\n",
       "  'P_20': '0.2663',\n",
       "  'ndcg_cut_20': '0.3195',\n",
       "  'n_leaves': 34,\n",
       "  'learning_rate': 0.8,\n",
       "  'n_trees': 17,\n",
       "  'lmart_model': <__main__.L2Ranker at 0x7fb219e44a20>},\n",
       " {'map': '0.1456',\n",
       "  'P_20': '0.2408',\n",
       "  'ndcg_cut_20': '0.2901',\n",
       "  'n_leaves': 30,\n",
       "  'learning_rate': 0.6,\n",
       "  'n_trees': 1,\n",
       "  'lmart_model': <__main__.L2Ranker at 0x7fb219e44550>},\n",
       " {'map': '0.2036',\n",
       "  'P_20': '0.3031',\n",
       "  'ndcg_cut_20': '0.3682',\n",
       "  'n_leaves': 35,\n",
       "  'learning_rate': 0.3,\n",
       "  'n_trees': 38,\n",
       "  'lmart_model': <__main__.L2Ranker at 0x7fb219e44ef0>},\n",
       " {'map': '0.1905',\n",
       "  'P_20': '0.2918',\n",
       "  'ndcg_cut_20': '0.3418',\n",
       "  'n_leaves': 38,\n",
       "  'learning_rate': 0.4,\n",
       "  'n_trees': 32,\n",
       "  'lmart_model': <__main__.L2Ranker at 0x7fb219e4b198>},\n",
       " {'map': '0.1084',\n",
       "  'P_20': '0.1245',\n",
       "  'ndcg_cut_20': '0.1879',\n",
       "  'n_leaves': 4,\n",
       "  'learning_rate': 0.8,\n",
       "  'n_trees': 9,\n",
       "  'lmart_model': <__main__.L2Ranker at 0x7fb219e44c88>},\n",
       " {'map': '0.1922',\n",
       "  'P_20': '0.2878',\n",
       "  'ndcg_cut_20': '0.3483',\n",
       "  'n_leaves': 24,\n",
       "  'learning_rate': 0.6,\n",
       "  'n_trees': 39,\n",
       "  'lmart_model': <__main__.L2Ranker at 0x7fb219e4b8d0>},\n",
       " {'map': '0.1919',\n",
       "  'P_20': '0.2837',\n",
       "  'ndcg_cut_20': '0.3574',\n",
       "  'n_leaves': 1,\n",
       "  'learning_rate': 0.4,\n",
       "  'n_trees': 38,\n",
       "  'lmart_model': <__main__.L2Ranker at 0x7fb219e4b668>},\n",
       " {'map': '0.1682',\n",
       "  'P_20': '0.2571',\n",
       "  'ndcg_cut_20': '0.3136',\n",
       "  'n_leaves': 32,\n",
       "  'learning_rate': 0.7,\n",
       "  'n_trees': 7,\n",
       "  'lmart_model': <__main__.L2Ranker at 0x7fb219e4b400>}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpo_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "./robust_dir/s1/run_rob04.s1_test_lmart_best\n",
      "./robust_dir/s1/pre_run_rob04.s1_test_lmart_best\n",
      "<class 'list'>\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './robust_dir/s1/rob04.test.s1_qrels', './robust_dir/s1/run_rob04.s1_test_lmart_best']\n",
      "Run error:  None\n"
     ]
    }
   ],
   "source": [
    "qrels_test_file = fold_dir + 'rob04.test.' + fold + '_qrels'\n",
    "\n",
    "run_file = fold_dir + 'run_' + 'rob04.' + fold + '_test_lmart_best'\n",
    "\n",
    "best_lmart_model.gen_run_file(test_data_file, run_file)\n",
    "\n",
    "eval_results = eval(trec_eval_command, qrels_test_file, run_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results['data_split'] = 'test'\n",
    "eval_results\n",
    "\n",
    "best_model_hpo['test'] = eval_results\n",
    "\n",
    "with open(best_model_hpo_file, 'wt') as best_model_f:\n",
    "    json.dump(best_model_hpo, best_model_f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
