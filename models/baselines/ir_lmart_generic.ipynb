{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ir_lmart.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "# Stronger baseline: Listwise L2R - LambdaMART\n",
    "# Hyperparameter optimziation HPonsteroids requires Python 3!\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from functools import partial\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# REMOVE!!\n",
    "from eval_utils import *\n",
    "\n",
    "# HPO\n",
    "from hpo_utils import *\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import ConfigSpace as CS\n",
    "import ConfigSpace.hyperparameters as CSH\n",
    "\n",
    "from hpbandster.core.worker import Worker\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "# HPO server and stuff\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "import argparse\n",
    "\n",
    "import hpbandster.core.nameserver as hpns\n",
    "import hpbandster.core.result as hpres\n",
    "\n",
    "from hpbandster.optimizers import BOHB as BOHB\n",
    "from hpbandster.optimizers import RandomSearch as RS\n",
    "from hpbandster.examples.commons import MyWorker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[3]:\n",
    "\n",
    "\n",
    "# Functions\n",
    "def generate_run_file(pre_run_file, run_file):\n",
    "    \n",
    "    with open(pre_run_file, 'rt') as input_f:\n",
    "        pre_run = input_f.readlines()\n",
    "#         print(type(pre_run))\n",
    "    with open(run_file, 'wt') as out_f:\n",
    "        for line in pre_run:\n",
    "            out_f.write(line.replace('docid=','').replace('indri', 'lambdaMART'))\n",
    "        \n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "# Classes\n",
    "class L2Ranker:\n",
    "    def __init__(self, ranklib_location, params, normalization=[]):\n",
    "        self.ranklib_location = ranklib_location\n",
    "        # Works with Oracle JSE\n",
    "        # java version \"1.8.0_211\"\n",
    "        # Java(TM) SE Runtime Environment (build 1.8.0_211-b12)\n",
    "        # Java HotSpot(TM) 64-Bit Server VM (build 25.211-b12, mixed mode)\n",
    "        self.params = params\n",
    "        self.ranker_command = ['java', '-jar', ranklib_location + 'RankLib-2.12.jar']\n",
    "        self.normalization = normalization\n",
    "        self.save_model_file = ''\n",
    "        \n",
    "#     def build(self, ir_tool_params):\n",
    "    def train(self, train_data_file, save_model_file, hpo_config):\n",
    "        self.save_model_file = save_model_file\n",
    "        self.log_file = save_model_file + '.log'\n",
    "        self.hpo_config= hpo_config\n",
    "        toolkit_parameters = [\n",
    "                                *self.ranker_command, # * to unpack list elements\n",
    "                                '-train',\n",
    "                                train_data_file,\n",
    "                                *self.normalization,\n",
    "                                *self.params,\n",
    "                                '-leaf', \n",
    "                                str(self.hpo_config['n_leaves']),\n",
    "                                '-shrinkage',\n",
    "                                str(self.hpo_config['learning_rate']),\n",
    "                                '-tree', # Oner regression tree per boosted iteration\n",
    "                                str(self.hpo_config['n_trees']),\n",
    "                                '-save',\n",
    "                                self.save_model_file   \n",
    "                            ] \n",
    "        \n",
    "#         print(toolkit_parameters)\n",
    "        with open(self.log_file, 'wt') as rf:\n",
    "            proc = subprocess.Popen(toolkit_parameters,stdin=subprocess.PIPE, stdout=rf, stderr=subprocess.PIPE, shell=False)\n",
    "#         proc = subprocess.Popen(toolkit_parameters,stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=False)\n",
    "            \n",
    "        (out, err)= proc.communicate()\n",
    "#         print(out.decode('utf-8').splitlines())\n",
    "#         print(out)\n",
    "\n",
    "        if err == b'':\n",
    "            print('Model saved: ', self.save_model_file)\n",
    "        else:\n",
    "#             print('error:', err, type(err))\n",
    "            print('Something went wrong on training, see log: ', self.log_file)\n",
    "            \n",
    "  \n",
    "\n",
    "    def gen_run_file(self, test_data_file, run_file):\n",
    "        pre_run_file = run_file.replace('run_', 'pre_run_', 1)\n",
    "        toolkit_parameters = [\n",
    "                                *self.ranker_command, # * to unpack list elements\n",
    "                                '-load',\n",
    "                                self.save_model_file,\n",
    "                                *self.normalization,\n",
    "                                '-rank',\n",
    "                                test_data_file,\n",
    "                                '-indri',\n",
    "                                pre_run_file     \n",
    "                            ] \n",
    "        \n",
    "#         print(toolkit_parameters)\n",
    "        with open(self.log_file, 'at') as rf:\n",
    "            proc = subprocess.Popen(toolkit_parameters,stdin=subprocess.PIPE, stdout=rf, stderr=subprocess.STDOUT, shell=False)\n",
    "#         proc = subprocess.Popen(toolkit_parameters,stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=False)\n",
    "            \n",
    "        (out, err)= proc.communicate()\n",
    "#         print(out.decode('utf-8').splitlines())\n",
    "#         print(out)\n",
    "        print(err)\n",
    "    \n",
    "        print(run_file)\n",
    "        print(pre_run_file)\n",
    "        \n",
    "        generate_run_file(pre_run_file, run_file)\n",
    "        \n",
    "#         print('Run model saved: ', run_file)\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "# try:\n",
    "#     import keras\n",
    "#     from keras.datasets import mnist\n",
    "#     from keras.models import Sequential\n",
    "#     from keras.layers import Dense, Dropout, Flatten\n",
    "#     from keras.layers import Conv2D, MaxPooling2D\n",
    "#     from keras import backend as K\n",
    "# except:\n",
    "#     raise ImportError(\"For this example you need to install keras.\")\n",
    "\n",
    "# try:\n",
    "#     import torchvision\n",
    "#     import torchvision.transforms as transforms\n",
    "# except:\n",
    "#     raise ImportError(\"For this example you need to install pytorch-vision.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fakeParser:\n",
    "    def __init__(self):\n",
    "        self.min_budget = 2 \n",
    "        self.max_budget = 4\n",
    "        self.n_iterations = 4 \n",
    "        self.n_workers =4\n",
    "        self.dataset = 'bioasq' \n",
    "        self.data_split = 'test'\n",
    "#         self.data_split = 'train'\n",
    "#         self.data_split = 'dev'\n",
    "#         self.build_index = True\n",
    "        self.build_index = None\n",
    "        self.fold = '1'\n",
    "        self.gen_features = True\n",
    "#         self.gen_features = None\n",
    "        \n",
    "# In[6]:\n",
    "\n",
    "\n",
    "# In[3]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Options and variables\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Example 1 - sequential and local execution.')\n",
    "    parser.add_argument('--min_budget',   type=float, help='Minimum budget used during the optimization.',    default=2)\n",
    "    parser.add_argument('--max_budget',   type=float, help='Maximum budget used during the optimization.',    default=4)\n",
    "    parser.add_argument('--n_iterations', type=int,   help='Number of iterations performed by the optimizer', default=500)\n",
    "    parser.add_argument('--n_workers', type=int,   help='Number of workers to run in parallel.', default=5)\n",
    "    \n",
    "    parser.add_argument('--dataset',   type=str, help='')\n",
    "    parser.add_argument('--data_split',   type=str, help='')\n",
    "    parser.add_argument('--pool_size',   type=int, help='')\n",
    "#     parser.add_argument('--build_index', action='store_true')\n",
    "    parser.add_argument('--fold', type=str,   help='')\n",
    "#     parser.add_argument('--gen_features', action='store_true')\n",
    "\n",
    "#     args=parser.parse_args()\n",
    "    args = fakeParser()\n",
    "    \n",
    "#     dataset = sys.argv[1] # 'bioasq'\n",
    "#     workdir = './' + dataset + '_dir/'\n",
    "#     data_split = sys.argv[2] # 'test'\n",
    "\n",
    "    dataset = args.dataset\n",
    "    workdir = './' + dataset + '_dir/'\n",
    "    confdir = './' + dataset + '_config/'\n",
    "    data_split =  args.data_split\n",
    "    fold = args.fold\n",
    "        \n",
    "    ranklib_location = '../../../ranklib/'\n",
    "    \n",
    "    \n",
    "    if (not args.fold or args.dataset == 'bioasq'):\n",
    "        args.fold = ['']\n",
    "    elif args.fold == 'all':\n",
    "        args.fold = ['1','2','3','4','5']\n",
    "#         args.fold = ['1']\n",
    "    else:\n",
    "        args.fold = [args.fold]\n",
    "        \n",
    "    for f in args.fold:\n",
    "        \n",
    "        fold = f # '1'\n",
    "        \n",
    "        if args.dataset == 'bioasq':\n",
    "            \n",
    "  \n",
    "            \n",
    "            fold_dir = workdir\n",
    "            qrels_val_file = fold_dir + dataset + '_' + 'dev' + '_qrels'\n",
    "            dataset_fold = dataset \n",
    "            \n",
    "            train_data_file = fold_dir + dataset + '_' + 'train'  + '_features'\n",
    "            val_data_file = fold_dir + dataset + '_' + 'dev' +  '_features'\n",
    "            test_data_file = fold_dir + dataset + '_' + 'test' + '_features'  \n",
    "            \n",
    "        elif args.dataset == 'robust':\n",
    "            \n",
    "            fold_dir = workdir + 's' + fold + '/'\n",
    "            qrels_val_file = fold_dir + dataset + '_' + 'dev' + '_' + fold + '_qrels'\n",
    "            dataset_fold = dataset + '_' + fold\n",
    "            \n",
    "            train_data_file = fold_dir + dataset + '_' + 'train' + '_' + fold + '_features'\n",
    "            val_data_file = fold_dir + dataset + '_' + 'dev' + '_' + fold + '_features'\n",
    "            test_data_file = fold_dir + dataset + '_' + 'test' + '_' + fold + '_features'\n",
    "\n",
    "        l2r_model = 'lmart'\n",
    "\n",
    "        enabled_features_file = confdir + dataset + '_' + l2r_model + '_enabled_features' # dont'f change to fold_dir!\n",
    "\n",
    "    #     print(enabled_features_file)\n",
    "        # Train L2R model: LambdaMART\n",
    "        # Parameters \n",
    "\n",
    "        n_leaves = '10'\n",
    "        learning_rate = '0.1'\n",
    "        n_trees = '1000'\n",
    "        hpo_params = {'n_leaves': n_leaves, 'learning_rate': learning_rate, 'n_trees': n_trees}\n",
    "\n",
    "\n",
    "\n",
    "        metric2t = 'MAP' # 'MAP, NDCG@k, DCG@k, P@k, RR@k, ERR@k (default=ERR@10)'\n",
    "\n",
    "        ranker_type = '6' # LambdaMART\n",
    "\n",
    "        # normalization: Feature Engineering?\n",
    "    #     norm_params = ['-norm', 'zscore'] # 'sum', 'zscore', 'linear'\n",
    "\n",
    "        norm_params = ['-norm', 'zscore'] # 'sum', 'zscore', 'linear'\n",
    "\n",
    "        l2r_params = [\n",
    "            '-validate',\n",
    "            val_data_file,\n",
    "            '-ranker',\n",
    "            ranker_type,\n",
    "            '-metric2t',\n",
    "            metric2t,\n",
    "            '-feature',\n",
    "            enabled_features_file\n",
    "        ]\n",
    "\n",
    "        # Run train\n",
    "\n",
    "        lmart_model = L2Ranker(ranklib_location, l2r_params)\n",
    "    #     lmart_model = L2Ranker(ranklib_location, l2r_params, norm_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trec_eval_command = '../../eval/trec_eval'\n",
    "\n",
    "\n",
    "params_list = [\n",
    "    train_data_file,\n",
    "    val_data_file,\n",
    "    fold_dir,\n",
    "    lmart_model,\n",
    "    qrels_val_file\n",
    "]\n",
    "\n",
    "def eval_hpo(params_list, hpo_params):\n",
    "    \n",
    "    train_data_file = params_list[0]\n",
    "    val_data_file = params_list[1]\n",
    "    fold_dir = params_list[2]\n",
    "    lmart_model = params_list[3]\n",
    "    qrels_val_file = params_list[4]\n",
    "    \n",
    "    hpo_params_suffix = 'nl' + str(hpo_params['n_leaves']) + 'lr' + str(hpo_params['learning_rate']) + 'nt' + str(hpo_params['n_trees'])\n",
    "    \n",
    "    save_model_file = fold_dir + dataset_fold + '_lmart_' + hpo_params_suffix + '_model' \n",
    "    \n",
    "    lmart_model.train(train_data_file, save_model_file, hpo_params)\n",
    "    run_file = fold_dir + 'run_' + dataset_fold + '_lmart_' + hpo_params_suffix\n",
    "    \n",
    "#   lmart_model.gen_run_file(test_data_file, run_file)\n",
    "\n",
    "    lmart_model.gen_run_file(val_data_file, run_file)\n",
    "    \n",
    "#     print(qrels_val_file)\n",
    "#     print(run_file)\n",
    "    eval_results = eval(trec_eval_command, qrels_val_file, run_file)\n",
    "    eval_results.update(lmart_model.hpo_config)\n",
    "    eval_results['lmart_model'] = lmart_model\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_dev_model(best_model_params_file, random_iterations = 5000):\n",
    "#     random_search = 'yes'\n",
    "    \n",
    "    if random_search == 'yes':\n",
    "        ## Heavy random search\n",
    "        brange = np.arange(0.1,1,0.05)\n",
    "        krange = np.arange(0.1,4,0.1)\n",
    "        N_range = np.arange(5,500,1) # num of docs\n",
    "        M_range = np.arange(5,500,1) # num of terms\n",
    "        lamb_range = np.arange(0,1,0.1) # weights of original query\n",
    "\n",
    "        ## Light random search\n",
    "#         brange = [0.2]\n",
    "#         krange = [0.8]\n",
    "#         N_range = np.arange(1,50,2)\n",
    "#         M_range = np.arange(1,50,2)\n",
    "#         lamb_range = np.arange(0,1,0.2)\n",
    "        \n",
    "        h_param_ranges = [brange, krange, N_range, M_range, lamb_range]\n",
    "        params = get_random_params(h_param_ranges, random_iterations)\n",
    "\n",
    "    else:\n",
    "        brange = [0.2]\n",
    "        krange = [0.8]\n",
    "        N_range = [11]\n",
    "        M_range = [10]\n",
    "        lamb_range = [0.5]\n",
    "       \n",
    "        params = [[round(b,3), round(k,3), round(N,3), round(M,3), round(Lambda,3)] \n",
    "                  for b in brange for k in krange for N in N_range for M in M_range for Lambda in lamb_range]\n",
    "   \n",
    "    print('# Params: ', len(params)) \n",
    "    pool_size = 20\n",
    "#     print(len(params))\n",
    "    pool = multiprocessing.Pool(processes=pool_size,\n",
    "                                initializer=start_process,\n",
    "                                )\n",
    "\n",
    "#     pool_outputs = pool.map(bm25_computing, params)\n",
    "    \n",
    "\n",
    "    pool_outputs = pool.map_async(bm25_computing, params)\n",
    "#     pool_outputs.get()\n",
    "    ###\n",
    "\n",
    "    \n",
    "    ##\n",
    "    \n",
    "    \n",
    "    pool.close() # no more tasks\n",
    "    while (True):\n",
    "        if (pool_outputs.ready()): break\n",
    "        remaining = pool_outputs._number_left\n",
    "#         remaining2 = remaining1\n",
    "#         remaining1 = pool_outputs._number_left\n",
    "        if remaining%10 == 0:\n",
    "            print(\"Waiting for\", remaining, \"tasks to complete...\")\n",
    "            time.sleep(2)\n",
    "        \n",
    "      \n",
    "    pool.join()  # wrap up current tasks\n",
    "    pool_outputs.get()\n",
    "    params_file = './best_ir_model/' + dataset_name_ext + '_' + 'bm25_rm3_' + split + '_hparams.pickle'\n",
    "    pickle.dump(pool_outputs.get(), open(params_file, \"wb\" ) )\n",
    "    print('Total parameters: ' + str(len(pool_outputs.get())))\n",
    "    best_model_params = max(pool_outputs.get(), key=lambda x: x[5])\n",
    "    \n",
    "    best_model_dict = {\n",
    "        'b': best_model_params[0],\n",
    "        'k': best_model_params[1],\n",
    "        'N': best_model_params[2],\n",
    "        'M': best_model_params[3],\n",
    "        'Lambda': best_model_params[4],\n",
    "        'random_iterations': random_iterations,\n",
    "        'map': best_model_params[5],\n",
    "        'p_20': best_model_params[6],\n",
    "        'ndcg_20': best_model_params[7]\n",
    "        \n",
    "    }\n",
    "    best_model_dict = {k:str(v) for k, v in best_model_dict.items()} # everything to string\n",
    "    \n",
    "    with open(best_model_params_file, 'wt') as best_model_f:\n",
    "        json.dump(best_model_dict, best_model_f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_process():\n",
    "    print( 'Starting', multiprocessing.current_process().name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_multi_hpo(params_list, hpo_params_list, pool_size):\n",
    "   \n",
    "    eval_hpo_partial = partial(eval_hpo, params_list)\n",
    "\n",
    "    pool = multiprocessing.Pool(processes=pool_size,\n",
    "                                initializer=start_process,\n",
    "                                )\n",
    "\n",
    "    pool_outputs = pool.map_async(eval_hpo_partial, hpo_params_list)\n",
    "    pool.close() # no more tasks\n",
    "    pool.join()  # wrap up current tasks\n",
    "    print('Total parameters: ' + str(len(pool_outputs.get())))\n",
    "    return pool_outputs.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "hpo_method = 'rs'\n",
    "\n",
    "random_iterations = 20 # these are outside parameters\n",
    "\n",
    "nleaves_range = np.arange(1,51,1)\n",
    "lrate_range = np.arange(0.1,1,0.1)\n",
    "ntrees_range = np.arange(1,51,1)\n",
    "\n",
    "h_param_ranges = [nleaves_range, lrate_range, ntrees_range]\n",
    "\n",
    "if hpo_method == 'rs':\n",
    "    h_params = get_random_params(h_param_ranges, random_iterations)\n",
    "elif hpo_method == 'gs':\n",
    "    h_params = get_grid_search_params(h_param_ranges)\n",
    "\n",
    "hpo_params_list = [{'n_leaves': x[0], 'learning_rate': x[1], 'n_trees': x[2]} for x in h_params]\n",
    "\n",
    "print(len(hpo_params_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ForkPoolWorker-1\n",
      "Starting ForkPoolWorker-2\n",
      "Starting ForkPoolWorker-3\n",
      "Starting ForkPoolWorker-4\n",
      "Starting ForkPoolWorker-5\n",
      "Model saved:  ./bioasq_dir/bioasq_lmart_nl24lr0.2nt20_model\n",
      "None\n",
      "./bioasq_dir/run_bioasq_lmart_nl24lr0.2nt20\n",
      "./bioasq_dir/pre_run_bioasq_lmart_nl24lr0.2nt20\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './bioasq_dir/bioasq_dev_qrels', './bioasq_dir/run_bioasq_lmart_nl24lr0.2nt20']\n",
      "Run error:  None\n",
      "Model saved:  ./bioasq_dir/bioasq_lmart_nl48lr0.9nt2_model\n",
      "Model saved:  ./bioasq_dir/bioasq_lmart_nl46lr0.9nt34_model\n",
      "None\n",
      "./bioasq_dir/run_bioasq_lmart_nl48lr0.9nt2\n",
      "./bioasq_dir/pre_run_bioasq_lmart_nl48lr0.9nt2\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './bioasq_dir/bioasq_dev_qrels', './bioasq_dir/run_bioasq_lmart_nl48lr0.9nt2']\n",
      "Run error:  None\n",
      "None\n",
      "./bioasq_dir/run_bioasq_lmart_nl46lr0.9nt34\n",
      "./bioasq_dir/pre_run_bioasq_lmart_nl46lr0.9nt34\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './bioasq_dir/bioasq_dev_qrels', './bioasq_dir/run_bioasq_lmart_nl46lr0.9nt34']\n",
      "Run error:  None\n",
      "Model saved:  ./bioasq_dir/bioasq_lmart_nl38lr0.6nt33_model\n",
      "None\n",
      "./bioasq_dir/run_bioasq_lmart_nl38lr0.6nt33\n",
      "./bioasq_dir/pre_run_bioasq_lmart_nl38lr0.6nt33\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './bioasq_dir/bioasq_dev_qrels', './bioasq_dir/run_bioasq_lmart_nl38lr0.6nt33']\n",
      "Run error:  None\n",
      "Model saved:  ./bioasq_dir/bioasq_lmart_nl11lr0.2nt39_model\n",
      "Model saved:  ./bioasq_dir/bioasq_lmart_nl31lr0.5nt28_model\n",
      "None\n",
      "./bioasq_dir/run_bioasq_lmart_nl11lr0.2nt39\n",
      "./bioasq_dir/pre_run_bioasq_lmart_nl11lr0.2nt39\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './bioasq_dir/bioasq_dev_qrels', './bioasq_dir/run_bioasq_lmart_nl11lr0.2nt39']\n",
      "Run error:  None\n",
      "None\n",
      "./bioasq_dir/run_bioasq_lmart_nl31lr0.5nt28\n",
      "./bioasq_dir/pre_run_bioasq_lmart_nl31lr0.5nt28\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './bioasq_dir/bioasq_dev_qrels', './bioasq_dir/run_bioasq_lmart_nl31lr0.5nt28']\n",
      "Run error:  None\n",
      "Model saved:  ./bioasq_dir/bioasq_lmart_nl3lr0.4nt8_model\n",
      "None\n",
      "./bioasq_dir/run_bioasq_lmart_nl3lr0.4nt8\n",
      "./bioasq_dir/pre_run_bioasq_lmart_nl3lr0.4nt8\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './bioasq_dir/bioasq_dev_qrels', './bioasq_dir/run_bioasq_lmart_nl3lr0.4nt8']\n",
      "Run error:  None\n",
      "Model saved:  ./bioasq_dir/bioasq_lmart_nl7lr0.5nt41_model\n",
      "None\n",
      "./bioasq_dir/run_bioasq_lmart_nl7lr0.5nt41\n",
      "./bioasq_dir/pre_run_bioasq_lmart_nl7lr0.5nt41\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './bioasq_dir/bioasq_dev_qrels', './bioasq_dir/run_bioasq_lmart_nl7lr0.5nt41']\n",
      "Run error:  None\n",
      "Model saved:  ./bioasq_dir/bioasq_lmart_nl19lr0.4nt6_model\n",
      "None\n",
      "./bioasq_dir/run_bioasq_lmart_nl19lr0.4nt6\n",
      "./bioasq_dir/pre_run_bioasq_lmart_nl19lr0.4nt6\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './bioasq_dir/bioasq_dev_qrels', './bioasq_dir/run_bioasq_lmart_nl19lr0.4nt6']\n",
      "Run error:  None\n",
      "Model saved:  ./bioasq_dir/bioasq_lmart_nl3lr0.6nt25_model\n",
      "None\n",
      "./bioasq_dir/run_bioasq_lmart_nl3lr0.6nt25\n",
      "./bioasq_dir/pre_run_bioasq_lmart_nl3lr0.6nt25\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './bioasq_dir/bioasq_dev_qrels', './bioasq_dir/run_bioasq_lmart_nl3lr0.6nt25']\n",
      "Run error:  None\n",
      "Model saved:  ./bioasq_dir/bioasq_lmart_nl1lr0.9nt22_model\n",
      "None\n",
      "./bioasq_dir/run_bioasq_lmart_nl1lr0.9nt22\n",
      "./bioasq_dir/pre_run_bioasq_lmart_nl1lr0.9nt22\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './bioasq_dir/bioasq_dev_qrels', './bioasq_dir/run_bioasq_lmart_nl1lr0.9nt22']\n",
      "Run error:  None\n",
      "Model saved:  ./bioasq_dir/bioasq_lmart_nl28lr0.1nt21_model\n",
      "None\n",
      "./bioasq_dir/run_bioasq_lmart_nl28lr0.1nt21\n",
      "./bioasq_dir/pre_run_bioasq_lmart_nl28lr0.1nt21\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './bioasq_dir/bioasq_dev_qrels', './bioasq_dir/run_bioasq_lmart_nl28lr0.1nt21']\n",
      "Run error:  None\n",
      "Model saved:  ./bioasq_dir/bioasq_lmart_nl25lr0.9nt17_model\n",
      "Model saved:  ./bioasq_dir/bioasq_lmart_nl27lr0.4nt41_model\n",
      "None\n",
      "./bioasq_dir/run_bioasq_lmart_nl25lr0.9nt17\n",
      "./bioasq_dir/pre_run_bioasq_lmart_nl25lr0.9nt17\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './bioasq_dir/bioasq_dev_qrels', './bioasq_dir/run_bioasq_lmart_nl25lr0.9nt17']\n",
      "Run error:  None\n",
      "None\n",
      "./bioasq_dir/run_bioasq_lmart_nl27lr0.4nt41\n",
      "./bioasq_dir/pre_run_bioasq_lmart_nl27lr0.4nt41\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './bioasq_dir/bioasq_dev_qrels', './bioasq_dir/run_bioasq_lmart_nl27lr0.4nt41']\n",
      "Run error:  None\n",
      "Model saved:  ./bioasq_dir/bioasq_lmart_nl23lr0.2nt34_model\n",
      "None\n",
      "./bioasq_dir/run_bioasq_lmart_nl23lr0.2nt34\n",
      "./bioasq_dir/pre_run_bioasq_lmart_nl23lr0.2nt34\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './bioasq_dir/bioasq_dev_qrels', './bioasq_dir/run_bioasq_lmart_nl23lr0.2nt34']\n",
      "Run error:  None\n",
      "Model saved:  ./bioasq_dir/bioasq_lmart_nl40lr0.3nt16_model\n",
      "None\n",
      "./bioasq_dir/run_bioasq_lmart_nl40lr0.3nt16\n",
      "./bioasq_dir/pre_run_bioasq_lmart_nl40lr0.3nt16\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './bioasq_dir/bioasq_dev_qrels', './bioasq_dir/run_bioasq_lmart_nl40lr0.3nt16']\n",
      "Run error:  None\n",
      "Model saved:  ./bioasq_dir/bioasq_lmart_nl40lr0.8nt36_model\n",
      "None\n",
      "./bioasq_dir/run_bioasq_lmart_nl40lr0.8nt36\n",
      "./bioasq_dir/pre_run_bioasq_lmart_nl40lr0.8nt36\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './bioasq_dir/bioasq_dev_qrels', './bioasq_dir/run_bioasq_lmart_nl40lr0.8nt36']\n",
      "Run error:  None\n",
      "Model saved:  ./bioasq_dir/bioasq_lmart_nl22lr0.1nt39_model\n",
      "None\n",
      "./bioasq_dir/run_bioasq_lmart_nl22lr0.1nt39\n",
      "./bioasq_dir/pre_run_bioasq_lmart_nl22lr0.1nt39\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './bioasq_dir/bioasq_dev_qrels', './bioasq_dir/run_bioasq_lmart_nl22lr0.1nt39']\n",
      "Run error:  None\n",
      "Model saved:  ./bioasq_dir/bioasq_lmart_nl41lr0.1nt47_model\n",
      "None\n",
      "./bioasq_dir/run_bioasq_lmart_nl41lr0.1nt47\n",
      "./bioasq_dir/pre_run_bioasq_lmart_nl41lr0.1nt47\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './bioasq_dir/bioasq_dev_qrels', './bioasq_dir/run_bioasq_lmart_nl41lr0.1nt47']\n",
      "Run error:  None\n",
      "Model saved:  ./bioasq_dir/bioasq_lmart_nl43lr0.6nt17_model\n",
      "None\n",
      "./bioasq_dir/run_bioasq_lmart_nl43lr0.6nt17\n",
      "./bioasq_dir/pre_run_bioasq_lmart_nl43lr0.6nt17\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './bioasq_dir/bioasq_dev_qrels', './bioasq_dir/run_bioasq_lmart_nl43lr0.6nt17']\n",
      "Run error:  None\n",
      "Total parameters: 20\n"
     ]
    }
   ],
   "source": [
    "# hpo_params_list = [hpo_params]\n",
    "pool_size = 5\n",
    "\n",
    "hpo_results = eval_multi_hpo(params_list, hpo_params_list, pool_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_hpo = max(hpo_results, key=lambda x: x['map'])\n",
    "best_model_hpo['random_iterations'] = random_iterations\n",
    "best_model_hpo['data_split'] = 'validation'\n",
    "best_lmart_model = best_model_hpo.pop('lmart_model')\n",
    "best_model_hpo = {k: str(v) for k, v in best_model_hpo.items()}\n",
    "\n",
    "hpo_results_file = fold_dir +  dataset + '_' + fold + '_' + l2r_model + '_hpo_results.pickle'\n",
    "best_model_hpo_file = fold_dir +  'best_' + dataset + '_' + fold + '_' + l2r_model + '_hparams.json'\n",
    "\n",
    "pickle.dump(hpo_results, open(hpo_results_file, \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'map': '0.3276',\n",
       "  'P_20': '0.1860',\n",
       "  'ndcg_cut_20': '0.4021',\n",
       "  'n_leaves': 48,\n",
       "  'learning_rate': 0.9,\n",
       "  'n_trees': 2,\n",
       "  'random_iterations': 20,\n",
       "  'data_split': 'validation'},\n",
       " {'map': '0.3111',\n",
       "  'P_20': '0.1705',\n",
       "  'ndcg_cut_20': '0.3816',\n",
       "  'n_leaves': 24,\n",
       "  'learning_rate': 0.2,\n",
       "  'n_trees': 20,\n",
       "  'lmart_model': <__main__.L2Ranker at 0x7f360bf8b438>},\n",
       " {'map': '0.2979',\n",
       "  'P_20': '0.1645',\n",
       "  'ndcg_cut_20': '0.3691',\n",
       "  'n_leaves': 38,\n",
       "  'learning_rate': 0.6,\n",
       "  'n_trees': 33,\n",
       "  'lmart_model': <__main__.L2Ranker at 0x7f360bf1b080>},\n",
       " {'map': '0.2744',\n",
       "  'P_20': '0.1465',\n",
       "  'ndcg_cut_20': '0.3334',\n",
       "  'n_leaves': 11,\n",
       "  'learning_rate': 0.2,\n",
       "  'n_trees': 39,\n",
       "  'lmart_model': <__main__.L2Ranker at 0x7f360bf1b2e8>},\n",
       " {'map': '0.3126',\n",
       "  'P_20': '0.1695',\n",
       "  'ndcg_cut_20': '0.3863',\n",
       "  'n_leaves': 46,\n",
       "  'learning_rate': 0.9,\n",
       "  'n_trees': 34,\n",
       "  'lmart_model': <__main__.L2Ranker at 0x7f360bf8b400>},\n",
       " {'map': '0.3052',\n",
       "  'P_20': '0.1680',\n",
       "  'ndcg_cut_20': '0.3769',\n",
       "  'n_leaves': 31,\n",
       "  'learning_rate': 0.5,\n",
       "  'n_trees': 28,\n",
       "  'lmart_model': <__main__.L2Ranker at 0x7f360bf1b550>},\n",
       " {'map': '0.2746',\n",
       "  'P_20': '0.1465',\n",
       "  'ndcg_cut_20': '0.3343',\n",
       "  'n_leaves': 7,\n",
       "  'learning_rate': 0.5,\n",
       "  'n_trees': 41,\n",
       "  'lmart_model': <__main__.L2Ranker at 0x7f360bf1ba20>},\n",
       " {'map': '0.2309',\n",
       "  'P_20': '0.1300',\n",
       "  'ndcg_cut_20': '0.2826',\n",
       "  'n_leaves': 3,\n",
       "  'learning_rate': 0.4,\n",
       "  'n_trees': 8,\n",
       "  'lmart_model': <__main__.L2Ranker at 0x7f360bf1b7b8>},\n",
       " {'map': '0.3073',\n",
       "  'P_20': '0.1690',\n",
       "  'ndcg_cut_20': '0.3771',\n",
       "  'n_leaves': 19,\n",
       "  'learning_rate': 0.4,\n",
       "  'n_trees': 6,\n",
       "  'lmart_model': <__main__.L2Ranker at 0x7f360bf1bc88>},\n",
       " {'map': '0.3063',\n",
       "  'P_20': '0.1680',\n",
       "  'ndcg_cut_20': '0.3773',\n",
       "  'n_leaves': 28,\n",
       "  'learning_rate': 0.1,\n",
       "  'n_trees': 21,\n",
       "  'lmart_model': <__main__.L2Ranker at 0x7f360bf1a400>},\n",
       " {'map': '0.3108',\n",
       "  'P_20': '0.1705',\n",
       "  'ndcg_cut_20': '0.3815',\n",
       "  'n_leaves': 23,\n",
       "  'learning_rate': 0.2,\n",
       "  'n_trees': 34,\n",
       "  'lmart_model': <__main__.L2Ranker at 0x7f360bf1ab38>},\n",
       " {'map': '0.2304',\n",
       "  'P_20': '0.1295',\n",
       "  'ndcg_cut_20': '0.2810',\n",
       "  'n_leaves': 1,\n",
       "  'learning_rate': 0.9,\n",
       "  'n_trees': 22,\n",
       "  'lmart_model': <__main__.L2Ranker at 0x7f360bf1a198>},\n",
       " {'map': '0.2309',\n",
       "  'P_20': '0.1300',\n",
       "  'ndcg_cut_20': '0.2826',\n",
       "  'n_leaves': 3,\n",
       "  'learning_rate': 0.6,\n",
       "  'n_trees': 25,\n",
       "  'lmart_model': <__main__.L2Ranker at 0x7f360bf1bef0>},\n",
       " {'map': '0.3063',\n",
       "  'P_20': '0.1680',\n",
       "  'ndcg_cut_20': '0.3773',\n",
       "  'n_leaves': 27,\n",
       "  'learning_rate': 0.4,\n",
       "  'n_trees': 41,\n",
       "  'lmart_model': <__main__.L2Ranker at 0x7f360bf1a8d0>},\n",
       " {'map': '0.3111',\n",
       "  'P_20': '0.1705',\n",
       "  'ndcg_cut_20': '0.3816',\n",
       "  'n_leaves': 25,\n",
       "  'learning_rate': 0.9,\n",
       "  'n_trees': 17,\n",
       "  'lmart_model': <__main__.L2Ranker at 0x7f360bf1a668>},\n",
       " {'map': '0.3071',\n",
       "  'P_20': '0.1660',\n",
       "  'ndcg_cut_20': '0.3790',\n",
       "  'n_leaves': 41,\n",
       "  'learning_rate': 0.1,\n",
       "  'n_trees': 47,\n",
       "  'lmart_model': <__main__.L2Ranker at 0x7f360bf24518>},\n",
       " {'map': '0.3108',\n",
       "  'P_20': '0.1705',\n",
       "  'ndcg_cut_20': '0.3815',\n",
       "  'n_leaves': 22,\n",
       "  'learning_rate': 0.1,\n",
       "  'n_trees': 39,\n",
       "  'lmart_model': <__main__.L2Ranker at 0x7f360bf242b0>},\n",
       " {'map': '0.3084',\n",
       "  'P_20': '0.1660',\n",
       "  'ndcg_cut_20': '0.3805',\n",
       "  'n_leaves': 40,\n",
       "  'learning_rate': 0.8,\n",
       "  'n_trees': 36,\n",
       "  'lmart_model': <__main__.L2Ranker at 0x7f360bf24048>},\n",
       " {'map': '0.3084',\n",
       "  'P_20': '0.1660',\n",
       "  'ndcg_cut_20': '0.3805',\n",
       "  'n_leaves': 40,\n",
       "  'learning_rate': 0.3,\n",
       "  'n_trees': 16,\n",
       "  'lmart_model': <__main__.L2Ranker at 0x7f360bf1ada0>},\n",
       " {'map': '0.3087',\n",
       "  'P_20': '0.1685',\n",
       "  'ndcg_cut_20': '0.3807',\n",
       "  'n_leaves': 43,\n",
       "  'learning_rate': 0.6,\n",
       "  'n_trees': 17,\n",
       "  'lmart_model': <__main__.L2Ranker at 0x7f360bf24780>}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpo_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "./bioasq_dir/run_bioasq__test_lmart_best\n",
      "./bioasq_dir/pre_run_bioasq__test_lmart_best\n",
      "['../../eval/trec_eval', '-m', 'map', '-m', 'P.20', '-m', 'ndcg_cut.20', './bioasq_dir/bioasq_test__qrels', './bioasq_dir/run_bioasq__test_lmart_best']\n",
      "Run error:  None\n"
     ]
    }
   ],
   "source": [
    "qrels_test_file = fold_dir + dataset + '_' + 'test_' + fold + '_qrels'\n",
    "\n",
    "run_file = fold_dir + 'run_' + dataset + '_' + fold + '_test_lmart_best'\n",
    "\n",
    "best_lmart_model.gen_run_file(test_data_file, run_file)\n",
    "\n",
    "eval_results = eval(trec_eval_command, qrels_test_file, run_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results['data_split'] = 'test'\n",
    "eval_results\n",
    "\n",
    "best_model_hpo['test'] = eval_results\n",
    "\n",
    "with open(best_model_hpo_file, 'wt') as best_model_f:\n",
    "    json.dump(best_model_hpo, best_model_f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
