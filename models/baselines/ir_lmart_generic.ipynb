{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ir_lmart.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "# Stronger baseline: Listwise L2R - LambdaMART\n",
    "# Hyperparameter optimziation HPonsteroids requires Python 3!\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# REMOVE!!\n",
    "from eval_utils import *\n",
    "\n",
    "# HPO\n",
    "import hpo_utils\n",
    "\n",
    "import ConfigSpace as CS\n",
    "import ConfigSpace.hyperparameters as CSH\n",
    "\n",
    "from hpbandster.core.worker import Worker\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "# HPO server and stuff\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "import argparse\n",
    "\n",
    "import hpbandster.core.nameserver as hpns\n",
    "import hpbandster.core.result as hpres\n",
    "\n",
    "from hpbandster.optimizers import BOHB as BOHB\n",
    "from hpbandster.optimizers import RandomSearch as RS\n",
    "from hpbandster.examples.commons import MyWorker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[3]:\n",
    "\n",
    "\n",
    "# Functions\n",
    "def generate_run_file(pre_run_file, run_file):\n",
    "    \n",
    "    with open(pre_run_file, 'rt') as input_f:\n",
    "        pre_run = input_f.readlines()\n",
    "#         print(type(pre_run))\n",
    "    with open(run_file, 'wt') as out_f:\n",
    "        for line in pre_run:\n",
    "            out_f.write(line.replace('docid=','').replace('indri', 'lambdaMART'))\n",
    "        \n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "# Classes\n",
    "class L2Ranker:\n",
    "    def __init__(self, ranklib_location, params, normalization=[]):\n",
    "        self.ranklib_location = ranklib_location\n",
    "        # Works with Oracle JSE\n",
    "        # java version \"1.8.0_211\"\n",
    "        # Java(TM) SE Runtime Environment (build 1.8.0_211-b12)\n",
    "        # Java HotSpot(TM) 64-Bit Server VM (build 25.211-b12, mixed mode)\n",
    "        self.params = params\n",
    "        self.ranker_command = ['java', '-jar', ranklib_location + 'RankLib-2.12.jar']\n",
    "        self.normalization = normalization\n",
    "        self.save_model_file = ''\n",
    "        \n",
    "#     def build(self, ir_tool_params):\n",
    "    def train(self, train_data_file, save_model_file, config):\n",
    "        self.save_model_file = save_model_file\n",
    "        self.log_file = save_model_file + '.log'\n",
    "        toolkit_parameters = [\n",
    "                                *self.ranker_command, # * to unpack list elements\n",
    "                                '-train',\n",
    "                                train_data_file,\n",
    "                                *self.normalization,\n",
    "                                *self.params,\n",
    "                                '-leaf', \n",
    "                                str(config['n_leaves']),\n",
    "                                '-shrinkage',\n",
    "                                str(config['learning_rate']),\n",
    "                                '-tree', # Oner regression tree per boosted iteration\n",
    "                                str(config['n_trees']),\n",
    "                                '-save',\n",
    "                                self.save_model_file   \n",
    "                            ] \n",
    "        \n",
    "        print(toolkit_parameters)\n",
    "        with open(self.log_file, 'wt') as rf:\n",
    "            proc = subprocess.Popen(toolkit_parameters,stdin=subprocess.PIPE, stdout=rf, stderr=subprocess.PIPE, shell=False)\n",
    "#         proc = subprocess.Popen(toolkit_parameters,stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=False)\n",
    "            \n",
    "        (out, err)= proc.communicate()\n",
    "#         print(out.decode('utf-8').splitlines())\n",
    "#         print(out)\n",
    "\n",
    "        if err == b'':\n",
    "            print('Model saved: ', self.save_model_file)\n",
    "        else:\n",
    "#             print('error:', err, type(err))\n",
    "            print('Something went wrong on training, see log: ', self.log_file)\n",
    "            \n",
    "  \n",
    "\n",
    "    def gen_run_file(self, test_data_file, run_file):\n",
    "        pre_run_file = run_file.replace('run_', 'pre_run_', 1)\n",
    "        toolkit_parameters = [\n",
    "                                *self.ranker_command, # * to unpack list elements\n",
    "                                '-load',\n",
    "                                self.save_model_file,\n",
    "                                *self.normalization,\n",
    "                                '-rank',\n",
    "                                test_data_file,\n",
    "                                '-indri',\n",
    "                                pre_run_file     \n",
    "                            ] \n",
    "        \n",
    "        print(toolkit_parameters)\n",
    "        with open(self.log_file, 'at') as rf:\n",
    "            proc = subprocess.Popen(toolkit_parameters,stdin=subprocess.PIPE, stdout=rf, stderr=subprocess.STDOUT, shell=False)\n",
    "#         proc = subprocess.Popen(toolkit_parameters,stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=False)\n",
    "            \n",
    "        (out, err)= proc.communicate()\n",
    "#         print(out.decode('utf-8').splitlines())\n",
    "#         print(out)\n",
    "#         print(err)\n",
    "    \n",
    "        print(run_file)\n",
    "        print(pre_run_file)\n",
    "        \n",
    "        generate_run_file(pre_run_file, run_file)\n",
    "        \n",
    "#         print('Run model saved: ', run_file)\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "# try:\n",
    "#     import keras\n",
    "#     from keras.datasets import mnist\n",
    "#     from keras.models import Sequential\n",
    "#     from keras.layers import Dense, Dropout, Flatten\n",
    "#     from keras.layers import Conv2D, MaxPooling2D\n",
    "#     from keras import backend as K\n",
    "# except:\n",
    "#     raise ImportError(\"For this example you need to install keras.\")\n",
    "\n",
    "# try:\n",
    "#     import torchvision\n",
    "#     import torchvision.transforms as transforms\n",
    "# except:\n",
    "#     raise ImportError(\"For this example you need to install pytorch-vision.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Options and variables\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Example 1 - sequential and local execution.')\n",
    "    parser.add_argument('--min_budget',   type=float, help='Minimum budget used during the optimization.',    default=2)\n",
    "    parser.add_argument('--max_budget',   type=float, help='Maximum budget used during the optimization.',    default=4)\n",
    "    parser.add_argument('--n_iterations', type=int,   help='Number of iterations performed by the optimizer', default=500)\n",
    "    parser.add_argument('--n_workers', type=int,   help='Number of workers to run in parallel.', default=5)\n",
    "\n",
    "#     args=parser.parse_args()\n",
    "    args = fakeParser()\n",
    "    \n",
    "#     dataset = sys.argv[1] # 'bioasq'\n",
    "#     workdir = './' + dataset + '_dir/'\n",
    "#     data_split = sys.argv[2] # 'test'\n",
    "\n",
    "    dataset = 'robust'\n",
    "    workdir = './' + dataset + '_dir/'\n",
    "    data_split =  'train'\n",
    "    fold = 's1'  \n",
    "    \n",
    "    fold_dir = workdir + fold + '/'\n",
    "    \n",
    "    if not os.path.exists(fold_dir):\n",
    "        os.makedirs(fold_dir)\n",
    "        \n",
    "    ranklib_location = '../../../ranklib/'\n",
    "    \n",
    "    \n",
    "    train_data_file = fold_dir + 'rob04.train.' + fold + '_features'\n",
    "    val_data_file = fold_dir + 'rob04.dev.'   +fold + '_features'\n",
    "    test_data_file = fold_dir + 'rob04.test.' +fold + '_features'\n",
    "    \n",
    "    l2r_model = '_lmart_'\n",
    "    \n",
    "   \n",
    "    enabled_features_file = workdir + dataset + l2r_model + 'enabled_features' # dont'f change to fold_dir!\n",
    "    \n",
    "#     print(enabled_features_file)\n",
    "    # Train L2R model: LambdaMART\n",
    "    # Parameters \n",
    "    \n",
    "    n_leaves = '10'\n",
    "    learning_rate = '0.1'\n",
    "    n_trees = '1000'\n",
    "    hpo_params = {'n_leaves': n_leaves, 'learning_rate': learning_rate, 'n_trees': n_trees}\n",
    "    \n",
    "    \n",
    "    \n",
    "    metric2t = 'MAP' # 'MAP, NDCG@k, DCG@k, P@k, RR@k, ERR@k (default=ERR@10)'\n",
    "    \n",
    "    ranker_type = '6' # LambdaMART\n",
    "    \n",
    "    # normalization: Feature Engineering?\n",
    "#     norm_params = ['-norm', 'zscore'] # 'sum', 'zscore', 'linear'\n",
    "    \n",
    "    norm_params = ['-norm', 'zscore'] # 'sum', 'zscore', 'linear'\n",
    "    \n",
    "    l2r_params = [\n",
    "        '-validate',\n",
    "        val_data_file,\n",
    "        '-ranker',\n",
    "        ranker_type,\n",
    "        '-metric2t',\n",
    "        metric2t,\n",
    "        '-feature',\n",
    "        enabled_features_file\n",
    "    ]\n",
    "    \n",
    "    # Run train\n",
    "    \n",
    "    lmart_model = L2Ranker(ranklib_location, l2r_params)\n",
    "#     lmart_model = L2Ranker(ranklib_location, l2r_params, norm_params)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     save_model_file = fold_dir + 'rob04.s1_lmart_model'\n",
    "    \n",
    "#     lmart_model.train(train_data_file, save_model_file, hpo_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     run_file = fold_dir + 'run_' + 'rob04.s1_lmart'\n",
    "    \n",
    "# #     lmart_model.gen_run_file(test_data_file, run_file)\n",
    "\n",
    "#     lmart_model.gen_run_file(val_data_file, run_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     trec_eval_command = '../../eval/trec_eval'\n",
    "#     qrels_val_file = fold_dir + 'rob04.dev.s1_qrels'\n",
    "#     print(qrels_val_file)\n",
    "#     print(run_file)\n",
    "#     eval(trec_eval_command, qrels_val_file, run_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trec_eval_command = '../../eval/trec_eval'\n",
    "\n",
    "def get_best_hpo(train_data_file, val_data_file, hpo_params, fold_dir, lmart_model):\n",
    "    hpo_params_suffix = 'nl' + str(hpo_params['n_leaves']) + 'lr' + str(hpo_params['learning_rate']) + 'nt' + str(hpo_params['n_trees'])\n",
    "    save_model_file = fold_dir + 'rob04.s1_lmart_' + hpo_params_suffix + '_model' \n",
    "    \n",
    "    lmart_model.train(train_data_file, save_model_file, hpo_params)\n",
    "    run_file = fold_dir + 'run_' + 'rob04.s1_lmart_' + hpo_params_suffix\n",
    "    \n",
    "#   lmart_model.gen_run_file(test_data_file, run_file)\n",
    "\n",
    "    lmart_model.gen_run_file(val_data_file, run_file)\n",
    "    qrels_val_file = fold_dir + 'rob04.dev.s1_qrels'\n",
    "    print(qrels_val_file)\n",
    "    print(run_file)\n",
    "    eval_results = eval(trec_eval_command, qrels_val_file, run_file)\n",
    "    eval_results = eval_results.splitlines()\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['java', '-jar', '../../../ranklib/RankLib-2.12.jar', '-train', './robust_dir/s1/rob04.train.s1_features', '-validate', './robust_dir/s1/rob04.dev.s1_features', '-ranker', '6', '-metric2t', 'MAP', '-feature', './robust_dir/robust_lmart_enabled_features', '-leaf', '10', '-shrinkage', '0.1', '-tree', '1000', '-save', './robust_dir/s1/rob04.s1_lmart_nl10lr0.1nt1000_model']\n",
      "Model saved:  ./robust_dir/s1/rob04.s1_lmart_nl10lr0.1nt1000_model\n",
      "['java', '-jar', '../../../ranklib/RankLib-2.12.jar', '-load', './robust_dir/s1/rob04.s1_lmart_nl10lr0.1nt1000_model', '-rank', './robust_dir/s1/rob04.dev.s1_features', '-indri', './robust_dir/s1/pre_run_rob04.s1_lmart_nl10lr0.1nt1000']\n",
      "./robust_dir/s1/run_rob04.s1_lmart_nl10lr0.1nt1000\n",
      "./robust_dir/s1/pre_run_rob04.s1_lmart_nl10lr0.1nt1000\n",
      "./robust_dir/s1/rob04.dev.s1_qrels\n",
      "./robust_dir/s1/run_rob04.s1_lmart_nl10lr0.1nt1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['map                   \\tall\\t0.1968',\n",
       " 'P_20                  \\tall\\t0.3112',\n",
       " 'ndcg_cut_20           \\tall\\t0.3689']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_best_hpo(train_data_file, val_data_file, hpo_params, fold_dir, lmart_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
