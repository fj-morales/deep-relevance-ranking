{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating query performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import gzip\n",
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "# dataloc = '../../bioasq_data/'\n",
    "dataloc = '../../robust04_data/'\n",
    "baseline_files ='./baseline_files/'\n",
    "galago_loc='./galago-3.10-bin/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select split to work with\n",
    "split = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_to_json(pickle_filename):\n",
    "    # Pickle to Trectext converter\n",
    "    doc_list = []\n",
    "    with open(dataloc + pickle_filename, 'rb') as f_in:\n",
    "        data = pickle.load(f_in)\n",
    "        if not os.path.exists(baseline_files):\n",
    "            os.makedirs(baseline_files)\n",
    "        out_file = baseline_files + pickle_filename[:-4] + '.gz'\n",
    "        with gzip.open(out_file,'wt', encoding='utf-8') as f_out:\n",
    "            docu = {}\n",
    "            for key, value in data.items():\n",
    "                if \"pmid\" in value.keys():\n",
    "                    doc_code = value.pop('pmid')\n",
    "                else:\n",
    "                    doc_code = key\n",
    "                f_out.write('<DOC>\\n' + \n",
    "                            '<DOCNO>' + doc_code + '</DOCNO>\\n' +\n",
    "                            '<TITLE>' + value.pop('title') + '</TITLE>\\n' +\n",
    "                            '<TEXT>' + value.pop('abstractText') + '</TEXT>\\n' + \n",
    "                            '</DOC>\\n')\n",
    "                doc_list.append(doc_code)\n",
    "        return [out_file, doc_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build corpus index \n",
    "def build_index(index_input, index_loc):\n",
    "    index_input_param = '--inputPath+' + index_input    \n",
    "    index_loc_param = '--indexPath=' + index_loc\n",
    "    print(index_input_param)\n",
    "    print(index_loc_param)\n",
    "    if not os.path.exists(index_loc):\n",
    "            os.makedirs(index_loc) \n",
    "    index_proc = subprocess.Popen(\n",
    "            [galago_loc + 'galago', 'build', '--stemmer+krovetz',\n",
    "                index_input_param, index_loc_param],\n",
    "            stdout=subprocess.PIPE, shell=False)\n",
    "    (out, err) = index_proc.communicate()\n",
    "    print(out.decode(\"utf-8\"))\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return top 100 bm25 scored docs, given query and corpus indexed by galago\n",
    "def get_bm25_docs(query, index_loc, b_val=0.75, k_val=1.2):\n",
    "    query = re.sub(r'[^\\w\\s]',' ',query)\n",
    "    query = query.lower()\n",
    "#     query = query.rstrip('.?')\n",
    "    index_loc_param = '--index=' + index_loc  \n",
    "    b=' --b=' + str(b_val)\n",
    "    k=' --k=' + str(k_val)\n",
    "    if \"'\" in query:\n",
    "        query_param = '--query=\"#stopword(' + query + ')\"' \n",
    "    else:\n",
    "        query_param = '--query=\\'#stopword(' + query + ')\\'' \n",
    "\n",
    "    command = galago_loc + 'galago batch-search --verbose=false --requested=100 ' + \\\n",
    "         index_loc_param + ' --scorer=bm25' + \\\n",
    "         b + \\\n",
    "         k + \\\n",
    "         ' --stemmer+krovetz ' + \\\n",
    "         query_param + ' | cut -d\" \" -f3'\n",
    "#     print(command)\n",
    "    galago_bm25_exec = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True)\n",
    "    (out, err) = galago_bm25_exec.communicate()\n",
    "    bm25_documents = out.decode(\"utf-8\")\n",
    "    return bm25_documents.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_files = [ x for x in os.listdir(dataloc) if all(y in x for y in ['docset', '.pkl'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert pickle to trectext file format to be processed with galago\n",
    "\n",
    "\n",
    "# pkl_file = [s for s in pkl_files if split in s]\n",
    "# [output_file, doc_list ]= pickle_to_json(pkl_file[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./baseline_files/rob04_bm25_docset_top1000.test.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = \n",
    "output_file = './baseline_files/rob04_bm25_docset_top1000.test.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split = split\n",
    "print(data_split)\n",
    "\n",
    "if \"rob04\" in output_file:\n",
    "    s = re.findall(\"(s[0-5]).pkl$\", pkl_file[0])\n",
    "    dataset_name = \"rob04\"\n",
    "#     dataset_name_ext = dataset_name + '_'+ s[0]\n",
    "    dataset_name_ext = dataset_name\n",
    "    print(dataset_name_ext)\n",
    "elif \"bioasq\" in output_file:\n",
    "    print(\"bioasq\")\n",
    "    dataset_name = \"bioasq\"\n",
    "    dataset_name_ext = dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_loc = baseline_files + 'index' + '_' + dataset_name_ext + '_' + data_split\n",
    "index_input = output_file\n",
    "build_index(index_input, index_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_filename = [ x for x in os.listdir(dataloc) if all(y in x for y in [dataset_name +'.'+ data_split, '.json'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_file = dataloc + q_filename[0]\n",
    "with open(queries_file, 'rb') as input_file:\n",
    "    query_data = json.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_preds(file, preds):\n",
    "    with open(file, 'wt') as f_out:\n",
    "        json.dump(preds, f_out, indent=4)\n",
    "    print('Predictions file: ' + file + ', done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(index_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bm25_computing(b_k):\n",
    "#     b = b_k[0]\n",
    "#     k = b_k[1]\n",
    "#     bm25_preds_file = baseline_files + 'bm25_preds_' + dataset_name_ext + '_'+ data_split + '_' + 'b' + str(b) + 'k' + str(k) + '.json'\n",
    "# #     print(bm25_preds_file)\n",
    "#     if os.path.isfile(bm25_preds_file):\n",
    "#         print(bm25_preds_file + \"Already exists!!\")\n",
    "#         return\n",
    "#     bm25_preds = {}\n",
    "#     questions = []\n",
    "#     question = {}\n",
    "#     for query in query_data['questions']:\n",
    "#         question['body'] = query['body']\n",
    "#         question['id'] = query['id']\n",
    "#     #     print(query['body'].rstrip('.'))\n",
    "#     #     documents = get_bm25_docs(query['body'].rstrip('.'), index_loc)\n",
    "#         documents = get_bm25_docs(query['body'], index_loc, b, k)\n",
    "#         if \"bioasq\" in dataset_name: \n",
    "#             documents_url = ['http://www.ncbi.nlm.nih.gov/pubmed/' + doc for doc in documents]\n",
    "#             question['documents'] = documents_url\n",
    "#         elif \"rob04\" in dataset_name:\n",
    "#             question['documents'] = documents\n",
    "#         questions.append(dict(question))\n",
    "    \n",
    "#     bm25_preds['questions'] = questions\n",
    "#     save_preds(bm25_preds_file, bm25_preds)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_process():\n",
    "    print( 'Starting', multiprocessing.current_process().name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_question(query):\n",
    "    question = {}\n",
    "    question['body'] = query['body']\n",
    "    question['id'] = query['id']\n",
    "#     print(query['body'].rstrip('.'))\n",
    "#     documents = get_bm25_docs(query['body'].rstrip('.'), index_loc)\n",
    "    documents = get_bm25_docs(query['body'], index_loc)\n",
    "    if \"bioasq\" in dataset_name: \n",
    "        documents_url = ['http://www.ncbi.nlm.nih.gov/pubmed/' + doc for doc in documents]\n",
    "        question['documents'] = documents_url\n",
    "    elif \"rob04\" in dataset_name:\n",
    "        question['documents'] = documents\n",
    "    return dict(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bm25_computing(b_k):\n",
    "#     b = b_k[0]\n",
    "#     k = b_k[1]\n",
    "b = '0.75'\n",
    "k = '1.2'\n",
    "bm25_preds_file = baseline_files + 'bm25_preds_' + dataset_name_ext + '_' + data_split + '_' + 'b' + str(b) + 'k' + str(k) + '.json'\n",
    "#     print(bm25_preds_file)\n",
    "if os.path.isfile(bm25_preds_file):\n",
    "    print(bm25_preds_file + \"Already exists!!\")\n",
    "#     return\n",
    "bm25_preds = {}\n",
    "questions = []\n",
    "pool_size = 50\n",
    "pool = multiprocessing.Pool(processes=pool_size,\n",
    "                            initializer=start_process,\n",
    "                            )\n",
    "questions = pool.map(extract_question, query_data['questions'])\n",
    "pool.close() # no more tasks\n",
    "pool.join()  # wrap up current tasks\n",
    "    \n",
    "bm25_preds['questions'] = questions\n",
    "save_preds(bm25_preds_file, bm25_preds)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     grid_search = 'no'\n",
    "#     if grid_search == 'yes':\n",
    "#         brange = np.arange(0.2,1,0.1)\n",
    "#         krange = np.arange(0.5,2,0.1)\n",
    "#     else:\n",
    "#         brange = [0.2]\n",
    "#         krange = [0.8]\n",
    "\n",
    "#     b_k = [(round(b,2), round(k,2)) for b in brange for k in krange]\n",
    "#     pool_size = 8\n",
    "#     pool = multiprocessing.Pool(processes=pool_size,\n",
    "#                                 initializer=start_process,\n",
    "#                                 )\n",
    "#     pool_outputs = pool.map(bm25_computing, b_k)\n",
    "#     pool.close() # no more tasks\n",
    "#     pool.join()  # wrap up current tasks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
