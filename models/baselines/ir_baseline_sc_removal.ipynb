{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating query performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import gzip\n",
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "dataloc = '../../bioasq_data/'\n",
    "# dataloc = '../../robust04_data/split_5/'\n",
    "baseline_files ='./baseline_files/'\n",
    "galago_loc='./galago-3.10-bin/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select data split to work with\n",
    "split = \"test\"\n",
    "# split = \"dev\"\n",
    "# split = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sc(text):\n",
    "#     text = re.sub('[.,?;*!%^&_+():-\\[\\]{}]', '', text.replace('\"', '').replace('/', '').replace('\\\\', '').replace(\"'\", '').strip())\n",
    "#     text = re.sub('[\\[\\]{}.,?;*!%^&_+():-]', '', text.replace('\"', '').replace('/', '').replace('\\\\', '').replace(\"'\", '').strip()) # DeepPaper method\n",
    "    text = re.sub(r'[^\\w\\s]',' ',text) # My method\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pickle_docs(pickle_filename):\n",
    "    # Pickle to Trectext converter\n",
    "    with open(pickle_filename, 'rb') as f_in:\n",
    "        data = pickle.load(f_in)\n",
    "        if not os.path.exists(baseline_files):\n",
    "            os.makedirs(baseline_files)\n",
    "        docs = {}\n",
    "        for key, value in data.items():\n",
    "            if \"pmid\" in value.keys():\n",
    "                doc_code = value.pop('pmid')\n",
    "            else:\n",
    "                doc_code = key\n",
    "            doc = '<DOC>\\n' + \\\n",
    "                  '<DOCNO>' + doc_code + '</DOCNO>\\n' + \\\n",
    "                  '<TITLE>' + remove_sc(value.pop('title')) + '</TITLE>\\n' + \\\n",
    "                  '<TEXT>' + remove_sc(value.pop('abstractText')) + '</TEXT>\\n' + \\\n",
    "                  '</DOC>\\n'\n",
    "            docs[doc_code] = doc\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_jsonfile(docs, filename):\n",
    "    # Pickle to Trectext converter\n",
    "    doc_list = []\n",
    "    with gzip.open(filename,'wt', encoding='utf-8') as f_out:\n",
    "        docus = {}\n",
    "        for key, value in docs.items():\n",
    "            f_out.write(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build corpus index \n",
    "def build_index(index_input, index_loc):\n",
    "    if not os.path.exists(index_loc):\n",
    "            os.makedirs(index_loc) \n",
    "    index_loc_param = '--indexPath=' + index_loc\n",
    "    galago_parameters = [galago_loc + 'galago', 'build', '--stemmer+krovetz']\n",
    "    [galago_parameters.append('--inputPath+' + idx) for idx in index_input]\n",
    "    galago_parameters.append(index_loc_param)\n",
    "    print(galago_parameters)\n",
    "\n",
    "    index_proc = subprocess.Popen(galago_parameters,\n",
    "            stdout=subprocess.PIPE, shell=False)\n",
    "    (out, err) = index_proc.communicate()\n",
    "    print(out.decode(\"utf-8\"))\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_queries_file(queries, filename):\n",
    "    queries_list = []\n",
    "    queries_dict = {}\n",
    "    query = {}\n",
    "    q_dict = {}\n",
    "    for q in queries:\n",
    "#         print(q['body'])\n",
    "        text = remove_sc(q['body'])\n",
    "#         print(text)\n",
    "    \n",
    "#         text = re.sub(r'[^\\w\\s]',' ',text)\n",
    "##     text = text.lower()\n",
    "##         text = text.rstrip('.?')\n",
    "    \n",
    "        q_dict[q['id']] = q['body']\n",
    "        query['number'] = q['id']\n",
    "        query['text'] = '#stopword(' + text + ')'\n",
    "        queries_list.append(dict(query))\n",
    "    queries_dict['queries'] = queries_list\n",
    "    with open(filename, 'wt', encoding='utf-8') as q_file:\n",
    "        json.dump(queries_dict, q_file, indent = 4)\n",
    "    return q_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return top 100 bm25 scored docs, given query and corpus indexed by galago\n",
    "def get_bm25_docs(queries_file, q_dict, index_loc, b_val=0.2, k_val=0.8):\n",
    "\n",
    "#     query = 'List the classical triad of symptoms of the Melkerssonâ€“Rosenthal syndrome.'\n",
    "#     print(query)\n",
    "    index_loc_param = '--index=' + index_loc  \n",
    "    b=' --b=' + str(b_val)\n",
    "    k=' --k=' + str(k_val)\n",
    "    \n",
    "    command = galago_loc + 'galago threaded-batch-search --threadCount=50 --verbose=true \\\n",
    "         --casefold=true --requested=100 ' + \\\n",
    "         index_loc_param + ' --scorer=bm25' + \\\n",
    "         b + \\\n",
    "         k + \\\n",
    "         '   ' + \\\n",
    "         queries_file + ' | cut -d\" \" -f1,3'\n",
    "#     print(command)\n",
    "#     command = command.encode('utf-8')\n",
    "    galago_bm25_exec = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True, encoding='utf-8')\n",
    "    (out, err) = galago_bm25_exec.communicate()\n",
    "    ids_docs = out.splitlines()\n",
    "    question = {}\n",
    "    bm25_docs = []\n",
    "    \n",
    "    for key, value in q_dict.items():\n",
    "        question = {}\n",
    "        question['body'] = value\n",
    "        question['id'] = key\n",
    "        \n",
    "        documents = [doc.split(' ')[1] for doc in ids_docs if key+' ' in doc]\n",
    "        if \"bioasq\" in dataset_name: \n",
    "            documents_url = ['http://www.ncbi.nlm.nih.gov/pubmed/' + doc for doc in documents]\n",
    "            question['documents'] = documents_url\n",
    "        elif \"rob04\" in dataset_name:\n",
    "            question['documents'] = documents\n",
    "        bm25_docs.append(dict(question))\n",
    "    return bm25_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_files = [os.path.join(root, name)\n",
    "             for root, dirs, files in os.walk(dataloc)\n",
    "             for name in files\n",
    "             if all(y in name for y in ['docset', split, '.pkl'])]\n",
    "\n",
    "# pkl_files = [ x for x in os.listdir(dataloc) if all(y in x for y in ['docset', '.pkl'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../bioasq_data/bioasq_bm25_docset_top100.test.pkl']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkl_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pickle to trectext file format to be processed with galago\n",
    "# pkl_file = [s for s in pkl_files if split in s]\n",
    "# [output_file, doc_list ]= pickle_to_json(pkl_file[0])\n",
    "doc_list = []\n",
    "output_files = []\n",
    "all_docs = []\n",
    "for pkl_file in pkl_files:\n",
    "#     print(pkl_file)\n",
    "    docs = get_pickle_docs(pkl_file)\n",
    "    doc_list = doc_list + list(docs.keys())\n",
    "    all_docs.append(docs)\n",
    "    out_name = pkl_file.split('/')[-1:][0]\n",
    "    out_name = re.sub('\\.pkl', '', out_name)\n",
    "    output_file = baseline_files + out_name + '.gz'\n",
    "    output_files.append(output_file)\n",
    "    # print(out_name)\n",
    "    doc_to_jsonfile(docs, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../bioasq_data/bioasq_bm25_docset_top100.test.pkl'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkl_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "bioasq\n"
     ]
    }
   ],
   "source": [
    "data_split = split\n",
    "print(data_split)\n",
    "\n",
    "if \"rob04\" in output_files[0]:\n",
    "    s = re.findall(\"(s[0-5]).pkl$\", pkl_file)\n",
    "    dataset_name = \"rob04\"\n",
    "    dataset_name_ext = dataset_name + '_'+ s[0]\n",
    "#     dataset_name_ext = dataset_name \n",
    "    gold_file = '../../robust04_data/rob04.' + split +'.json'\n",
    "#     with open(gold_file, 'w') as outfile:\n",
    "#         json.dump(query_data, outfile, indent = 4)\n",
    "    print(dataset_name_ext)\n",
    "elif \"bioasq\" in output_file:\n",
    "    print(\"bioasq\")\n",
    "    dataset_name = \"bioasq\"\n",
    "    dataset_name_ext = dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_loc = baseline_files + 'index' + '_' + dataset_name_ext + '_' + data_split\n",
    "index_input = output_files\n",
    "# build_index(index_input, index_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./baseline_files/bioasq_bm25_docset_top100.test.gz'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_filename = [ x for x in os.listdir(dataloc) if all(y in x for y in [dataset_name +'.'+ data_split, '.json'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bioasq.test.json']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries_file = dataloc + q_filename[0]\n",
    "\n",
    "def load_queries(queries_file):\n",
    "    with open(queries_file, 'rb') as input_file:\n",
    "        query_data = json.load(input_file)\n",
    "        return query_data['questions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_files = [os.path.join(root, name)\n",
    "             for root, dirs, files in os.walk(dataloc)\n",
    "             for name in files\n",
    "             if all(y in name for y in [dataset_name +'.'+ data_split, '.json'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = []\n",
    "query_data = {}\n",
    "for file in query_files:\n",
    "    queries = queries + load_queries(file)\n",
    "query_data['questions'] = queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_preds(file, preds):\n",
    "    with open(file, 'wt') as f_out:\n",
    "        json.dump(preds, f_out, indent=4)\n",
    "    print('Predictions file: ' + file + ', done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../bioasq_data/bioasq.test.json'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_files[0].strip('split_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_process():\n",
    "    print( 'Starting', multiprocessing.current_process().name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_question(query):\n",
    "    question = {}\n",
    "    question['body'] = query['body']\n",
    "    question['id'] = query['id']\n",
    "#     print(query['body'].rstrip('.'))\n",
    "#     documents = get_bm25_docs(query['body'].rstrip('.'), index_loc)\n",
    "    documents = get_bm25_docs(query['body'], index_loc)\n",
    "    if \"bioasq\" in dataset_name: \n",
    "        documents_url = ['http://www.ncbi.nlm.nih.gov/pubmed/' + doc for doc in documents]\n",
    "        question['documents'] = documents_url\n",
    "    elif \"rob04\" in dataset_name:\n",
    "        question['documents'] = documents\n",
    "    return dict(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./baseline_files/index_bioasq_test'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_bm25_docs(query_data['questions'][0]['body'], index_loc)\n",
    "index_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_queries_file = baseline_files + 'bm25_queries_' + dataset_name_ext + '_' + data_split + '.json'\n",
    "q_dict = generate_queries_file(queries,bm25_queries_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_computing(b_k):\n",
    "    b = b_k[0]\n",
    "    k = b_k[1]\n",
    "#     b = 0.2\n",
    "#     k = 0.8\n",
    "    bm25_preds_file = baseline_files + 'bm25_preds_' + dataset_name_ext + '_' + data_split + '_' + 'b' + str(b) + 'k' + str(k) + '.json'\n",
    "    #     print(bm25_preds_file)\n",
    "    if os.path.isfile(bm25_preds_file):\n",
    "        print(bm25_preds_file + \"Already exists!!\")\n",
    "    #     return\n",
    "    bm25_preds = {}\n",
    "    bm25_preds['questions'] = get_bm25_docs(bm25_queries_file, q_dict, index_loc, b, k)\n",
    "\n",
    "    save_preds(bm25_preds_file, bm25_preds)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ForkPoolWorker-41\n",
      "Starting ForkPoolWorker-42\n",
      "Starting ForkPoolWorker-43\n",
      "Starting ForkPoolWorker-44\n",
      "Starting ForkPoolWorker-45\n",
      "Starting ForkPoolWorker-46\n",
      "Starting ForkPoolWorker-47\n",
      "Starting ForkPoolWorker-48\n",
      "Starting ForkPoolWorker-49\n",
      "Starting ForkPoolWorker-50\n",
      "Starting ForkPoolWorker-51\n",
      "Starting ForkPoolWorker-52\n",
      "Starting ForkPoolWorker-53\n",
      "Starting ForkPoolWorker-54\n",
      "Starting ForkPoolWorker-55\n",
      "Starting ForkPoolWorker-56\n",
      "Starting ForkPoolWorker-57\n",
      "Starting ForkPoolWorker-58\n",
      "Starting ForkPoolWorker-59\n",
      "Starting ForkPoolWorker-60\n",
      "Predictions file: ./baseline_files/bm25_preds_bioasq_test_b0.35k0.4.json, done!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    grid_search = 'no'\n",
    "    if grid_search == 'yes':\n",
    "        brange = np.arange(0,1,0.05)\n",
    "        krange = np.arange(0.4,2,0.05)\n",
    "    else:\n",
    "        brange = [0.35]\n",
    "        krange = [0.40]\n",
    "\n",
    "    b_k = [(round(b,3), round(k,3)) for b in brange for k in krange]\n",
    "    pool_size = 20\n",
    "    pool = multiprocessing.Pool(processes=pool_size,\n",
    "                                initializer=start_process,\n",
    "                                )\n",
    "    pool_outputs = pool.map(bm25_computing, b_k)\n",
    "    pool.close() # no more tasks\n",
    "    pool.join()  # wrap up current tasks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
