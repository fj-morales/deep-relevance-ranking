{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating query performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import gzip\n",
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "dataloc = '../../bioasq_data/'\n",
    "# dataloc = '../../robust04_data/split_5/'\n",
    "baseline_files ='./baseline_files/'\n",
    "galago_loc='./galago-3.10-bin/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select data split to work with\n",
    "split = \"test\"\n",
    "# split = \"dev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pickle_docs(pickle_filename):\n",
    "    # Pickle to Trectext converter\n",
    "    with open(pickle_filename, 'rb') as f_in:\n",
    "        data = pickle.load(f_in)\n",
    "        if not os.path.exists(baseline_files):\n",
    "            os.makedirs(baseline_files)\n",
    "        docs = {}\n",
    "        for key, value in data.items():\n",
    "            if \"pmid\" in value.keys():\n",
    "                doc_code = value.pop('pmid')\n",
    "            else:\n",
    "                doc_code = key\n",
    "            doc = '<DOC>\\n' + \\\n",
    "                  '<DOCNO>' + doc_code + '</DOCNO>\\n' + \\\n",
    "                  '<TITLE>' + value.pop('title') + '</TITLE>\\n' + \\\n",
    "                  '<TEXT>' + value.pop('abstractText') + '</TEXT>\\n' + \\\n",
    "                  '</DOC>\\n'\n",
    "            docs[doc_code] = doc\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_jsonfile(docs, filename):\n",
    "    # Pickle to Trectext converter\n",
    "    doc_list = []\n",
    "    with gzip.open(filename,'wt', encoding='utf-8') as f_out:\n",
    "        docus = {}\n",
    "        for key, value in docs.items():\n",
    "            f_out.write(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build corpus index \n",
    "def build_index(index_input, index_loc):\n",
    "    if not os.path.exists(index_loc):\n",
    "            os.makedirs(index_loc) \n",
    "    index_loc_param = '--indexPath=' + index_loc\n",
    "    galago_parameters = [galago_loc + 'galago', 'build', '--stemmer+krovetz']\n",
    "    [galago_parameters.append('--inputPath+' + idx) for idx in index_input]\n",
    "    galago_parameters.append(index_loc_param)\n",
    "    print(galago_parameters)\n",
    "\n",
    "    index_proc = subprocess.Popen(galago_parameters,\n",
    "            stdout=subprocess.PIPE, shell=False)\n",
    "    (out, err) = index_proc.communicate()\n",
    "    print(out.decode(\"utf-8\"))\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Return top 100 bm25 scored docs, given query and corpus indexed by galago\n",
    "# def get_bm25_docs(query, index_loc, b_val, k_val):\n",
    "# #     query = query.lower()\n",
    "#     query = query.rstrip('.?')\n",
    "#     index_loc_param = '--index=' + index_loc  \n",
    "#     b=' --b=' + str(b_val)\n",
    "#     k=' --k=' + str(k_val)\n",
    "#     if \"'\" in query:\n",
    "#         query_param = '--query=\"#stopword(' + query + ')\"' \n",
    "#     else:\n",
    "#         query_param = '--query=\\'#stopword(' + query + ')\\'' \n",
    "\n",
    "#     command = galago_loc + 'galago batch-search --verbose=false --casefold=true --requested=100 ' + \\\n",
    "#          index_loc_param + ' --scorer=bm25' + \\\n",
    "#          b + \\\n",
    "#          k + \\\n",
    "#          ' --stemmer+krovetz ' + \\\n",
    "#          query_param + ' | cut -d\" \" -f3'\n",
    "# #     print(command)\n",
    "#     galago_bm25_exec = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True)\n",
    "#     (out, err) = galago_bm25_exec.communicate()\n",
    "#     bm25_documents = out.decode(\"utf-8\")\n",
    "#     return bm25_documents.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return top 100 bm25 scored docs, given query and corpus indexed by galago\n",
    "def get_bm25_docs(query, index_loc, b_val=0.2, k_val=0.8):\n",
    "#     query = re.sub(r'[^\\w\\s]',' ',query)\n",
    "#     query = query.lower()\n",
    "#     query = query.rstrip('.?')\n",
    "#     query = 'List the classical triad of symptoms of the Melkersson–Rosenthal syndrome.'\n",
    "#     print(query)\n",
    "    index_loc_param = '--index=' + index_loc  \n",
    "    b=' --b=' + str(b_val)\n",
    "    k=' --k=' + str(k_val)\n",
    "    if \"'\" in query:\n",
    "        query_param = '--query=\"#stopword(' + query + ')\"' \n",
    "    else:\n",
    "        query_param = '--query=\\'#stopword(' + query + ')\\'' \n",
    "\n",
    "    command = galago_loc + 'galago batch-search --verbose=true --casefold=true --requested=100 ' + \\\n",
    "         index_loc_param + ' --scorer=bm25' + \\\n",
    "         b + \\\n",
    "         k + \\\n",
    "         ' --stemmer+krovetz ' + \\\n",
    "         query_param + ' | cut -d\" \" -f3'\n",
    "#     print(command)\n",
    "#     command = command.encode('utf-8')\n",
    "    galago_bm25_exec = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True, encoding='utf-8')\n",
    "    (out, err) = galago_bm25_exec.communicate()\n",
    "    bm25_documents = out\n",
    "    return bm25_documents.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = 'List the classical |triad of symptoms of the Melkersson–Rosenthal syndrome'\n",
    "# get_bm25_docs(query, index_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_files = [os.path.join(root, name)\n",
    "             for root, dirs, files in os.walk(dataloc)\n",
    "             for name in files\n",
    "             if all(y in name for y in ['docset', split, '.pkl'])]\n",
    "\n",
    "# pkl_files = [ x for x in os.listdir(dataloc) if all(y in x for y in ['docset', '.pkl'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../bioasq_data/bioasq_bm25_docset_top100.test.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkl_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pickle to trectext file format to be processed with galago\n",
    "# pkl_file = [s for s in pkl_files if split in s]\n",
    "# [output_file, doc_list ]= pickle_to_json(pkl_file[0])\n",
    "doc_list = []\n",
    "output_files = []\n",
    "all_docs = []\n",
    "for pkl_file in pkl_files:\n",
    "#     print(pkl_file)\n",
    "    docs = get_pickle_docs(pkl_file)\n",
    "    doc_list = doc_list + list(docs.keys())\n",
    "    all_docs.append(docs)\n",
    "    out_name = pkl_file.split('/')[-1:][0]\n",
    "    out_name = re.sub('\\.pkl', '', out_name)\n",
    "    output_file = baseline_files + out_name + '.gz'\n",
    "    output_files.append(output_file)\n",
    "    # print(out_name)\n",
    "    doc_to_jsonfile(docs, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets = [set(doc.keys()) for doc in all_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../bioasq_data/bioasq_bm25_docset_top100.test.pkl'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkl_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "bioasq\n"
     ]
    }
   ],
   "source": [
    "data_split = split\n",
    "print(data_split)\n",
    "\n",
    "if \"rob04\" in output_files[0]:\n",
    "    s = re.findall(\"(s[0-5]).pkl$\", pkl_file)\n",
    "    dataset_name = \"rob04\"\n",
    "    dataset_name_ext = dataset_name + '_'+ s[0]\n",
    "#     dataset_name_ext = dataset_name \n",
    "    gold_file = '../../robust04_data/rob04.' + split +'.json'\n",
    "#     with open(gold_file, 'w') as outfile:\n",
    "#         json.dump(query_data, outfile, indent = 4)\n",
    "    print(dataset_name_ext)\n",
    "elif \"bioasq\" in output_file:\n",
    "    print(\"bioasq\")\n",
    "    dataset_name = \"bioasq\"\n",
    "    dataset_name_ext = dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_loc = baseline_files + 'index' + '_' + dataset_name_ext + '_' + data_split\n",
    "index_input = output_files\n",
    "# build_index(index_input, index_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./baseline_files/bioasq_bm25_docset_top100.test.gz'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_filename = [ x for x in os.listdir(dataloc) if all(y in x for y in [dataset_name +'.'+ data_split, '.json'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bioasq.test.json']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries_file = dataloc + q_filename[0]\n",
    "\n",
    "def load_queries(queries_file):\n",
    "    with open(queries_file, 'rb') as input_file:\n",
    "        query_data = json.load(input_file)\n",
    "        return query_data['questions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_files = [os.path.join(root, name)\n",
    "             for root, dirs, files in os.walk(dataloc)\n",
    "             for name in files\n",
    "             if all(y in name for y in [dataset_name +'.'+ data_split, '.json'])]\n",
    "\n",
    "# pkl_files = [ x for x in os.listdir(dataloc) if all(y in x for y in ['docset', '.pkl'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = []\n",
    "query_data = {}\n",
    "for file in query_files:\n",
    "    queries = queries + load_queries(file)\n",
    "query_data['questions'] = queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_preds(file, preds):\n",
    "    with open(file, 'wt') as f_out:\n",
    "        json.dump(preds, f_out, indent=4)\n",
    "    print('Predictions file: ' + file + ', done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../bioasq_data/bioasq.test.json'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_files[0].strip('split_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./baseline_files/index_bioasq_test\n"
     ]
    }
   ],
   "source": [
    "print(index_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allows BM25 b and k grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bm25_computing(b_k):\n",
    "#     b = b_k[0]\n",
    "#     k = b_k[1]\n",
    "#     bm25_preds_file = baseline_files + 'bm25_preds_' + dataset_name_ext + '_'+ data_split + '_' + 'b' + str(b) + 'k' + str(k) + '.json'\n",
    "# #     print(bm25_preds_file)\n",
    "#     if os.path.isfile(bm25_preds_file):\n",
    "#         print(bm25_preds_file + \"Already exists!!\")\n",
    "#         return\n",
    "#     bm25_preds = {}\n",
    "#     questions = []\n",
    "#     question = {}\n",
    "#     for query in query_data['questions']:\n",
    "#         question['body'] = query['body']\n",
    "#         question['id'] = query['id']\n",
    "#     #     print(query['body'].rstrip('.'))\n",
    "#     #     documents = get_bm25_docs(query['body'].rstrip('.'), index_loc)\n",
    "#         documents = get_bm25_docs(query['body'], index_loc, b, k)\n",
    "#         if \"bioasq\" in dataset_name: \n",
    "#             documents_url = ['http://www.ncbi.nlm.nih.gov/pubmed/' + doc for doc in documents]\n",
    "#             question['documents'] = documents_url\n",
    "#         elif \"rob04\" in dataset_name:\n",
    "#             question['documents'] = documents\n",
    "#         questions.append(dict(question))\n",
    "    \n",
    "#     bm25_preds['questions'] = questions\n",
    "#     save_preds(bm25_preds_file, bm25_preds)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_process():\n",
    "    print( 'Starting', multiprocessing.current_process().name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_question(query):\n",
    "    question = {}\n",
    "    question['body'] = query['body']\n",
    "    question['id'] = query['id']\n",
    "#     print(query['body'].rstrip('.'))\n",
    "#     documents = get_bm25_docs(query['body'].rstrip('.'), index_loc)\n",
    "    documents = get_bm25_docs(query['body'], index_loc)\n",
    "    if \"bioasq\" in dataset_name: \n",
    "        documents_url = ['http://www.ncbi.nlm.nih.gov/pubmed/' + doc for doc in documents]\n",
    "        question['documents'] = documents_url\n",
    "    elif \"rob04\" in dataset_name:\n",
    "        question['documents'] = documents\n",
    "    return dict(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./baseline_files/index_bioasq_test'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_bm25_docs(query_data['questions'][0]['body'], index_loc)\n",
    "index_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./baseline_files/bm25_preds_bioasq_test_b0.2k0.8.jsonAlready exists!!\n",
      "Starting ForkPoolWorker-101\n",
      "Starting ForkPoolWorker-102\n",
      "Starting ForkPoolWorker-103\n",
      "Starting ForkPoolWorker-104\n",
      "Starting ForkPoolWorker-105\n",
      "Starting ForkPoolWorker-106\n",
      "Starting ForkPoolWorker-107\n",
      "Starting ForkPoolWorker-108\n",
      "Starting ForkPoolWorker-109\n",
      "Starting ForkPoolWorker-110\n",
      "Starting ForkPoolWorker-111\n",
      "Starting ForkPoolWorker-112\n",
      "Starting ForkPoolWorker-113\n",
      "Starting ForkPoolWorker-114\n",
      "Starting ForkPoolWorker-115\n",
      "Starting ForkPoolWorker-116\n",
      "Starting ForkPoolWorker-117\n",
      "Starting ForkPoolWorker-118\n",
      "Starting ForkPoolWorker-119\n",
      "Starting ForkPoolWorker-120\n",
      "Starting ForkPoolWorker-121\n",
      "Starting ForkPoolWorker-122\n",
      "Starting ForkPoolWorker-123\n",
      "Starting ForkPoolWorker-124\n",
      "Starting ForkPoolWorker-125\n",
      "Starting ForkPoolWorker-126\n",
      "Starting ForkPoolWorker-127\n",
      "Starting ForkPoolWorker-128\n",
      "Starting ForkPoolWorker-129\n",
      "Starting ForkPoolWorker-130\n",
      "Starting ForkPoolWorker-131\n",
      "Starting ForkPoolWorker-132\n",
      "Starting ForkPoolWorker-133\n",
      "Starting ForkPoolWorker-134\n",
      "Starting ForkPoolWorker-135\n",
      "Starting ForkPoolWorker-136\n",
      "Starting ForkPoolWorker-137\n",
      "Starting ForkPoolWorker-138\n",
      "Starting ForkPoolWorker-139\n",
      "Starting ForkPoolWorker-140\n",
      "Starting ForkPoolWorker-141\n",
      "Starting ForkPoolWorker-142\n",
      "Starting ForkPoolWorker-143\n",
      "Starting ForkPoolWorker-144\n",
      "Starting ForkPoolWorker-145\n",
      "Starting ForkPoolWorker-146\n",
      "Starting ForkPoolWorker-147\n",
      "Starting ForkPoolWorker-148\n",
      "Starting ForkPoolWorker-149\n",
      "Starting ForkPoolWorker-150\n"
     ]
    }
   ],
   "source": [
    "# def bm25_computing(b_k):\n",
    "#     b = b_k[0]\n",
    "#     k = b_k[1]\n",
    "b = 0.2\n",
    "k = 0.8\n",
    "bm25_preds_file = baseline_files + 'bm25_preds_' + dataset_name_ext + '_' + data_split + '_' + 'b' + str(b) + 'k' + str(k) + '.json'\n",
    "#     print(bm25_preds_file)\n",
    "if os.path.isfile(bm25_preds_file):\n",
    "    print(bm25_preds_file + \"Already exists!!\")\n",
    "#     return\n",
    "bm25_preds = {}\n",
    "questions = []\n",
    "pool_size = 50\n",
    "pool = multiprocessing.Pool(processes=pool_size,\n",
    "                            initializer=start_process,\n",
    "                            )\n",
    "questions = pool.map(extract_question, query_data['questions'])\n",
    "pool.close() # no more tasks\n",
    "pool.join()  # wrap up current tasks\n",
    "    \n",
    "bm25_preds['questions'] = questions\n",
    "save_preds(bm25_preds_file, bm25_preds)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     grid_search = 'no'\n",
    "#     if grid_search == 'yes':\n",
    "#         brange = np.arange(0.2,1,0.1)\n",
    "#         krange = np.arange(0.5,2,0.1)\n",
    "#     else:\n",
    "#         brange = [0.2]\n",
    "#         krange = [0.8]\n",
    "\n",
    "#     b_k = [(round(b,2), round(k,2)) for b in brange for k in krange]\n",
    "#     pool_size = 8\n",
    "#     pool = multiprocessing.Pool(processes=pool_size,\n",
    "#                                 initializer=start_process,\n",
    "#                                 )\n",
    "#     pool_outputs = pool.map(bm25_computing, b_k)\n",
    "#     pool.close() # no more tasks\n",
    "#     pool.join()  # wrap up current tasks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
