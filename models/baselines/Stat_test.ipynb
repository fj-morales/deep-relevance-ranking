{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomized statistical testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import random\n",
    "import timeit\n",
    "from scipy import stats\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_dev_algo_A = './bioasq_dir/run_bioasq_linearModel_test_filtered'\n",
    "# run_dev_algo_B = './bioasq_dir/run_bm25_bioasq_test_filtered'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qrels_file = './robust_dir/s1/robust_test_s1_qrels'\n",
    "# trec_eval_command = '../../eval/trec_eval'\n",
    "\n",
    "# run_dev_algo_A = './robust_dir/s1/run_robust_s1_best_lmart_test'\n",
    "\n",
    "# run_dev_algo_B = './robust_dir/s1/run_bm25_robust_test_s1'\n",
    "# # run_dev_algo_B = run_dev_algo_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp(x):\n",
    "    return 1 - abs(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanAP(list_X):\n",
    "    return np.mean([x for x in list_X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_A_B(paired_list):\n",
    "    map_A = meanAP([x[1] for x in paired_list])\n",
    "    map_B = meanAP([x[2] for x in paired_list])\n",
    "    return [map_A, map_B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_str(int_list):\n",
    "    string = \"\"\n",
    "    int_list = [str(x) for x in int_list] \n",
    "    return string.join(int_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_avgs(trec_eval_command, metric, qrel, qret):\n",
    "    \n",
    "    params = ['-q', '-m']\n",
    "    toolkit_parameters = [\n",
    "                            trec_eval_command,\n",
    "                            *params,\n",
    "                            metric,\n",
    "                            qrel,\n",
    "                            qret]\n",
    "\n",
    "#     print(toolkit_parameters)\n",
    "\n",
    "    proc = subprocess.Popen(toolkit_parameters, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=False)\n",
    "    (out, err) = proc.communicate()\n",
    "##     print(out.decode(\"utf-8\"))\n",
    "#     print('Run error: ', err)\n",
    "    if err == None:\n",
    "        pass\n",
    "#         print('No errors')\n",
    "    out_split = out.decode(\"utf-8\").replace('\\tall\\t','').splitlines()[:-1]\n",
    "    out_dict = {item.split()[1]:float(item.split()[2]) for item in out_split}\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paired_list(dict_A, dict_B):\n",
    "    if not (set(dict_A.keys())  == set(dict_B.keys())):\n",
    "        print('Queries sets are different!')\n",
    "        return\n",
    "    paired_list = []\n",
    "    for k in dict_A.keys():\n",
    "        paired_list.append([k, dict_A[k], dict_B[k]])\n",
    "    return paired_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_paired_list(paired_list, b_filter):    \n",
    "    return [[p[0], p[b+1], p[comp(b)+1]] for p,b in zip(paired_list, b_filter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_value(permuted_maps, observed_value):\n",
    "    permuted_diff = [x[0] - x[1] for x in permuted_maps]\n",
    "    observed_value\n",
    "    count = 0\n",
    "    for i in permuted_diff:\n",
    "        if (i < -abs(observed_value)) or (i > abs(observed_value)):\n",
    "            count += 1\n",
    "#     print(count)\n",
    "    p_value = count / len(permuted_maps)\n",
    "    return [observed_value, p_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pvalue(paired_list, alpha= 0.05, max_iter=20000, min_i=1000):\n",
    "    '''Compute randomized two-tailed significance testing'''\n",
    "    \n",
    "    seen_filters = set()\n",
    "    permuted_maps = []\n",
    "    permuted_lists = []\n",
    "    for i in range(0, max_iter):\n",
    "        b_filter = list(np.random.randint(2, size=(len(paired_list),)))\n",
    "\n",
    "        while list_to_str(b_filter) in seen_filters:\n",
    "            b_filter = list(np.random.randint(2, size=(len(paired_list),)))\n",
    "    #         clear_output()\n",
    "            print('repeated')\n",
    "\n",
    "        perm_list = permute_paired_list(paired_list, b_filter)\n",
    "        permuted_lists.append(perm_list)\n",
    "\n",
    "        seen_filters.add(list_to_str(b_filter))\n",
    "        permuted_maps.append(map_A_B(perm_list))\n",
    "\n",
    "        maps_two_algorithms = map_A_B(paired_list)\n",
    "        map_diff_test = maps_two_algorithms[0] - maps_two_algorithms[1]\n",
    "        map_diff_test\n",
    "        \n",
    "        [obs_value, pvalue] = get_p_value(permuted_maps, map_diff_test)\n",
    "\n",
    "        if i > min_i:\n",
    "            if (pvalue < 0.01) or (pvalue > 0.1):\n",
    "                break\n",
    "        if i % 1000 == 0:\n",
    "#             print(i)\n",
    "#             print(pvalue)\n",
    "            pass\n",
    "\n",
    "            \n",
    "            \n",
    "    # Compare against Student t-test 1sample\n",
    "    \n",
    "    np.random.seed(12345678)\n",
    "\n",
    "    rs_diff = [x[1] - x[2] for x in paired_list]\n",
    "\n",
    "    # rvs1 = [x[1] for x in paired_list]\n",
    "    # rvs2 = [x[2] for x in paired_list]\n",
    "    [t_statistic, t_pvalue] = stats.ttest_1samp(rs_diff,0)\n",
    "    \n",
    "    \n",
    "    if pvalue < alpha:\n",
    "        sign_flag = True\n",
    "    else:\n",
    "        sign_flag = False\n",
    "    \n",
    "    return {'Significant': str(sign_flag),\n",
    "            'rand_pvalue':pvalue,\n",
    "            't_pvalue': t_pvalue,\n",
    "            'Metric diff': obs_value,            \n",
    "            't_statistic': t_statistic\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each dataset and hpo method\n",
    "\n",
    "\n",
    "bio_test_runs = ['./deep-relevance-ranking/models/baselines/bioasq_dir/bioasq_test_qrels', # qrel\n",
    "                 './deep-relevance-ranking/models/baselines/bioasq_dir/run_bm25_bioasq_test_filtered', # bm25\n",
    "                 './deep-relevance-ranking/models/baselines/bioasq_dir/run_bioasq_linearModel_test_filtered', # bm25+extra\n",
    "                 './deep-relevance-ranking/models/baselines/posit_results4/qret.txt', # Deep model\n",
    "                 './deep-relevance-ranking/models/baselines/bioasq_dir/run_bioasq_best_lmart_test_leaves15_lr0.07_n750', # lambdaMart  HPO                \n",
    "                 './deep-relevance-ranking/models/baselines/bioasq_dir/run_bioasq_best_lmart_test_leaves10_lr0.1_n1000' # lambdaMart  defaults                \n",
    "                ]\n",
    "\n",
    "# folds = ['s1', 's2', 's3', 's4', 's5']\n",
    "robust_test_runs = ['./deep-relevance-ranking/models/baselines/robust_dir/s0/rob_test_qrels', # qrel\n",
    "                 './deep-relevance-ranking/models/baselines/robust_dir/s0/run_bm25_robust_test_s0', # bm25 \n",
    "                 './deep-relevance-ranking/models/baselines/robust_dir/s0/run_robust_linearModel_test_s0', # bm25+extra \n",
    "#                  '', # Deep model\n",
    "                 './deep-relevance-ranking/models/baselines/robust_dir/s0/run_robust_s0_best_lmart_test_leaves25_lr0.03_n450', # lambdaMart HPO                  \n",
    "                 './deep-relevance-ranking/models/baselines/robust_dir/s0/run_robust_s0_best_lmart_test_leaves10_lr0.1_n1000' # lambdaMart defaults                  \n",
    "                ]   \n",
    "#                 ] for f in folds]\n",
    "\n",
    "\n",
    "tvqa_test_runs = ['./TVQA/workdir/gold_answer_qrels_test', # qrel\n",
    "                 './TVQA/workdir/retrieved_files/run_tfidf_test', # baseline \n",
    "                 './TVQA/deep_results/run_deep_test', # Deep model, check the results when it finishes the training\n",
    "                 './TVQA/workdir/retrieved_files/run_best_lmart_test_leaves5_lr0.44_n1350', # lambdaMart                  \n",
    "                 './TVQA/workdir/retrieved_files/run_best_lmart_test_leaves10_lr0.1_n1000' # lambdaMart \n",
    "                ]\n",
    "\n",
    "# file_dirs = [bio_test_runs, *robust_test_runs, tvqa_test_runs]\n",
    "file_dirs = [bio_test_runs, robust_test_runs, tvqa_test_runs]\n",
    "# file_dirs = [bio_test_runs, robust_test_runs]\n",
    "# file_dirs = [robust_test_runs]\n",
    "# file_dirs = [bio_test_runs,  tvqa_test_runs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trec_eval_command = './trec_eval/trec_eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model A: run_bm25_bioasq_test_filtered : MAP :  0.4598\n",
      "Model B: run_bioasq_linearModel_test_filtered : MAP :  0.4641\n",
      "{'Significant': 'False', 'rand_pvalue': 0.5209580838323353, 't_pvalue': 0.5168462929992957, 'Metric diff': -0.004243249999999976, 't_statistic': -0.6487916502629775}\n",
      "Model A: run_bm25_bioasq_test_filtered : P.20 :  0.2558\n",
      "Model B: run_bioasq_linearModel_test_filtered : P.20 :  0.2609\n",
      "{'Significant': 'False', 'rand_pvalue': 0.15568862275449102, 't_pvalue': 0.1684344966907121, 'Metric diff': -0.005124999999999935, 't_statistic': -1.379755939450135}\n",
      "Model A: run_bm25_bioasq_test_filtered : NDCG_CUT.20 :  0.5518\n",
      "Model B: run_bioasq_linearModel_test_filtered : NDCG_CUT.20 :  0.5515\n",
      "{'Significant': 'False', 'rand_pvalue': 0.9730538922155688, 't_pvalue': 0.9699271457834753, 'Metric diff': 0.0002662499999999124, 't_statistic': 0.03772332398104928}\n",
      "Model A: run_bm25_bioasq_test_filtered : MAP :  0.4598\n",
      "Model B: qret.txt : MAP :  0.4763\n",
      "{'Significant': 'True', 'rand_pvalue': 0.001996007984031936, 't_pvalue': 0.0033938463770196046, 'Metric diff': -0.016519249999999985, 't_statistic': -2.9473005326272705}\n"
     ]
    }
   ],
   "source": [
    "max_iter = 20000\n",
    "\n",
    "min_i =1000\n",
    "alpha = 0.05\n",
    "\n",
    "# initial_b_filter = [1] * 400\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "dict_A = {}\n",
    "dict_B = {}\n",
    "for fdir in file_dirs:\n",
    "    \n",
    "    qrels_file = fdir[0]    \n",
    "    if 'TVQA' in qrels_file:\n",
    "        metrics = ['success.1']\n",
    "    else:\n",
    "        metrics = ['map', 'P.20', 'ndcg_cut.20']\n",
    "#         metrics = ['map','ndcg_cut.20']\n",
    "#         metrics = ['ndcg_cut.20']\n",
    "    comb_folder = list(itertools.combinations(fdir[1:],2))\n",
    "    \n",
    "#     print(comb_folder)\n",
    "    \n",
    "    for comb in comb_folder:\n",
    "#         print(comb)\n",
    "#         continue\n",
    "        model_A = comb[0]\n",
    "        model_B = comb[1]\n",
    "        \n",
    "        for metric in metrics:\n",
    "            dict_A = get_run_avgs(trec_eval_command, metric, qrels_file, model_A)\n",
    "            dict_B = get_run_avgs(trec_eval_command, metric, qrels_file, model_B)\n",
    "\n",
    "\n",
    "            paired_list = get_paired_list(dict_A, dict_B)\n",
    "\n",
    "            metric_A = meanAP(list(dict_A.values()))\n",
    "            metric_B = meanAP(list(dict_B.values()))\n",
    "\n",
    "            results = compute_pvalue(paired_list, alpha, max_iter, min_i=1000)\n",
    "\n",
    "            print('Model A: ' + model_A.split('/')[-2:][-1].replace('_s0', '_summed'), ':', metric.upper(), ': ' , round(metric_A,4))\n",
    "\n",
    "            print('Model B: ' + model_B.split('/')[-2:][-1].replace('_s0', '_summed'), ':', metric.upper(), ': ', round(metric_B,4))\n",
    "            print(results)\n",
    "    #         break\n",
    "    #         print('Time spent: ', timeit.default_timer() - start_time, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
