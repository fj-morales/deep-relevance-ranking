{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomized statistical testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import random\n",
    "import timeit\n",
    "from scipy import stats\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_dev_algo_A = './bioasq_dir/run_bioasq_linearModel_test_filtered'\n",
    "# run_dev_algo_B = './bioasq_dir/run_bm25_bioasq_test_filtered'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qrels_file = './robust_dir/s1/robust_test_s1_qrels'\n",
    "# trec_eval_command = '../../eval/trec_eval'\n",
    "\n",
    "# run_dev_algo_A = './robust_dir/s1/run_robust_s1_best_lmart_test'\n",
    "\n",
    "# run_dev_algo_B = './robust_dir/s1/run_bm25_robust_test_s1'\n",
    "# # run_dev_algo_B = run_dev_algo_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp(x):\n",
    "    return 1 - abs(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanAP(list_X):\n",
    "    return np.mean([x for x in  list_X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_A_B(paired_list):\n",
    "    map_A = meanAP([x[1] for x in paired_list])\n",
    "    map_B = meanAP([x[2] for x in paired_list])\n",
    "    return [map_A, map_B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_str(int_list):\n",
    "    string = \"\"\n",
    "    int_list = [str(x) for x in int_list] \n",
    "    return string.join(int_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_avgs(trec_eval_command, metric, qrel, qret):\n",
    "    \n",
    "    params = ['-q', '-m']\n",
    "    toolkit_parameters = [\n",
    "                            trec_eval_command,\n",
    "                            *params,\n",
    "                            metric,\n",
    "                            qrel,\n",
    "                            qret]\n",
    "\n",
    "#     print(toolkit_parameters)\n",
    "\n",
    "    proc = subprocess.Popen(toolkit_parameters, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=False)\n",
    "    (out, err) = proc.communicate()\n",
    "##     print(out.decode(\"utf-8\"))\n",
    "#     print('Run error: ', err)\n",
    "    if err == None:\n",
    "        pass\n",
    "#         print('No errors')\n",
    "    out_split = out.decode(\"utf-8\").replace('\\tall\\t','').splitlines()[:-1]\n",
    "    out_dict = {item.split()[1]:float(item.split()[2]) for item in out_split}\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paired_list(dict_A, dict_B):\n",
    "    if not (set(dict_A.keys())  == set(dict_B.keys())):\n",
    "        print('Queries sets are different!')\n",
    "        return\n",
    "    paired_list = []\n",
    "    for k in dict_A.keys():\n",
    "        paired_list.append([k, dict_A[k], dict_B[k]])\n",
    "    return paired_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_paired_list(paired_list, b_filter):    \n",
    "    return [[p[0], p[b+1], p[comp(b)+1]] for p,b in zip(paired_list, b_filter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_value(permuted_maps, observed_value):\n",
    "    permuted_diff = [x[0] - x[1] for x in permuted_maps]\n",
    "    observed_value\n",
    "    count = 0\n",
    "    for i in permuted_diff:\n",
    "        if (i < -abs(observed_value)) or (i > abs(observed_value)):\n",
    "            count += 1\n",
    "#     print(count)\n",
    "    p_value = count / len(permuted_maps)\n",
    "    return [observed_value, p_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pvalue(paired_list, alpha= 0.05, max_iter=20000, min_i=1000):\n",
    "    '''Compute randomized two-tailed significance testing'''\n",
    "    \n",
    "    seen_filters = set()\n",
    "    permuted_maps = []\n",
    "    permuted_lists = []\n",
    "    for i in range(0, max_iter):\n",
    "        b_filter = list(np.random.randint(2, size=(len(paired_list),)))\n",
    "\n",
    "        while list_to_str(b_filter) in seen_filters:\n",
    "            b_filter = list(np.random.randint(2, size=(len(paired_list),)))\n",
    "    #         clear_output()\n",
    "            print('repeated')\n",
    "\n",
    "        perm_list = permute_paired_list(paired_list, b_filter)\n",
    "        permuted_lists.append(perm_list)\n",
    "\n",
    "        seen_filters.add(list_to_str(b_filter))\n",
    "        permuted_maps.append(map_A_B(perm_list))\n",
    "\n",
    "        maps_two_algorithms = map_A_B(paired_list)\n",
    "        map_diff_test = maps_two_algorithms[0] - maps_two_algorithms[1]\n",
    "        map_diff_test\n",
    "        \n",
    "        [obs_value, pvalue] = get_p_value(permuted_maps, map_diff_test)\n",
    "\n",
    "        if i > min_i:\n",
    "            if (pvalue < 0.01) or (pvalue > 0.1):\n",
    "                break\n",
    "        if i % 1000 == 0:\n",
    "            pass\n",
    "#             print(pvalue)\n",
    "            \n",
    "    # Compare against Student t-test 1sample\n",
    "    \n",
    "    np.random.seed(12345678)\n",
    "\n",
    "    rs_diff = [x[1] - x[2] for x in paired_list]\n",
    "\n",
    "    # rvs1 = [x[1] for x in paired_list]\n",
    "    # rvs2 = [x[2] for x in paired_list]\n",
    "    [t_statistic, t_pvalue] = stats.ttest_1samp(rs_diff,0)\n",
    "    \n",
    "    \n",
    "    if pvalue < alpha:\n",
    "        sign_flag = True\n",
    "    else:\n",
    "        sign_flag = False\n",
    "    \n",
    "    return {'Significant': str(sign_flag),\n",
    "            'rand_pvalue':pvalue,\n",
    "            't_pvalue': t_pvalue,\n",
    "            'Metric diff': obs_value,            \n",
    "            't_statistic': t_statistic\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each dataset and hpo method\n",
    "\n",
    "\n",
    "bio_test_runs = ['./deep-relevance-ranking/models/baselines/bioasq_dir/bioasq_test_qrels', # qrel\n",
    "                 './deep-relevance-ranking/models/baselines/bioasq_dir/run_bm25_bioasq_test_filtered', # bm25\n",
    "                 './deep-relevance-ranking/models/baselines/bioasq_dir/run_bioasq_linearModel_test_filtered', # bm25+extra\n",
    "                 './deep-relevance-ranking/models/baselines/posit_results4/qret.txt', # Deep model\n",
    "                 './deep-relevance-ranking/models/baselines/bioasq_dir/run_bioasq_best_lmart_test_leaves15_lr0.07_n750' # lambdaMart                  \n",
    "                ]\n",
    "\n",
    "folds = ['s1', 's2', 's3', 's4', 's5']\n",
    "robust_test_runs = [['./deep-relevance-ranking/models/baselines/robust_dir/' + f + '/robust_test_' + f + '_qrels', # qrel\n",
    "                 './deep-relevance-ranking/models/baselines/robust_dir/' + f + '/run_bm25_robust_test_' + f, # bm25 \n",
    "                 './deep-relevance-ranking/models/baselines/robust_dir/' + f + '/run_robust_linearModel_test_' + f, # bm25+extra \n",
    "#                  '', # Deep model\n",
    "                 './deep-relevance-ranking/models/baselines/robust_dir/' + f + '/run_robust_' + f + '_best_lmart_test_leaves25_lr0.03_n450' # lambdaMart                  \n",
    "                ] for f in folds]\n",
    "\n",
    "\n",
    "tvqa_test_runs = ['./TVQA/workdir/gold_answer_qrels_test', # qrel\n",
    "                 './TVQA/workdir/retrieved_files/run_tfidf_test', # baseline \n",
    "                 './TVQA/deep_results/run_deep_test', # Deep model, check the results when it finishes the training\n",
    "                 './TVQA/workdir/retrieved_files/run_best_lmart_test_leaves5_lr0.44_n1350' # lambdaMart                  \n",
    "                ]\n",
    "\n",
    "file_dirs = [bio_test_runs, *robust_test_runs, tvqa_test_runs]\n",
    "# file_dirs = [bio_test_runs,  tvqa_test_runs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trec_eval_command = './trec_eval/trec_eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model A: run_bm25_bioasq_test_filtered |  MAP :  0.4598\n",
      "Model B: run_bioasq_linearModel_test_filtered |  MAP :  0.4641\n",
      "{'Significant': 'False', 'rand_pvalue': 0.5449101796407185, 't_pvalue': 0.5168462929992957, 'Metric diff': -0.004243249999999976, 't_statistic': -0.6487916502629775}\n",
      "Model A: run_bm25_bioasq_test_filtered |  MAP :  0.4598\n",
      "Model B: qret.txt |  MAP :  0.4763\n",
      "{'Significant': 'True', 'rand_pvalue': 0.001996007984031936, 't_pvalue': 0.0033938463770196046, 'Metric diff': -0.016519249999999985, 't_statistic': -2.9473005326272705}\n",
      "Model A: run_bm25_bioasq_test_filtered |  MAP :  0.4598\n",
      "Model B: run_bioasq_best_lmart_test_leaves15_lr0.07_n750 |  MAP :  0.4692\n",
      "{'Significant': 'False', 'rand_pvalue': 0.1536926147704591, 't_pvalue': 0.14501867696864665, 'Metric diff': -0.009346999999999939, 't_statistic': -1.460212231257566}\n",
      "Model A: run_bioasq_linearModel_test_filtered |  MAP :  0.4641\n",
      "Model B: qret.txt |  MAP :  0.4763\n",
      "{'Significant': 'False', 'rand_pvalue': 0.10778443113772455, 't_pvalue': 0.10615475451902721, 'Metric diff': -0.012276000000000009, 't_statistic': -1.6193840247824494}\n",
      "Model A: run_bioasq_linearModel_test_filtered |  MAP :  0.4641\n",
      "Model B: run_bioasq_best_lmart_test_leaves15_lr0.07_n750 |  MAP :  0.4692\n",
      "{'Significant': 'False', 'rand_pvalue': 0.4590818363273453, 't_pvalue': 0.4362241151780082, 'Metric diff': -0.0051037499999999625, 't_statistic': -0.7793694735909604}\n",
      "Model A: qret.txt |  MAP :  0.4763\n",
      "Model B: run_bioasq_best_lmart_test_leaves15_lr0.07_n750 |  MAP :  0.4692\n",
      "{'Significant': 'False', 'rand_pvalue': 0.36726546906187624, 't_pvalue': 0.35548162488740587, 'Metric diff': 0.007172250000000047, 't_statistic': 0.9250834968395053}\n",
      "HEHE:  249 249\n",
      "Model A: run_bm25_robust_test_summed |  MAP :  0.229\n",
      "Model B: run_robust_linearModel_test_summed |  MAP :  0.2328\n",
      "{'Significant': 'False', 'rand_pvalue': 0.37924151696606784, 't_pvalue': 0.38755045787151043, 'Metric diff': -0.0038698795180722723, 't_statistic': -0.8655921040364934}\n",
      "HEHE:  249 249\n",
      "Model A: run_bm25_robust_test_summed |  MAP :  0.229\n",
      "Model B: run_robust_summed_best_lmart_test_leaves25_lr0.03_n450 |  MAP :  0.2395\n",
      "{'Significant': 'True', 'rand_pvalue': 0.01015, 't_pvalue': 0.010711176780499489, 'Metric diff': -0.010556224899598371, 't_statistic': -2.571448361373562}\n",
      "HEHE:  249 249\n",
      "Model A: run_robust_linearModel_test_summed |  MAP :  0.2236\n",
      "Model B: run_robust_summed_best_lmart_test_leaves25_lr0.03_n450 |  MAP :  0.2395\n",
      "{'Significant': 'True', 'rand_pvalue': 0.0, 't_pvalue': 0.0002369101549058798, 'Metric diff': -0.01587510040160639, 't_statistic': -3.7305023720707418}\n",
      "Model A: run_tfidf_test |  SUCCESS.1 :  0.5242\n",
      "Model B: run_deep_test |  SUCCESS.1 :  0.6586\n",
      "{'Significant': 'True', 'rand_pvalue': 0.0, 't_pvalue': 2.0771385903898689e-218, 'Metric diff': -0.13439979020520554, 't_statistic': -32.06748590367127}\n",
      "Model A: run_tfidf_test |  SUCCESS.1 :  0.5242\n",
      "Model B: run_best_lmart_test_leavesummed_lr0.44_n1350 |  SUCCESS.1 :  0.563\n",
      "{'Significant': 'True', 'rand_pvalue': 0.0, 't_pvalue': 1.9414127671468704e-36, 'Metric diff': -0.0387464761030617, 't_statistic': -12.639740251532006}\n",
      "Model A: run_deep_test |  SUCCESS.1 :  0.6586\n",
      "Model B: run_best_lmart_test_leavesummed_lr0.44_n1350 |  SUCCESS.1 :  0.563\n",
      "{'Significant': 'True', 'rand_pvalue': 0.0, 't_pvalue': 2.9691346019264735e-124, 'Metric diff': 0.09565331410214384, 't_statistic': 23.92869230541027}\n"
     ]
    }
   ],
   "source": [
    "max_iter = 20000\n",
    "\n",
    "min_i =1000\n",
    "alpha = 0.05\n",
    "\n",
    "# initial_b_filter = [1] * 400\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "dict_A = {}\n",
    "dict_B = {}\n",
    "for fdir in file_dirs:\n",
    "    \n",
    "    qrels_file = fdir[0]    \n",
    "    if 'TVQA' in qrels_file:\n",
    "        metric = 'success.1'\n",
    "    else:\n",
    "        metric = 'map'\n",
    "    comb_folder = list(itertools.combinations(fdir[1:],2))\n",
    "    \n",
    "    if ('robust' in qrels_file) and (any(x in qrels_file for x in ['s2', 's3', 's4', 's5'])):        \n",
    "        pass\n",
    "    else:\n",
    "        dict_A = {}\n",
    "        dict_B = {}\n",
    "        \n",
    "    for comb in comb_folder:\n",
    "        model_A = comb[0]\n",
    "        model_B = comb[1]\n",
    "        \n",
    "        \n",
    "        dict_part_A = get_run_avgs(trec_eval_command, metric, qrels_file, model_A)\n",
    "        dict_part_B = get_run_avgs(trec_eval_command, metric, qrels_file, model_B)\n",
    "                            \n",
    "        if ('robust' in qrels_file) and not ('s5' in qrels_file):\n",
    "            dict_A.update(dict_part_A)\n",
    "            dict_B.update(dict_part_B)\n",
    "            \n",
    "            continue\n",
    "        elif ('robust' in qrels_file) and  ('s5' in qrels_file):\n",
    "            dict_A.update(dict_part_A)\n",
    "            dict_B.update(dict_part_B)\n",
    "            print(\"HEHE: \", len(dict_A), len(dict_B))\n",
    "\n",
    "        \n",
    "        dict_A.update(dict_part_A)\n",
    "        dict_B.update(dict_part_B)\n",
    "        \n",
    "#         print(\"HEHE2: \", len(dict_A))\n",
    "#         print(\"HEHE2: \", len(dict_B))\n",
    "        \n",
    "        paired_list = get_paired_list(dict_A, dict_B)\n",
    "        \n",
    "        metric_A = meanAP(list(dict_A.values()))\n",
    "        metric_B = meanAP(list(dict_B.values()))\n",
    "        \n",
    "        results = compute_pvalue(paired_list, alpha, max_iter, min_i=1000)\n",
    "        \n",
    "        print('Model A: ' + model_A.split('/')[-2:][-1].replace('s5', 'summed'), '| ', metric.upper(), ': ' , round(metric_A,4))\n",
    "        \n",
    "        print('Model B: ' + model_B.split('/')[-2:][-1].replace('s5', 'summed'), '| ', metric.upper(), ': ', round(metric_B,4))\n",
    "        print(results)\n",
    "#         print('Time spent: ', timeit.default_timer() - start_time, '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
