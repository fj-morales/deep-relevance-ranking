{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomized statistical testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import random\n",
    "import timeit\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_dev_algo_A = './bioasq_dir/run_bioasq_linearModel_test_filtered'\n",
    "# run_dev_algo_B = './bioasq_dir/run_bm25_bioasq_test_filtered'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qrels_file = './robust_dir/s1/robust_test_s1_qrels'\n",
    "# trec_eval_command = '../../eval/trec_eval'\n",
    "\n",
    "# run_dev_algo_A = './robust_dir/s1/run_robust_s1_best_lmart_test'\n",
    "\n",
    "# run_dev_algo_B = './robust_dir/s1/run_bm25_robust_test_s1'\n",
    "# # run_dev_algo_B = run_dev_algo_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp(x):\n",
    "    return 1 - abs(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanAP(list_X):\n",
    "    return np.mean([x for x in  list_X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_A_B(paired_list):\n",
    "    map_A = meanAP([x[1] for x in paired_list])\n",
    "    map_B = meanAP([x[2] for x in paired_list])\n",
    "    return [map_A, map_B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_str(int_list):\n",
    "    string = \"\"\n",
    "    int_list = [str(x) for x in int_list] \n",
    "    return string.join(int_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_avgs(trec_eval_command, metric, qrel, qret):\n",
    "    \n",
    "    params = ['-q', '-m']\n",
    "    toolkit_parameters = [\n",
    "                            trec_eval_command,\n",
    "                            *params,\n",
    "                            metric,\n",
    "                            qrel,\n",
    "                            qret]\n",
    "\n",
    "#     print(toolkit_parameters)\n",
    "\n",
    "    proc = subprocess.Popen(toolkit_parameters, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=False)\n",
    "    (out, err) = proc.communicate()\n",
    "##     print(out.decode(\"utf-8\"))\n",
    "#     print('Run error: ', err)\n",
    "    if err == None:\n",
    "        pass\n",
    "#         print('No errors')\n",
    "    out_split = out.decode(\"utf-8\").replace('\\tall\\t','').splitlines()[:-1]\n",
    "    out_dict = {item.split()[1]:float(item.split()[2]) for item in out_split}\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paired_list(dict_A, dict_B):\n",
    "    if not (set(dict_A.keys())  == set(dict_B.keys())):\n",
    "        print('Queries sets are different!')\n",
    "        return\n",
    "    paired_list = []\n",
    "    for k in dict_A.keys():\n",
    "        paired_list.append([k, dict_A[k], dict_B[k]])\n",
    "    return paired_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_paired_list(paired_list, b_filter):    \n",
    "    return [[p[0], p[b+1], p[comp(b)+1]] for p,b in zip(paired_list, b_filter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_value(permuted_maps, observed_value):\n",
    "    permuted_diff = [x[0] - x[1] for x in permuted_maps]\n",
    "    observed_value\n",
    "    count = 0\n",
    "    for i in permuted_diff:\n",
    "        if (i < -abs(observed_value)) or (i > abs(observed_value)):\n",
    "            count += 1\n",
    "#     print(count)\n",
    "    p_value = count / len(permuted_maps)\n",
    "    return [observed_value, p_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pvalue(paired_list, max_iter=20000, min_i=1000):\n",
    "    '''Compute randomized two-tailed significance testing'''\n",
    "    \n",
    "    seen_filters = set()\n",
    "    permuted_maps = []\n",
    "    permuted_lists = []\n",
    "    for i in range(0, max_iter):\n",
    "        b_filter = list(np.random.randint(2, size=(len(paired_list),)))\n",
    "\n",
    "        while list_to_str(b_filter) in seen_filters:\n",
    "            b_filter = list(np.random.randint(2, size=(len(paired_list),)))\n",
    "    #         clear_output()\n",
    "            print('repeated')\n",
    "\n",
    "        perm_list = permute_paired_list(paired_list, b_filter)\n",
    "        permuted_lists.append(perm_list)\n",
    "\n",
    "        seen_filters.add(list_to_str(b_filter))\n",
    "        permuted_maps.append(map_A_B(perm_list))\n",
    "\n",
    "        maps_two_algorithms = map_A_B(paired_list)\n",
    "        map_diff_test = maps_two_algorithms[0] - maps_two_algorithms[1]\n",
    "        map_diff_test\n",
    "        \n",
    "        [obs_value, pvalue] = get_p_value(permuted_maps, map_diff_test)\n",
    "\n",
    "        if i > min_i:\n",
    "            if (pvalue < 0.01) or (pvalue > 0.1):\n",
    "                break\n",
    "        if i % 1000 == 0:\n",
    "            pass\n",
    "#             print(pvalue)\n",
    "            \n",
    "    # Compare against Student t-test 1sample\n",
    "    \n",
    "    np.random.seed(12345678)\n",
    "\n",
    "    rs_diff = [x[1] - x[2] for x in paired_list]\n",
    "\n",
    "    # rvs1 = [x[1] for x in paired_list]\n",
    "    # rvs2 = [x[2] for x in paired_list]\n",
    "    [t_statistic, t_pvalue] = stats.ttest_1samp(rs_diff,0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return {'rand_pvalue':pvalue,\n",
    "            'obs_value': obs_value,\n",
    "            't_statistic': t_statistic, \n",
    "            't_pvalue': t_pvalue\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each dataset and hpo method\n",
    "\n",
    "\n",
    "bio_test_runs = ['./deep-relevance-ranking/models/baselines/bioasq_dir/bioasq_test_qrels', # qrel\n",
    "                 './deep-relevance-ranking/models/baselines/bioasq_dir/run_bm25_bioasq_test_filtered', # baseline \n",
    "                 './deep-relevance-ranking/models/baselines/posit_results4/qret.txt', # Deep model\n",
    "                 './deep-relevance-ranking/models/baselines/bioasq_dir/run_bioasq_best_lmart_test_leaves15_lr0.07_n750' # lambdaMart                  \n",
    "                ]\n",
    "\n",
    "folds = ['s1', 's2', 's3', 's4', 's5']\n",
    "robust_test_runs = [['./deep-relevance-ranking/models/baselines/robust_dir/' + f + '/robust_test_' + f + '_qrels', # qrel\n",
    "                 './deep-relevance-ranking/models/baselines/robust_dir/' + f + '/run_bm25_robust_test_' + f, # baseline \n",
    "#                  '', # Deep model\n",
    "                 './deep-relevance-ranking/models/baselines/robust_dir/' + f + '/run_robust_' + f + '_best_lmart_test_leaves25_lr0.03_n400' # lambdaMart                  \n",
    "                ] for f in folds]\n",
    "\n",
    "\n",
    "tvqa_test_runs = ['./TVQA/workdir/gold_answer_qrels_test', # qrel\n",
    "                 './TVQA/workdir/retrieved_files/run_tfidf_test', # baseline \n",
    "                 './TVQA/workdir/deep_model/run_deep_test', # Deep model, check the results when it finishes the training\n",
    "                 './TVQA/workdir/retrieved_files/run_best_lmart_test_leaves5_lr0.44_n1350' # lambdaMart                  \n",
    "                ]\n",
    "\n",
    "file_dirs = [bio_test_runs, *robust_test_runs, tvqa_test_runs]\n",
    "# file_dirs = [bio_test_runs,  tvqa_test_runs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trec_eval_command = './trec_eval/trec_eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bioasq_dir', 'run_bm25_bioasq_test_filtered'] \n",
      " MAP :  0.45982575000000003\n",
      "['posit_results4', 'qret.txt'] \n",
      " MAP :  0.476345\n",
      "{'rand_pvalue': 0.001996007984031936, 'obs_value': -0.016519249999999985, 't_statistic': -2.9473005326272705, 't_pvalue': 0.0033938463770196046}\n",
      "Time spent:  1.5606641955673695 \n",
      "\n",
      "['bioasq_dir', 'run_bm25_bioasq_test_filtered'] \n",
      " MAP :  0.45982575000000003\n",
      "['bioasq_dir', 'run_bioasq_best_lmart_test_leaves15_lr0.07_n750'] \n",
      " MAP :  0.46917274999999997\n",
      "{'rand_pvalue': 0.1536926147704591, 'obs_value': -0.009346999999999939, 't_statistic': -1.460212231257566, 't_pvalue': 0.14501867696864665}\n",
      "Time spent:  3.2167252264916897 \n",
      "\n",
      "['s1', 'run_bm25_robust_test_s1'] \n",
      " MAP :  0.24165199999999998\n",
      "['s1', 'run_robust_s1_best_lmart_test_leaves25_lr0.03_n400'] \n",
      " MAP :  0.24643199999999996\n",
      "{'rand_pvalue': 0.4810379241516966, 'obs_value': -0.004779999999999979, 't_statistic': -0.7147571828804317, 't_pvalue': 0.47815073250613915}\n",
      "Time spent:  3.7642423920333385 \n",
      "\n",
      "['s2', 'run_bm25_robust_test_s2'] \n",
      " MAP :  0.21135714285714285\n",
      "['s2', 'run_robust_s2_best_lmart_test_leaves25_lr0.03_n400'] \n",
      " MAP :  0.20513469387755104\n",
      "{'rand_pvalue': 0.46506986027944114, 'obs_value': 0.006222448979591816, 't_statistic': 0.7148614567586555, 't_pvalue': 0.47815724717321606}\n",
      "Time spent:  4.295448878780007 \n",
      "\n",
      "['s3', 'run_bm25_robust_test_s3'] \n",
      " MAP :  0.21302200000000002\n",
      "['s3', 'run_robust_s3_best_lmart_test_leaves25_lr0.03_n400'] \n",
      " MAP :  0.201568\n",
      "{'rand_pvalue': 0.28043912175648705, 'obs_value': 0.01145400000000002, 't_statistic': 1.1508406377397753, 't_pvalue': 0.2553829658207985}\n",
      "Time spent:  4.86441539786756 \n",
      "\n",
      "['s4', 'run_bm25_robust_test_s4'] \n",
      " MAP :  0.252082\n",
      "['s4', 'run_robust_s4_best_lmart_test_leaves25_lr0.03_n400'] \n",
      " MAP :  0.281726\n",
      "{'rand_pvalue': 0.0, 'obs_value': -0.029644000000000004, 't_statistic': -3.6956831433471815, 't_pvalue': 0.0005529787504748122}\n",
      "Time spent:  5.481744144111872 \n",
      "\n",
      "['s5', 'run_bm25_robust_test_s5'] \n",
      " MAP :  0.25520400000000004\n",
      "['s5', 'run_robust_s5_best_lmart_test_leaves25_lr0.03_n400'] \n",
      " MAP :  0.26201399999999997\n",
      "{'rand_pvalue': 0.4151696606786427, 'obs_value': -0.006809999999999927, 't_statistic': -0.8166384777746287, 't_pvalue': 0.4180877595787076}\n",
      "Time spent:  6.07229326851666 \n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-e5ecdaf89e04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodel_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfdir\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mdict_baseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_run_avgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrec_eval_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqrels_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mdict_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_run_avgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrec_eval_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqrels_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mpaired_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_paired_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_baseline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-1a7f613cf65b>\u001b[0m in \u001b[0;36mget_run_avgs\u001b[0;34m(trec_eval_command, metric, qrel, qret)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#         print('No errors')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mout_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\tall\\t'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mout_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout_split\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-1a7f613cf65b>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#         print('No errors')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mout_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\tall\\t'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mout_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout_split\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'read'"
     ]
    }
   ],
   "source": [
    "max_iter = 20000\n",
    "\n",
    "min_i =1000\n",
    "\n",
    "# initial_b_filter = [1] * 400\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "for fdir in file_dirs:\n",
    "    qrels_file = fdir[0]\n",
    "    baseline_file = fdir[1]\n",
    "    \n",
    "    if 'TVQA' in fdir[0]:\n",
    "        metric = 'success.1'\n",
    "    else:\n",
    "        metric = 'map'\n",
    "    \n",
    "    for model_file in fdir[2:]: \n",
    "        dict_baseline = get_run_avgs(trec_eval_command, metric, qrels_file, baseline_file)\n",
    "        dict_model = get_run_avgs(trec_eval_command, metric, qrels_file, model_file)\n",
    "        paired_list = get_paired_list(dict_baseline, dict_model)\n",
    "        \n",
    "        map_bas = meanAP(list(dict_baseline.values()))\n",
    "        map_model = meanAP(list(dict_model.values()))\n",
    "        \n",
    "        results = compute_pvalue(paired_list, max_iter, min_i=1000)\n",
    "        \n",
    "        print(baseline_file.split('/')[-2:], '\\n', metric.upper(), ': ' , map_bas)\n",
    "        \n",
    "        print(model_file.split('/')[-2:], '\\n', metric.upper(), ': ', map_model)\n",
    "        print(results)\n",
    "        print('Time spent: ', timeit.default_timer() - start_time, '\\n')\n",
    "    # Plot\n",
    "\n",
    "    # Save results all results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
