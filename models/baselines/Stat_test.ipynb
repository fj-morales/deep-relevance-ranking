{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomized statistical testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import random\n",
    "import timeit\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_dev_algo_A = './bioasq_dir/run_bioasq_linearModel_test_filtered'\n",
    "# run_dev_algo_B = './bioasq_dir/run_bm25_bioasq_test_filtered'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qrels_file = './robust_dir/s1/robust_test_s1_qrels'\n",
    "# trec_eval_command = '../../eval/trec_eval'\n",
    "\n",
    "# run_dev_algo_A = './robust_dir/s1/run_robust_s1_best_lmart_test'\n",
    "\n",
    "# run_dev_algo_B = './robust_dir/s1/run_bm25_robust_test_s1'\n",
    "# # run_dev_algo_B = run_dev_algo_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp(x):\n",
    "    return 1 - abs(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanAP(list_X):\n",
    "    return np.mean([x for x in  list_X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_A_B(paired_list):\n",
    "    map_A = meanAP([x[1] for x in paired_list])\n",
    "    map_B = meanAP([x[2] for x in paired_list])\n",
    "    return [map_A, map_B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_str(int_list):\n",
    "    string = \"\"\n",
    "    int_list = [str(x) for x in int_list] \n",
    "    return string.join(int_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_avgs(trec_eval_command, metric, qrel, qret):\n",
    "    \n",
    "    params = ['-q', '-m']\n",
    "    toolkit_parameters = [\n",
    "                            trec_eval_command,\n",
    "                            *params,\n",
    "                            metric,\n",
    "                            qrel,\n",
    "                            qret]\n",
    "\n",
    "    print(toolkit_parameters)\n",
    "\n",
    "    proc = subprocess.Popen(toolkit_parameters, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=False)\n",
    "    (out, err) = proc.communicate()\n",
    "##     print(out.decode(\"utf-8\"))\n",
    "#     print('Run error: ', err)\n",
    "    if err == None:\n",
    "        pass\n",
    "#         print('No errors')\n",
    "    out_split = out.decode(\"utf-8\").replace('\\tall\\t','').splitlines()[:-1]\n",
    "    out_dict = {item.split()[1]:float(item.split()[2]) for item in out_split}\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paired_list(dict_A, dict_B):\n",
    "    if not (set(dict_A.keys())  == set(dict_B.keys())):\n",
    "        print('Queries sets are different!')\n",
    "        return\n",
    "    paired_list = []\n",
    "    for k in dict_A.keys():\n",
    "        paired_list.append([k, dict_A[k], dict_B[k]])\n",
    "    return paired_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_paired_list(paired_list, b_filter):    \n",
    "    return [[p[0], p[b+1], p[comp(b)+1]] for p,b in zip(paired_list, b_filter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_value(permuted_maps, observed_value):\n",
    "    permuted_diff = [x[0] - x[1] for x in permuted_maps]\n",
    "    observed_value\n",
    "    count = 0\n",
    "    for i in permuted_diff:\n",
    "        if (i < -abs(observed_value)) or (i > abs(observed_value)):\n",
    "            count += 1\n",
    "#     print(count)\n",
    "    p_value = count / len(permuted_maps)\n",
    "    return [observed_value, p_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_test_pred_files(workdir):\n",
    "    \n",
    "#     contain = ['run','test']\n",
    "#     not_contain = ['pre_','feat']\n",
    "#     test_pred_files = []\n",
    "#     for root, dirs, files in os.walk(workdir, topdown=False):\n",
    "#         for name in files:\n",
    "#             if all(x in name for x in contain) and not any(x in name for x in not_contain):\n",
    "#                 test_pred_files.append(os.path.join(root, name))\n",
    "#     return test_pred_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pvalue(paired_list, max_iter=20000, min_i=1000):\n",
    "    '''Compute randomized two-tailed significance testing'''\n",
    "    \n",
    "    seen_filters = set()\n",
    "    permuted_maps = []\n",
    "    permuted_lists = []\n",
    "    for i in range(0, max_iter):\n",
    "        b_filter = list(np.random.randint(2, size=(len(paired_list),)))\n",
    "\n",
    "        while list_to_str(b_filter) in seen_filters:\n",
    "            b_filter = list(np.random.randint(2, size=(len(paired_list),)))\n",
    "    #         clear_output()\n",
    "            print('repeated')\n",
    "\n",
    "        perm_list = permute_paired_list(paired_list, b_filter)\n",
    "        permuted_lists.append(perm_list)\n",
    "\n",
    "        seen_filters.add(list_to_str(b_filter))\n",
    "        permuted_maps.append(map_A_B(perm_list))\n",
    "\n",
    "        maps_two_algorithms = map_A_B(paired_list)\n",
    "        map_diff_test = maps_two_algorithms[0] - maps_two_algorithms[1]\n",
    "        map_diff_test\n",
    "        \n",
    "        [obs_value, pvalue] = get_p_value(permuted_maps, map_diff_test)\n",
    "\n",
    "        if i > min_i:\n",
    "            if (pvalue < 0.01) or (pvalue > 0.1):\n",
    "                break\n",
    "        if i % 1000 == 0:\n",
    "            pass\n",
    "#             print(pvalue)\n",
    "            \n",
    "    # Compare against Student t-test 1sample\n",
    "    \n",
    "    np.random.seed(12345678)\n",
    "\n",
    "    rs_diff = [x[1] - x[2] for x in paired_list]\n",
    "\n",
    "    # rvs1 = [x[1] for x in paired_list]\n",
    "    # rvs2 = [x[2] for x in paired_list]\n",
    "    [t_statistic, t_pvalue] = stats.ttest_1samp(rs_diff,0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return {'rand_pvalue':pvalue,\n",
    "            'obs_value': obs_value,\n",
    "            't_statistic': t_statistic, \n",
    "            't_pvalue': t_pvalue\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each dataset and hpo method\n",
    "\n",
    "\n",
    "bio_test_runs = ['./deep-relevance-ranking/models/baselines/bioasq_dir/bioasq_test_qrels', # qrel\n",
    "                 './deep-relevance-ranking/models/baselines/bioasq_dir/run_bm25_bioasq_test_filtered', # baseline \n",
    "                 './deep-relevance-ranking/models/baselines/posit_results1/qret.txt', # Deep model\n",
    "                 './deep-relevance-ranking/models/baselines/bioasq_dir/run_bioasq_best_lmart_test_leaves15_lr0.07_n750' # lambdaMart                  \n",
    "                ]\n",
    "\n",
    "folds = ['s1', 's2', 's3', 's4', 's5']\n",
    "robust_test_runs = [['./deep-relevance-ranking/models/baselines/robust_dir/' + f + '/robust_test_' + f + '_qrels', # qrel\n",
    "                 './deep-relevance-ranking/models/baselines/robust_dir/' + f + '/run_bm25_robust_test_' + f, # baseline \n",
    "#                  '', # Deep model\n",
    "                 './deep-relevance-ranking/models/baselines/robust_dir/' + f + '/run_robust_' + f + '_best_lmart_test_leaves25_lr0.03_n400' # lambdaMart                  \n",
    "                ] for f in folds]\n",
    "\n",
    "\n",
    "tvqa_test_runs = ['./TVQA/workdir/gold_answer_qrels_test', # qrel\n",
    "                 './TVQA/workdir/retrieved_files/run_tfidf_test', # baseline \n",
    "#                  './TVQA/workdir/deep_model/run_deep_test', # Deep model, check the results when it finishes the training\n",
    "                 './TVQA/workdir/retrieved_files/run_best_lmart_test_leaves5_lr0.44_n1350' # lambdaMart                  \n",
    "                ]\n",
    "\n",
    "file_dirs = [bio_test_runs, *robust_test_runs, tvqa_test_runs]\n",
    "# file_dirs = [bio_test_runs,  tvqa_test_runs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trec_eval_command = './trec_eval/trec_eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./trec_eval/trec_eval', '-q', '-m', 'map', './deep-relevance-ranking/models/baselines/bioasq_dir/bioasq_test_qrels', './deep-relevance-ranking/models/baselines/bioasq_dir/run_bm25_bioasq_test_filtered']\n",
      "['./trec_eval/trec_eval', '-q', '-m', 'map', './deep-relevance-ranking/models/baselines/bioasq_dir/bioasq_test_qrels', './deep-relevance-ranking/models/baselines/posit_results1/qret.txt']\n",
      "['bioasq_dir', 'run_bm25_bioasq_test_filtered']\n",
      "['posit_results1', 'qret.txt']\n",
      "{'rand_pvalue': 0.0, 'obs_value': 0.45982575000000003, 't_statistic': 26.63494141399122, 't_pvalue': 1.4895015699400376e-90}\n",
      "Time spent:  1.6144593469798565 \n",
      "\n",
      "['./trec_eval/trec_eval', '-q', '-m', 'map', './deep-relevance-ranking/models/baselines/bioasq_dir/bioasq_test_qrels', './deep-relevance-ranking/models/baselines/bioasq_dir/run_bm25_bioasq_test_filtered']\n",
      "['./trec_eval/trec_eval', '-q', '-m', 'map', './deep-relevance-ranking/models/baselines/bioasq_dir/bioasq_test_qrels', './deep-relevance-ranking/models/baselines/bioasq_dir/run_bioasq_best_lmart_test_leaves15_lr0.07_n750']\n",
      "['bioasq_dir', 'run_bm25_bioasq_test_filtered']\n",
      "['bioasq_dir', 'run_bioasq_best_lmart_test_leaves15_lr0.07_n750']\n",
      "{'rand_pvalue': 0.1536926147704591, 'obs_value': -0.009346999999999939, 't_statistic': -1.460212231257566, 't_pvalue': 0.14501867696864665}\n",
      "Time spent:  3.4535279870033264 \n",
      "\n",
      "['./trec_eval/trec_eval', '-q', '-m', 'map', './deep-relevance-ranking/models/baselines/robust_dir/s1/robust_test_s1_qrels', './deep-relevance-ranking/models/baselines/robust_dir/s1/run_bm25_robust_test_s1']\n",
      "['./trec_eval/trec_eval', '-q', '-m', 'map', './deep-relevance-ranking/models/baselines/robust_dir/s1/robust_test_s1_qrels', './deep-relevance-ranking/models/baselines/robust_dir/s1/run_robust_s1_best_lmart_test_leaves25_lr0.03_n400']\n",
      "['s1', 'run_bm25_robust_test_s1']\n",
      "['s1', 'run_robust_s1_best_lmart_test_leaves25_lr0.03_n400']\n",
      "{'rand_pvalue': 0.4810379241516966, 'obs_value': -0.004779999999999979, 't_statistic': -0.7147571828804317, 't_pvalue': 0.47815073250613915}\n",
      "Time spent:  4.094332503154874 \n",
      "\n",
      "['./trec_eval/trec_eval', '-q', '-m', 'map', './deep-relevance-ranking/models/baselines/robust_dir/s2/robust_test_s2_qrels', './deep-relevance-ranking/models/baselines/robust_dir/s2/run_bm25_robust_test_s2']\n",
      "['./trec_eval/trec_eval', '-q', '-m', 'map', './deep-relevance-ranking/models/baselines/robust_dir/s2/robust_test_s2_qrels', './deep-relevance-ranking/models/baselines/robust_dir/s2/run_robust_s2_best_lmart_test_leaves25_lr0.03_n400']\n",
      "['s2', 'run_bm25_robust_test_s2']\n",
      "['s2', 'run_robust_s2_best_lmart_test_leaves25_lr0.03_n400']\n",
      "{'rand_pvalue': 0.46506986027944114, 'obs_value': 0.006222448979591816, 't_statistic': 0.7148614567586555, 't_pvalue': 0.47815724717321606}\n",
      "Time spent:  4.834800390526652 \n",
      "\n",
      "['./trec_eval/trec_eval', '-q', '-m', 'map', './deep-relevance-ranking/models/baselines/robust_dir/s3/robust_test_s3_qrels', './deep-relevance-ranking/models/baselines/robust_dir/s3/run_bm25_robust_test_s3']\n",
      "['./trec_eval/trec_eval', '-q', '-m', 'map', './deep-relevance-ranking/models/baselines/robust_dir/s3/robust_test_s3_qrels', './deep-relevance-ranking/models/baselines/robust_dir/s3/run_robust_s3_best_lmart_test_leaves25_lr0.03_n400']\n",
      "['s3', 'run_bm25_robust_test_s3']\n",
      "['s3', 'run_robust_s3_best_lmart_test_leaves25_lr0.03_n400']\n",
      "{'rand_pvalue': 0.28043912175648705, 'obs_value': 0.01145400000000002, 't_statistic': 1.1508406377397753, 't_pvalue': 0.2553829658207985}\n",
      "Time spent:  5.433345679193735 \n",
      "\n",
      "['./trec_eval/trec_eval', '-q', '-m', 'map', './deep-relevance-ranking/models/baselines/robust_dir/s4/robust_test_s4_qrels', './deep-relevance-ranking/models/baselines/robust_dir/s4/run_bm25_robust_test_s4']\n",
      "['./trec_eval/trec_eval', '-q', '-m', 'map', './deep-relevance-ranking/models/baselines/robust_dir/s4/robust_test_s4_qrels', './deep-relevance-ranking/models/baselines/robust_dir/s4/run_robust_s4_best_lmart_test_leaves25_lr0.03_n400']\n",
      "['s4', 'run_bm25_robust_test_s4']\n",
      "['s4', 'run_robust_s4_best_lmart_test_leaves25_lr0.03_n400']\n",
      "{'rand_pvalue': 0.0, 'obs_value': -0.029644000000000004, 't_statistic': -3.6956831433471815, 't_pvalue': 0.0005529787504748122}\n",
      "Time spent:  6.117973128333688 \n",
      "\n",
      "['./trec_eval/trec_eval', '-q', '-m', 'map', './deep-relevance-ranking/models/baselines/robust_dir/s5/robust_test_s5_qrels', './deep-relevance-ranking/models/baselines/robust_dir/s5/run_bm25_robust_test_s5']\n",
      "['./trec_eval/trec_eval', '-q', '-m', 'map', './deep-relevance-ranking/models/baselines/robust_dir/s5/robust_test_s5_qrels', './deep-relevance-ranking/models/baselines/robust_dir/s5/run_robust_s5_best_lmart_test_leaves25_lr0.03_n400']\n",
      "['s5', 'run_bm25_robust_test_s5']\n",
      "['s5', 'run_robust_s5_best_lmart_test_leaves25_lr0.03_n400']\n",
      "{'rand_pvalue': 0.4151696606786427, 'obs_value': -0.006809999999999927, 't_statistic': -0.8166384777746287, 't_pvalue': 0.4180877595787076}\n",
      "Time spent:  6.726904310286045 \n",
      "\n",
      "['./trec_eval/trec_eval', '-q', '-m', 'success.1', './TVQA/workdir/gold_answer_qrels_test', './TVQA/workdir/retrieved_files/run_tfidf_test']\n",
      "['./trec_eval/trec_eval', '-q', '-m', 'success.1', './TVQA/workdir/gold_answer_qrels_test', './TVQA/workdir/retrieved_files/run_best_lmart_test_leaves5_lr0.44_n1350']\n"
     ]
    }
   ],
   "source": [
    "max_iter = 20\n",
    "\n",
    "min_i =1000\n",
    "\n",
    "# initial_b_filter = [1] * 400\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "for fdir in file_dirs:\n",
    "    qrels_file = fdir[0]\n",
    "    baseline_file = fdir[1]\n",
    "    \n",
    "    if 'TVQA' in fdir[0]:\n",
    "        metric = 'success.1'\n",
    "    else:\n",
    "        metric = 'map'\n",
    "    \n",
    "    for model_file in fdir[2:]: \n",
    "        dict_baseline = get_run_avgs(trec_eval_command, metric, qrels_file, baseline_file)\n",
    "        dict_model = get_run_avgs(trec_eval_command, metric, qrels_file, model_file)\n",
    "        paired_list = get_paired_list(dict_baseline, dict_model)\n",
    "        \n",
    "        # map_A = meanAP(list(dict_A.values()))\n",
    "        # map_B = meanAP(list(dict_B.values()))\n",
    "        \n",
    "        results = compute_pvalue(paired_list, max_iter=20000, min_i=1000)\n",
    "        print(baseline_file.split('/')[-2:])\n",
    "        print(model_file.split('/')[-2:])        \n",
    "        print(results)\n",
    "        print('Time spent: ', timeit.default_timer() - start_time, '\\n')\n",
    "    # Plot\n",
    "\n",
    "    # Save results all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [obs_value, pvalue] = get_p_value(permuted_maps, map_diff_test)\n",
    "\n",
    "# print('statistic: ', obs_value)\n",
    "# print('p-value: ', pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
