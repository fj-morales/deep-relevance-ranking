{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./robust_dir/s2/\n",
      "../../robust04_data/split_2/rob04.dev.s2.json\n",
      "./robust_dir/s2/rob04.dev.s2\n",
      "./robust_dir/s2/rob04.dev.s2_trec_query\n",
      "./robust_dir/s2/rob04.dev.s2_qrels\n",
      "['../../../indri-l2r/runquery/IndriRunQuery', './robust_dir/s2/rob04.dev.s2_trec_query', './robust_dir/robust_query_params', './stopwords']\n"
     ]
    }
   ],
   "source": [
    "# %load ir_baseline.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# import pickle\n",
    "import json\n",
    "# import gzip\n",
    "import os\n",
    "import subprocess\n",
    "# import numpy as np\n",
    "# import multiprocessing\n",
    "# import re \n",
    "# import csv\n",
    "# import torch\n",
    "import sys\n",
    "# import shutil\n",
    "# import random\n",
    "import argparse\n",
    "# import argparse\n",
    "\n",
    "# import uuid\n",
    "# import datetime\n",
    "# import time\n",
    "\n",
    "# import bz2\n",
    "# import pandas as pd\n",
    "# # import dbmanager  as dbmanager\n",
    "# from os.path import join\n",
    "\n",
    "## My libraries\n",
    "\n",
    "import utils\n",
    "# import bioasq_corpus_parser\n",
    "import query_parser\n",
    "from join_split_files import join_files\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def load_queries(queries_file):\n",
    "    with open(queries_file, 'rb') as input_file:\n",
    "        query_data = json.load(input_file)\n",
    "        return query_data['questions']\n",
    "\n",
    "# Functions\n",
    "\n",
    "def features_dict(features_file):\n",
    "    with open(features_file, 'rt') as f_file:\n",
    "        feat_dict = {}\n",
    "        for line in f_file:\n",
    "            qid = line.split(' ')[1].split(':')[1]\n",
    "            if qid in feat_dict.keys():\n",
    "                feat_dict[qid].append(line)\n",
    "            else:\n",
    "                feat_dict[qid] = [line]\n",
    "    return feat_dict\n",
    "\n",
    "def save_features(all_features_dict,qids_list, features_split_file):\n",
    "    with open(features_split_file, 'wt') as out_f:\n",
    "        for qid in qids_list:\n",
    "            to_write = all_features_dict[qid]\n",
    "            out_f.write(\"\".join(to_write))\n",
    "\n",
    "\n",
    "class Index:\n",
    "    def __init__(self, ir_toolkit_location, parameter_file_location):\n",
    "        self.ir_toolkit_location = ir_toolkit_location\n",
    "        self.parameter_file_location = parameter_file_location\n",
    "#     def build(self, ir_tool_params):\n",
    "    def build(self):\n",
    "        \n",
    "#         utils.create_dir(self.index_location)\n",
    "    #     index_loc_param = '--indexPath=' + index_loc\n",
    "        stopwords_file = './stopwords'\n",
    "        build_index_command = self.ir_toolkit_location + 'buildindex/IndriBuildIndex'\n",
    "        toolkit_parameters = [\n",
    "                                build_index_command,\n",
    "                                self.parameter_file_location,\n",
    "                                stopwords_file\n",
    "                                ]\n",
    "\n",
    "        print(toolkit_parameters)\n",
    "\n",
    "        proc = subprocess.Popen(toolkit_parameters,stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=False)\n",
    "        (out, err) = proc.communicate()\n",
    "        print(out.decode(\"utf-8\"))\n",
    "        print('Index error: ', err)\n",
    "        if err == None:\n",
    "            return 'Ok'\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "class Query:\n",
    "    def __init__(self, ir_toolkit_location, query_file, query_parameter_file, run_filename):\n",
    "        self.ir_toolkit_location = ir_toolkit_location\n",
    "        self.query_file = query_file\n",
    "        self.query_parameter_file = query_parameter_file\n",
    "        self.run_filename = run_filename\n",
    "        \n",
    "#     def build(self, ir_tool_params):\n",
    "    def run(self):\n",
    "        \n",
    "#         utils.create_dir(self.index_location)\n",
    "    \n",
    "        stopwords_file = './stopwords'\n",
    "        query_command = self.ir_toolkit_location + 'runquery/IndriRunQuery'\n",
    "        toolkit_parameters = [\n",
    "                                query_command,\n",
    "                                self.query_file,\n",
    "                                self.query_parameter_file,\n",
    "                                stopwords_file]\n",
    "        print(toolkit_parameters)\n",
    "        with open(self.run_filename, 'wt') as rf:\n",
    "#             proc = subprocess.Popen(toolkit_parameters,stdin=subprocess.PIPE, stdout=rf, stderr=subprocess.STDOUT, shell=False)\n",
    "            proc = subprocess.Popen(toolkit_parameters,stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=False)\n",
    "            proc2 = subprocess.Popen(['grep', '^.*[ ]Q0[ ]'],stdin=proc.stdout, stdout=rf, stderr=subprocess.STDOUT, shell=False)\n",
    "            (out, err)= proc2.communicate()\n",
    "\n",
    "#             print(out.decode('utf-8'))\n",
    "#             print('Run error: ', err)\n",
    "            if err == None:\n",
    "                pass\n",
    "#                 return 'Ok'\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def generate_features_params(params, feat_param_file):\n",
    "    with open(params[0], 'rt') as q_trec_f:\n",
    "        trec_lines = q_trec_f.readlines()\n",
    "    \n",
    "    with open(feat_param_file, 'wt') as f_out:\n",
    "        for line in trec_lines[:-1]:\n",
    "            f_out.write(line)\n",
    "        f_out.write('<index>' + params[1] + '</index>\\n')    \n",
    "        f_out.write('<outFile>' + params[2] + '</outFile>\\n')    \n",
    "        f_out.write('<rankedDocsFile>' + params[3] + '</rankedDocsFile>\\n')    \n",
    "        f_out.write('<qrelsFile>' + params[4] + '</qrelsFile>\\n')    \n",
    "        f_out.write('<stemmer>' + params[5] + '</stemmer>\\n') # This should not be here!, Fix GenerateExtraFeatures.cpp to read from index manifest\n",
    "        f_out.write('</parameters>\\n')    \n",
    "         \n",
    "        \n",
    "class GenerateExtraFeatures:\n",
    "    def __init__(self, ir_toolkit_location, feat_param_file):\n",
    "        self.ir_toolkit_location = ir_toolkit_location\n",
    "        self.feat_param_file = feat_param_file\n",
    "        self.log_file = feat_param_file + '_run.log'\n",
    "        \n",
    "#     def build(self, ir_tool_params):\n",
    "    def run(self):\n",
    "        \n",
    "        features_command = self.ir_toolkit_location + 'L2R-features/GenerateExtraFeatures'\n",
    "        toolkit_parameters = [\n",
    "                                features_command,\n",
    "                                self.feat_param_file]\n",
    "        print(toolkit_parameters)\n",
    "        with open(self.log_file, 'wt') as rf:\n",
    "            proc = subprocess.Popen(toolkit_parameters,stdin=subprocess.PIPE, stdout=rf, stderr=subprocess.STDOUT, shell=False)\n",
    "#             proc = subprocess.Popen(toolkit_parameters,stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=False)\n",
    "            \n",
    "            (out, err)= proc.communicate()\n",
    "#             print(out.decode('utf-8'))\n",
    "            print(err)\n",
    "        print('Features file generated. Log: ', self.log_file)\n",
    "            \n",
    "        \n",
    "\n",
    "def eval(trec_eval_command, qrel, qret):\n",
    "    \n",
    "    metrics = '-m map -m P.20 -m ndcg_cut.20'\n",
    "    toolkit_parameters = [\n",
    "                            trec_eval_command,\n",
    "                            '-m',\n",
    "                            'map',\n",
    "                            '-m',\n",
    "                            'P.20',\n",
    "                            '-m',\n",
    "                            'ndcg_cut.20',\n",
    "                            qrel,\n",
    "                            qret]\n",
    "\n",
    "#     print(toolkit_parameters)\n",
    "\n",
    "    proc = subprocess.Popen(toolkit_parameters, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=False)\n",
    "    (out, err) = proc.communicate()\n",
    "#     print(out.decode(\"utf-8\"))\n",
    "#    print('Run error: ', err)\n",
    "    if err == None:\n",
    "        pass\n",
    "#         print('No errors')\n",
    "    return out.decode(\"utf-8\")\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "class fakeParser:\n",
    "    def __init__(self):\n",
    "        self.dataset = 'robust' \n",
    "#         self.data_split = 'test'\n",
    "#         self.data_split = 'train'\n",
    "        self.data_split = 'dev'\n",
    "        self.build_index = False\n",
    "        self.fold = '2'\n",
    "        self.gen_features_flag = False\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "# #     ir_toolkit_location = sys.argv[1] # '../indri/'\n",
    "\n",
    "#     # create dataset files dir\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Example 1 - sequential and local execution.')\n",
    "    parser.add_argument('--dataset',   type=str, help='')\n",
    "    parser.add_argument('--data_split',   type=str, help='')\n",
    "    parser.add_argument('--buildindex', action='store_true')\n",
    "    parser.add_argument('--fold', type=str,   help='')\n",
    "    parser.add_argument('--gen_features', action='store_true')\n",
    "    \n",
    "    \n",
    "#     args=parser.parse_args()\n",
    "    args = fakeParser()\n",
    "    \n",
    "    \n",
    "    gen_features_flag = args.gen_features_flag\n",
    "    dataset = args.dataset # \n",
    "    workdir = './' + dataset + '_dir/'\n",
    "    data_split =  args.data_split# 'test'\n",
    "    fold = args.fold\n",
    "    \n",
    "    fold_dir = workdir + 's' + fold + '/'\n",
    "    \n",
    "    if not os.path.exists(fold_dir):\n",
    "        os.makedirs(fold_dir)\n",
    "    \n",
    "    try:\n",
    "        build_index_flag = args.build_index # True\n",
    "    except:\n",
    "        build_index_flag = False\n",
    "    \n",
    "#     # generate corpus files to index\n",
    "    \n",
    "    pool_size = 40 # scales very well when server is not used\n",
    "#     # Get all filenames\n",
    "    \n",
    "#     # Options\n",
    "#     data_dir = '/ssd/francisco/pubmed19-test/'\n",
    "    data_dir = '/ssd/francisco/deep-relevance-ranking/robust04_data/collection/'\n",
    "    \n",
    "    \n",
    "    to_index_dir =  workdir + dataset + '_corpus/'\n",
    "    index_dir = workdir + dataset + '_indri_index'\n",
    "\n",
    "    ir_toolkit_location = '../../../indri-l2r/'\n",
    "    trec_eval_command = '../../eval/trec_eval'\n",
    "    parameter_file_location = workdir + dataset + '_index_param_file'\n",
    "\n",
    "    print(fold_dir)\n",
    "#     utils.create_dir(fold_dir)\n",
    "    \n",
    "    if build_index_flag == True: \n",
    "        \n",
    "        utils.create_dir(index_dir)\n",
    "\n",
    "        index_data = Index(ir_toolkit_location, parameter_file_location)\n",
    "        index_data.build() # time consuming\n",
    "\n",
    "    \n",
    "#     # Generate qrels and qret\n",
    "    \n",
    "    queries_file = '../../robust04_data/split_' + fold + '/rob04.' +  data_split + '.s' + fold + '.json'\n",
    "\n",
    "    print(queries_file)\n",
    "    \n",
    "    prefix = queries_file.split('/')[-1].strip('.json')\n",
    "    filename_prefix = fold_dir + prefix\n",
    "    \n",
    "    print(filename_prefix)\n",
    "    \n",
    "    \n",
    "\n",
    "    #     print(filename_prefix)\n",
    "    \n",
    "    trec_query_file = filename_prefix + '_trec_query'\n",
    "    qrels_file = filename_prefix + '_qrels'\n",
    "    print(trec_query_file)\n",
    "    print(qrels_file)\n",
    "    query_parser.query_parser(queries_file, trec_query_file, qrels_file) # fast\n",
    "    \n",
    "\n",
    "    # Run query\n",
    "    run_filename = fold_dir + 'run_bm25_' + prefix\n",
    "    query_parameter_file = workdir + dataset + '_query_params'\n",
    "    \n",
    "\n",
    "    bm25_query = Query(ir_toolkit_location, trec_query_file, query_parameter_file, run_filename)\n",
    "    bm25_query.run() # fast\n",
    "    \n",
    "    \n",
    "\n",
    "    # Eval\n",
    "    eval(trec_eval_command, qrels_file, run_filename)    \n",
    "    \n",
    "    \n",
    "\n",
    "    # Generate feature param file for all queries\n",
    "    # Only generate if not existent!\n",
    "    \n",
    "#     trec_query_all_file = workdir + 'rob04.trec_queries.json'\n",
    "#     qrels_all_file = workdir + 'rob04_qrels_all'\n",
    "#     run_filename_all = workdir + 'run_bm25_rob04.all'\n",
    "    \n",
    "    gen_features_param_file = workdir + 'rob04' + '_gen_features_params'\n",
    "    out_features_file = workdir + 'rob04' + '_features'\n",
    "    \n",
    "    if gen_features_flag:\n",
    "    \n",
    "        [all_dict_queries_file, run_filename_all, qrels_all_file, trec_query_all_file] = join_files()\n",
    "\n",
    "\n",
    "        features_params =[\n",
    "            trec_query_all_file,\n",
    "            index_dir,\n",
    "            out_features_file,\n",
    "            run_filename_all,\n",
    "            qrels_all_file,\n",
    "            'krovetz', # This should not be here!, Fix GenerateExtraFeatures.cpp to read from index manifest\n",
    "        ]\n",
    "        print(features_params)\n",
    "        print(gen_features_param_file)\n",
    "\n",
    "        generate_features_params(features_params, gen_features_param_file)\n",
    "\n",
    "        # Generate L2R features \n",
    "\n",
    "        feature_generator = GenerateExtraFeatures(ir_toolkit_location, gen_features_param_file)\n",
    "\n",
    "        feature_generator.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "    q_file = queries_file\n",
    "\n",
    "\n",
    "    query_list = load_queries(queries_file)\n",
    "    qid_list = [q['id'] for q in query_list] \n",
    "\n",
    "    feat_dic = features_dict(out_features_file)\n",
    "    \n",
    "    out_features_file = filename_prefix + '_features'\n",
    "\n",
    "    save_features(feat_dic, qid_list, out_features_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
